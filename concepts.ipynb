{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict, Counter\n",
    "from copy import deepcopy\n",
    "import itertools\n",
    "import json\n",
    "import matplotlib.pylab as plt\n",
    "import networkx as nx\n",
    "from numbers import Number\n",
    "from networkx.algorithms import isomorphism\n",
    "from networkx.readwrite import json_graph\n",
    "from networkx import DiGraph\n",
    "from networkx import line_graph\n",
    "from networkx.classes.reportviews import NodeView\n",
    "import numpy as np\n",
    "import pdb\n",
    "from scipy import optimize\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from kmeans_pytorch import kmeans\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..'))\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..', '..'))\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..', '..', '..'))\n",
    "from concept_library.settings import REPR_DIM, DEFAULT_OBJ_TYPE, DEFAULT_BODY_TYPE, RELATION_EMBEDDING\n",
    "from concept_library.models import get_neg_mask_overlap, get_pixel_entropy, get_pixel_gm, get_graph_energy, GNN_energy\n",
    "from concept_library.util import get_connected_loss, visualize_dataset, visualize_matrices, combine_pos, accepts, broadcast_inputs, get_op_shape, action_equal, get_patch, set_patch, find_connected_components, find_connected_components_colordiff, get_op_type, persist_hash\n",
    "from concept_library.util import Combined_Dict, Shared_Param_Dict, repeat_n, tensor_to_string, get_inherit_modes, canonical, canonicalize_keys, masked_equal, get_attr_proper_name, combine_dicts, check_result_true, find_valid_operators, get_obj_bounding_pos, get_comp_obj, get_pos_intersection, shrink, get_repr_dict\n",
    "from concept_library.util import get_soft_Jaccard_distance, to_np_array, to_Variable, plot_matrices, make_dir, remove_duplicates, broadcast_keys, to_string, check_same_set, ddeepcopy as deepcopy\n",
    "from concept_library.util import COLOR_LIST, get_pdict, copy_with_model_dict, record_data, canonicalize_strings, split_string, get_generalized_mean, try_eval, get_rename_mapping, get_next_available_key\n",
    "\n",
    "# Node types:\n",
    "ATTR_TYPE = \"attr\"        # attribute node in Graph class or Concept class\n",
    "OBJ_TYPE = \"obj\"          # Object node in Concept class for a candidate segmentation of an object.\n",
    "INPUT_TYPE = \"input\"      # input node in Graph class\n",
    "INNODE_TYPE = \"fun-in\"    # in_node in Graph class\n",
    "OUTNODE_TYPE = \"fun-out\"  # out_node in Graph class\n",
    "OPERATOR_TYPE = \"self\"    # operator node in Graph class\n",
    "CONCEPT_TYPE = \"concept\"  # concept node in Concept class, or a constant concept in an operator graph\n",
    "\n",
    "# Edge types:\n",
    "OPERATOR_INTRA_EDGE = \"intra\"         # Edge between operator node and its in_node or out_node\n",
    "OPERATOR_INTER_EDGE = \"inter-input\"   # Edge connecting from an out_node (input, attr or fun-out) to an in_node\n",
    "OPERATOR_CONTROL_EDGE = \"inter-ctrl\"  # Edge connecting from an in_node to the Ctrl node of a goal operator\n",
    "GET_ATTR_EDGE = \"intra-attr\"          # Edge from a concept to its attributes.\n",
    "RELATION_EDGE = \"relation\"\n",
    "\n",
    "# Concept Library:\n",
    "CONCEPTS = OrderedDict()     # Predefined concepts\n",
    "NEW_CONCEPTS = OrderedDict() # Newly learned concepts\n",
    "OPERATORS = OrderedDict()    # Predefined operators\n",
    "\n",
    "IS_VIEW = True    # Whether to draw graph when printing in ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Placeholder and Basic functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Placeholder(object):\n",
    "    \"\"\"Placeholder class. Holds a Tensor or a concept string.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        mode,\n",
    "        name=None,\n",
    "        value=None,\n",
    "        shape=None,\n",
    "        range=None,\n",
    "        selector=None,\n",
    "        ebm_key=None,\n",
    "        inplace=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mode: type of the concept.\n",
    "            value: value held by the placeholder\n",
    "            shape: required shape of the placeholder.\n",
    "            range: range of the placeholder's value\n",
    "            selector: if not None, will have a selector.\n",
    "            ebm_key:  if not None, will be the key that points to an EBM in the ebm_dict.\n",
    "            inplace: whether the selector is inplace. If inplace=True, then the operator will copy other parts\n",
    "                not selected by the selector to the output. If inplace=False, then the operator's output will\n",
    "                only consist of the operated objects selected by the selector.\n",
    "        \"\"\"\n",
    "        assert isinstance(mode, Tensor) or isinstance(mode, str), \"mode must be a Tensor or a concept string\"\n",
    "        self.mode = mode\n",
    "        self.value = value\n",
    "        self.shape = shape\n",
    "        self.range = range\n",
    "        self.selector = selector\n",
    "        self.ebm_key = ebm_key\n",
    "        self.inplace = inplace\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.value is not None:\n",
    "            string = \"value->\"\n",
    "        else:\n",
    "            string = \"\"\n",
    "        return \"Placeholder({}{})\".format(string, self.mode)\n",
    "\n",
    "\n",
    "    def __bool__(self):\n",
    "        if (self.mode == \"Bool\" or self.mode.dtype == \"bool\") and not self.value:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "\n",
    "    def change_mode(self, new_mode, new_ebm_key=None):\n",
    "        \"\"\"Change the placeholder's mode to a new mode.\"\"\"\n",
    "        if new_mode != self.mode:\n",
    "            self.mode = new_mode\n",
    "            if hasattr(self, \"ebm_key\"):\n",
    "                assert new_ebm_key is not None\n",
    "                self.ebm_key = new_ebm_key\n",
    "        return self\n",
    "\n",
    "\n",
    "    def set_value(self, value):\n",
    "        self.value = value\n",
    "\n",
    "\n",
    "    def set_selector(self, selector):\n",
    "        self.selector = selector\n",
    "\n",
    "\n",
    "    def get_selector(self):\n",
    "        return self.selector\n",
    "\n",
    "\n",
    "    def set_ebm_key(self, ebm_key):\n",
    "        self.ebm_key = ebm_key\n",
    "        return self\n",
    "\n",
    "\n",
    "    def get_ebm_key(self):\n",
    "        return self.ebm_key\n",
    "\n",
    "\n",
    "    def set_inplace(self, inplace):\n",
    "        self.inplace = inplace\n",
    "\n",
    "\n",
    "    def get_inplace(self):\n",
    "        return self.inplace\n",
    "    \n",
    "    \n",
    "    def copy_with_grad(self, is_copy_module=True, global_attrs=None):\n",
    "        copied_dict = {}\n",
    "        copied_placeholder = Placeholder(self.mode)\n",
    "        for name, value in self.__dict__.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                if value.requires_grad:\n",
    "                    # Copy tensor, detach it from the original computation graph, then allow gradients again\n",
    "                    copied_dict[name] = value.clone().detach().requires_grad_()\n",
    "                else:\n",
    "                    copied_dict[name] = value.clone()\n",
    "            elif isinstance(value, Concept):\n",
    "                copied_dict[name] = value.copy_with_grad(is_copy_module=is_copy_module, global_attrs=global_attrs)\n",
    "            else:\n",
    "                copied_dict[name] = deepcopy(value)\n",
    "        copied_placeholder.__dict__.update(copied_dict)\n",
    "        return copied_placeholder\n",
    "\n",
    "\n",
    "    def is_valid(self, input):\n",
    "        \"\"\"Check if the input is valid for the current placeholder.\"\"\"\n",
    "        if isinstance(self.mode, str):\n",
    "            is_valid, err = input.name == self.mode, \"concept-type\"\n",
    "        else:\n",
    "            is_valid, err = self.mode.is_valid(input)\n",
    "        return is_valid, err\n",
    "\n",
    "\n",
    "    def accepts(self, placeholder, node_name=None):\n",
    "        \"\"\"Whether the current placeholder accepts input from the given placeholder.\"\"\"\n",
    "        if self.mode == \"Concept\":\n",
    "            if isinstance(placeholder, Concept):\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        if isinstance(placeholder, Concept):\n",
    "            node_name = placeholder.name\n",
    "        else:\n",
    "            if not isinstance(placeholder, Placeholder):\n",
    "                return False\n",
    "        if node_name is not None:\n",
    "            # the self placeholder accepts a full node:\n",
    "            if node_name.startswith(self.mode):\n",
    "                return True\n",
    "        if self.mode in [\"Args\", \"Ctrl\"]:\n",
    "            # Input nodes for Cri (criteria):\n",
    "            return True\n",
    "        else:\n",
    "            # Grounding by providing input:\n",
    "            if isinstance(self.mode, str):\n",
    "                if not isinstance(placeholder.mode, str):\n",
    "                    return False\n",
    "                else:\n",
    "                    if canonical(self.mode) == canonical(placeholder.mode):\n",
    "                        return True\n",
    "                    else:\n",
    "                        # Check if it is inherit from the concept:\n",
    "                        if canonical(placeholder.mode) in CONCEPTS and hasattr(CONCEPTS[canonical(placeholder.mode)], \"inherit_from\") and canonical(self.mode) in CONCEPTS[canonical(placeholder.mode)].inherit_from:\n",
    "                            return True\n",
    "                        elif canonical(placeholder.mode) in NEW_CONCEPTS and hasattr(NEW_CONCEPTS[canonical(placeholder.mode)], \"inherit_from\") and canonical(self.mode) in NEW_CONCEPTS[canonical(placeholder.mode)].inherit_from:\n",
    "                            return True\n",
    "                        else:\n",
    "                            return False\n",
    "            else:\n",
    "                return self.mode.dtype == placeholder.mode.dtype\n",
    "\n",
    "\n",
    "# Core classes:\n",
    "class Tensor(object):\n",
    "    \"\"\"Tensor class.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dtype,\n",
    "        shape=None,\n",
    "        range=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        assert dtype in [\"cat\", \"bool\", \"N\", \"real\"]\n",
    "        self.dtype = dtype\n",
    "        self.shape = shape\n",
    "        self.range = range\n",
    "        for key, item in kwargs.items():\n",
    "            setattr(self, key, item)\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        string = \"{}-Tensor(\".format(self.dtype)\n",
    "        if hasattr(self, \"shape\"):\n",
    "            string += \"shape={}, \".format(self.shape)\n",
    "        if hasattr(self, \"range\"):\n",
    "            string += \"range={}, \".format(self.range)\n",
    "        return string[:-2] + \")\"\n",
    "\n",
    "\n",
    "    def is_valid(self, value):\n",
    "        # Check dtype:\n",
    "        if self.dtype in [\"cat\", \"N\"]:\n",
    "            if value.dtype != torch.int64:\n",
    "                return False, \"dtype\"\n",
    "        elif self.dtype == \"real\":\n",
    "            if value.dtype != torch.float32:\n",
    "                return False, \"dtype\"\n",
    "        elif self.dtype == \"bool\":\n",
    "            if value.dtype != torch.bool:\n",
    "                return False, \"dtype\"\n",
    "        # Check shape:\n",
    "        if self.shape is not None and tuple(value.shape) != self.shape:\n",
    "            return False, \"shape\"\n",
    "        # Check range:\n",
    "        if self.range is not None and (value.max() > max(self.range) or value.min() < min(self.range)):\n",
    "            return False, \"range\"\n",
    "        return True, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions:\n",
    "def get_SL_loss(\n",
    "    graph_state,\n",
    "    pred=None,\n",
    "    w_op_dict=None,\n",
    "    loss_type=\"mse\",\n",
    "    channel_coef=None,\n",
    "    empty_coef=None,\n",
    "    obj_coef=None,\n",
    "    mutual_exclusive_coef=None,\n",
    "    pixel_entropy_coef=None,\n",
    "    pixel_gm_coef=None,\n",
    "    iou_batch_consistency_coef=None,\n",
    "    iou_concept_repel_coef=None,\n",
    "    iou_relation_repel_coef=None,\n",
    "    iou_relation_overlap_coef=None,\n",
    "    iou_attract_coef=None,\n",
    "    connected_coef=None,\n",
    "    SGLD_is_anneal=None,\n",
    "    SGLD_is_penalize_lower=None,\n",
    "    SGLD_mutual_exclusive_coef=None,\n",
    "    SGLD_pixel_entropy_coef=None,\n",
    "    SGLD_pixel_gm_coef=None,\n",
    "    SGLD_iou_batch_consistency_coef=None,\n",
    "    SGLD_iou_concept_repel_coef=None,\n",
    "    SGLD_iou_relation_repel_coef=None,\n",
    "    SGLD_iou_relation_overlap_coef=None,\n",
    "    SGLD_iou_attract_coef=None,\n",
    "    lambd_start=None,\n",
    "    lambd=None,\n",
    "    image_value_range=None,\n",
    "    w_init_type=None,\n",
    "    indiv_sample=None,\n",
    "    step_size=None,\n",
    "    step_size_img=None,\n",
    "    step_size_z=None,\n",
    "    step_size_zgnn=None,\n",
    "    step_size_wtarget=None,\n",
    "    connected_num_samples=None,\n",
    "    is_grad=True,\n",
    "    isplot=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Get supervised learning loss on a graph_state.\n",
    "\n",
    "    Args:\n",
    "        graph_state: a GraphState instance\n",
    "        loss_type: Choose from \"mse\", \"ce\" (cross-entropy), \"l1\".\n",
    "        channel_coef: coeffient for the loss between predicted image and the target.\n",
    "        empty_coef: coefficient for the empty channel\n",
    "        obj_coef: coefficient for regularizing that each EBM discovers the objects in the target image\n",
    "        mutual_exclusive_coef: coefficient for penalizing the overlap in mask.\n",
    "        lambd_start: initial noise scale\n",
    "        lambd: ending noise scale\n",
    "        image_value_range: Minimum and maximum value for the values of the image at each pixel. For BabyARC/ARC, use \"0,1\", for CLEVR, use \"-1,1\".\n",
    "        is_grad: whether allowing the gradient to flow through when computing the loss\n",
    "\n",
    "    Returns:\n",
    "        loss: a scalar of the loss.\n",
    "    \"\"\"\n",
    "    g = graph_state.operator_graph\n",
    "    selectors = g.get_selectors()\n",
    "    if len(selectors) == 0:\n",
    "        return None, None\n",
    "    selector = selectors[\"Identity-1:Image\"]\n",
    "    input = graph_state.input[0][0]\n",
    "    target = graph_state.target[0]\n",
    "    loss, loss_dict = get_SL_loss_core(\n",
    "        selector=selector,\n",
    "        input=input,\n",
    "        pred=pred,\n",
    "        w_op_dict=w_op_dict,\n",
    "        target=target,\n",
    "        loss_type=loss_type,\n",
    "        channel_coef=channel_coef,\n",
    "        empty_coef=empty_coef,\n",
    "        obj_coef=obj_coef,\n",
    "        mutual_exclusive_coef=mutual_exclusive_coef,\n",
    "        pixel_entropy_coef=pixel_entropy_coef,\n",
    "        pixel_gm_coef=pixel_gm_coef,\n",
    "        iou_batch_consistency_coef=iou_batch_consistency_coef,\n",
    "        iou_concept_repel_coef=iou_concept_repel_coef,\n",
    "        iou_relation_repel_coef=iou_relation_repel_coef,\n",
    "        iou_relation_overlap_coef=iou_relation_overlap_coef,\n",
    "        iou_attract_coef=iou_attract_coef,\n",
    "        connected_coef=connected_coef,\n",
    "        SGLD_is_anneal=SGLD_is_anneal,\n",
    "        SGLD_is_penalize_lower=SGLD_is_penalize_lower,\n",
    "        SGLD_mutual_exclusive_coef=SGLD_mutual_exclusive_coef,\n",
    "        SGLD_pixel_entropy_coef=SGLD_pixel_entropy_coef,\n",
    "        SGLD_pixel_gm_coef=SGLD_pixel_gm_coef,\n",
    "        SGLD_iou_batch_consistency_coef=SGLD_iou_batch_consistency_coef,\n",
    "        SGLD_iou_concept_repel_coef=SGLD_iou_concept_repel_coef,\n",
    "        SGLD_iou_relation_repel_coef=SGLD_iou_relation_repel_coef,\n",
    "        SGLD_iou_relation_overlap_coef=SGLD_iou_relation_overlap_coef,\n",
    "        SGLD_iou_attract_coef=SGLD_iou_attract_coef,\n",
    "        lambd_start=lambd_start,\n",
    "        lambd=lambd,\n",
    "        image_value_range=image_value_range,\n",
    "        w_init_type=w_init_type,\n",
    "        indiv_sample=indiv_sample,\n",
    "        step_size=step_size,\n",
    "        step_size_img=step_size_img,\n",
    "        step_size_z=step_size_z,\n",
    "        step_size_zgnn=step_size_zgnn,\n",
    "        step_size_wtarget=step_size_wtarget,\n",
    "        connected_num_samples=connected_num_samples,\n",
    "        is_grad=is_grad,\n",
    "        isplot=isplot,\n",
    "    )\n",
    "    return loss, loss_dict\n",
    "\n",
    "\n",
    "def get_SL_loss_core(\n",
    "    selector,\n",
    "    input,\n",
    "    pred,\n",
    "    w_op_dict,\n",
    "    target,\n",
    "    loss_type,\n",
    "    channel_coef,\n",
    "    empty_coef,\n",
    "    obj_coef,\n",
    "    mutual_exclusive_coef,\n",
    "    pixel_entropy_coef,\n",
    "    pixel_gm_coef,\n",
    "    iou_batch_consistency_coef,\n",
    "    iou_concept_repel_coef,\n",
    "    iou_relation_repel_coef,\n",
    "    iou_relation_overlap_coef,\n",
    "    iou_attract_coef,\n",
    "    connected_coef,\n",
    "    SGLD_is_anneal,\n",
    "    SGLD_is_penalize_lower,\n",
    "    SGLD_mutual_exclusive_coef,\n",
    "    SGLD_pixel_entropy_coef,\n",
    "    SGLD_pixel_gm_coef,\n",
    "    SGLD_iou_batch_consistency_coef,\n",
    "    SGLD_iou_concept_repel_coef,\n",
    "    SGLD_iou_relation_repel_coef,\n",
    "    SGLD_iou_relation_overlap_coef,\n",
    "    SGLD_iou_attract_coef,\n",
    "    lambd_start,\n",
    "    lambd,\n",
    "    image_value_range,\n",
    "    w_init_type,\n",
    "    indiv_sample,\n",
    "    step_size,\n",
    "    step_size_img,\n",
    "    step_size_z,\n",
    "    step_size_zgnn,\n",
    "    step_size_wtarget,\n",
    "    connected_num_samples,\n",
    "    is_grad=True,\n",
    "    isplot=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Get supervised learning loss on a selector.\n",
    "\n",
    "    Args (priority: the arguments passed in > selector's attributes > default values):\n",
    "        selector: a selector\n",
    "        input: a tensor with shape [B, C:10, H, W]\n",
    "        target: a tensor with shape [B, C:10, H, W]\n",
    "        loss_type: Choose from \"mse\", \"ce\" (cross-entropy), \"l1\".\n",
    "        channel_coef: coeffient for the loss between predicted image and the target.\n",
    "        empty_coef: coefficient for the empty channel.\n",
    "        obj_coef: coefficient for regularizing that each EBM discovers the objects in the target image.\n",
    "        mutual_exclusive_coef: coefficient for penalizing the overlap in mask.\n",
    "        lambd_start: initial noise scale\n",
    "        lambd: ending noise scale\n",
    "        image_value_range: Minimum and maximum value for the values of the image at each pixel. For BabyARC/ARC, use \"0,1\", for CLEVR, use \"-1,1\".\n",
    "        is_grad: whether allowing the gradient to flow through when computing the loss\n",
    "\n",
    "    Returns:\n",
    "        loss: a scalar of the loss.\n",
    "    \"\"\"\n",
    "    # is_grad=True is important to be able to pass gradient back to the SGLD:\n",
    "    if w_op_dict is None:\n",
    "        assert pred is None\n",
    "        device = input.device\n",
    "        pred, w_op_dict = selector.forward_NN(\n",
    "            input,\n",
    "            is_grad=is_grad,\n",
    "            lambd_start=lambd_start,\n",
    "            lambd=lambd,\n",
    "            SGLD_is_anneal=SGLD_is_anneal,\n",
    "            SGLD_is_penalize_lower=SGLD_is_penalize_lower,\n",
    "            SGLD_mutual_exclusive_coef=SGLD_mutual_exclusive_coef,\n",
    "            SGLD_pixel_entropy_coef=SGLD_pixel_entropy_coef,\n",
    "            SGLD_pixel_gm_coef=SGLD_pixel_gm_coef,\n",
    "            SGLD_iou_batch_consistency_coef=SGLD_iou_batch_consistency_coef,\n",
    "            SGLD_iou_concept_repel_coef=SGLD_iou_concept_repel_coef,\n",
    "            SGLD_iou_relation_repel_coef=SGLD_iou_relation_repel_coef,\n",
    "            SGLD_iou_relation_overlap_coef=SGLD_iou_relation_overlap_coef,\n",
    "            SGLD_iou_attract_coef=SGLD_iou_attract_coef,\n",
    "            image_value_range=image_value_range,\n",
    "            w_init_type=w_init_type,\n",
    "            indiv_sample=indiv_sample,\n",
    "            step_size=step_size,\n",
    "            step_size_img=step_size_img,\n",
    "            step_size_z=step_size_z,\n",
    "            step_size_zgnn=step_size_zgnn,\n",
    "            step_size_wtarget=step_size_wtarget,\n",
    "            isplot=isplot,\n",
    "        )\n",
    "    else:\n",
    "        w_0 = w_op_dict[next(iter(w_op_dict))]\n",
    "        device = w_0.device\n",
    "    if len(w_0.shape) == 5:\n",
    "        batch_shape = tuple(target.shape[:2])  # [B_task, B_example]\n",
    "        target = target.view(-1, *target.shape[-3:])\n",
    "        if pred is not None:\n",
    "            pred = pred.view(-1, *pred.shape[-3:])\n",
    "        w_op_dict = {key: item.view(-1, *item.shape[-3:]) for key, item in w_op_dict.items()}\n",
    "    else:\n",
    "        batch_shape = None\n",
    "\n",
    "    assert len(target.shape) == 4\n",
    "    assert pred is None or len(pred.shape) == 4\n",
    "    assert len(w_op_dict[next(iter(w_op_dict))].shape) == 4\n",
    "\n",
    "    channel_coef = channel_coef if channel_coef is not None else selector.channel_coef if selector.channel_coef is not None else 1\n",
    "    obj_coef = obj_coef if obj_coef is not None else selector.obj_coef if selector.obj_coef is not None else 0.1\n",
    "    empty_coef = empty_coef if empty_coef is not None else selector.empty_coef if selector.empty_coef is not None else 0.02\n",
    "    mutual_exclusive_coef = mutual_exclusive_coef if mutual_exclusive_coef is not None else selector.mutual_exclusive_coef if selector.mutual_exclusive_coef is not None else 0.1\n",
    "    pixel_entropy_coef = pixel_entropy_coef if pixel_entropy_coef is not None else selector.pixel_entropy_coef if selector.pixel_entropy_coef is not None else 0.\n",
    "    pixel_gm_coef = pixel_gm_coef if pixel_gm_coef is not None else selector.pixel_gm_coef if selector.pixel_gm_coef is not None else 0.\n",
    "    iou_batch_consistency_coef = iou_batch_consistency_coef if iou_batch_consistency_coef is not None else selector.iou_batch_consistency_coef if selector.iou_batch_consistency_coef is not None else 0.\n",
    "    iou_concept_repel_coef = iou_concept_repel_coef if iou_concept_repel_coef is not None else selector.iou_concept_repel_coef if selector.iou_concept_repel_coef is not None else 0.\n",
    "    iou_relation_repel_coef = iou_relation_repel_coef if iou_relation_repel_coef is not None else selector.iou_relation_repel_coef if selector.iou_relation_repel_coef is not None else 0.\n",
    "    iou_relation_overlap_coef = iou_relation_overlap_coef if iou_relation_overlap_coef is not None else selector.iou_relation_overlap_coef if selector.iou_relation_overlap_coef is not None else 0.\n",
    "    iou_attract_coef = iou_attract_coef if iou_attract_coef is not None else selector.iou_attract_coef if selector.iou_attract_coef is not None else 0.\n",
    "\n",
    "    loss_dict = {}\n",
    "    loss = torch.tensor(0., dtype=torch.float32).to(device)\n",
    "    if obj_coef > 0 and pred is not None:\n",
    "        loss_obj = get_obj_loss(pred, w_op_dict, target, loss_type=loss_type) * obj_coef\n",
    "        loss = loss + loss_obj\n",
    "        loss_dict[\"loss_obj\"] = to_np_array(loss_obj)\n",
    "\n",
    "    if loss_type == \"mse\":\n",
    "        loss_fun = nn.MSELoss()\n",
    "    elif loss_type == \"l1\":\n",
    "        loss_fun = nn.L1Loss()\n",
    "\n",
    "    if pred is not None:\n",
    "        if pred.shape[1] == 10:\n",
    "            if loss_type in [\"mse\", \"l1\"]:\n",
    "                if channel_coef > 0:\n",
    "                    loss_channel = loss_fun(pred[:,1:], target[:,1:]) * channel_coef\n",
    "                else:\n",
    "                    loss_channel = torch.tensor(0., dtype=torch.float32).to(device)\n",
    "                if empty_coef > 0:\n",
    "                    loss_empty = loss_fun(pred[:,:1], target[:,:1]) * empty_coef\n",
    "                else:\n",
    "                    loss_empty = torch.tensor(0., dtype=torch.float32).to(device)\n",
    "            else:\n",
    "                raise Exception(\"loss_type {} is not valid!\".format(loss_type))\n",
    "        else:\n",
    "            # No channel dedicated to background pixels\n",
    "            assert pred.shape[1] == 3 or pred.shape[1] == 2\n",
    "            if loss_type in [\"mse\", \"l1\"]:\n",
    "                if channel_coef > 0:\n",
    "                    loss_channel = loss_fun(pred, target) * channel_coef\n",
    "                else:\n",
    "                    loss_channel = torch.tensor(0., dtype=torch.float32).to(device)\n",
    "                loss_empty = torch.tensor(0., dtype=torch.float32).to(device)\n",
    "            else:\n",
    "                raise\n",
    "    else:\n",
    "        loss_channel = torch.tensor(0., dtype=torch.float32).to(device)\n",
    "        loss_empty = torch.tensor(0., dtype=torch.float32).to(device)\n",
    "\n",
    "    loss = loss + loss_channel + loss_empty\n",
    "    loss_dict[\"loss_channel\"] = to_np_array(loss_channel)\n",
    "    loss_dict[\"loss_empty\"] = to_np_array(loss_empty)\n",
    "\n",
    "    mask_list = list(w_op_dict.values())\n",
    "    w_op = mask_list[0]\n",
    "\n",
    "    # Mutual exclusive loss for mask:\n",
    "    if mutual_exclusive_coef > 0 and w_op.shape[1] == 1:\n",
    "        loss_mask_overlap = get_neg_mask_overlap(mask_list, mask_info=selector.get_mask_info()).mean() * mutual_exclusive_coef\n",
    "        loss = loss + loss_mask_overlap\n",
    "        loss_dict[\"loss_mask_overlap\"] = to_np_array(loss_mask_overlap)\n",
    "\n",
    "    if pixel_entropy_coef > 0 and w_op.shape[1] == 1:\n",
    "        loss_pixel_entropy = get_pixel_entropy(mask_list).mean() * pixel_entropy_coef\n",
    "        loss = loss + loss_pixel_entropy\n",
    "        loss_dict[\"loss_pixel_entropy\"] = to_np_array(loss_pixel_entropy)\n",
    "\n",
    "    if pixel_gm_coef > 0 and w_op.shape[1] == 1:\n",
    "        loss_pixel_gm = get_pixel_gm(mask_list).mean() * pixel_gm_coef\n",
    "        loss = loss + loss_pixel_gm\n",
    "        loss_dict[\"loss_pixel_gm\"] = to_np_array(loss_pixel_gm)\n",
    "        \n",
    "    if connected_coef > 0 and w_op.shape[1] == 1:\n",
    "        loss_connected = get_connected_loss(torch.cat(mask_list), connected_num_samples).mean() * connected_coef\n",
    "        loss = loss + loss_connected\n",
    "        loss_dict[\"loss_connected\"] = to_np_array(loss_connected)\n",
    "\n",
    "    if w_op.shape[1] == 1 and (\n",
    "        iou_batch_consistency_coef > 0 or\n",
    "        iou_concept_repel_coef > 0 or\n",
    "        iou_relation_repel_coef > 0 or\n",
    "        iou_relation_overlap_coef > 0 or\n",
    "        iou_attract_coef > 0\n",
    "    ):\n",
    "        loss_iou, loss_iou_dict = get_graph_energy(\n",
    "            mask_list,\n",
    "            mask_info=selector.get_mask_info(),\n",
    "            iou_batch_consistency_coef=iou_batch_consistency_coef,\n",
    "            iou_concept_repel_coef=iou_concept_repel_coef,\n",
    "            iou_relation_repel_coef=iou_relation_repel_coef,\n",
    "            iou_relation_overlap_coef=iou_relation_overlap_coef,\n",
    "            iou_attract_coef=iou_attract_coef,\n",
    "            batch_shape=batch_shape,\n",
    "        )\n",
    "        loss = loss + loss_iou.mean()\n",
    "        loss_iou_dict_mean = {key: item.mean() for key, item in loss_iou_dict.items()}\n",
    "        loss_dict.update(loss_iou_dict_mean)\n",
    "\n",
    "    return loss, loss_dict\n",
    "\n",
    "\n",
    "def get_obj_loss(pred, w_op_dict, target, loss_type=\"mse\"):\n",
    "    \"\"\"Get loss on individual discovered objects.\n",
    "\n",
    "    4 scenarios:\n",
    "        (1) pred.shape[1] == 10 and w_op.shape[1] == 1:  BabyARC, each w_op is a mask from 1st SGLD, and pred comes from 2nd SGLD\n",
    "        (2) pred.shape[1] == 10 and w_op.shape[1] == 10: BabyARC, each w_op is an object from 1st SGLD, and pred comes from combining objs from w_op_dict\n",
    "        (3) pred.shape[1] == 3  and w_op.shape[1] == 1:  CLEVR, each w_op is a mask from 1st SGLD, and pred comes from 2nd SGLD\n",
    "        (4) pred.shape[1] == 3  and w_op.shape[1] == 3:  CLEVR, each w_op is an object from 1st SGLD. In this case no object loss, and pred comes from combining objs from w_op_dict\n",
    "    \"\"\"\n",
    "    if loss_type == \"mse\":\n",
    "        loss_fun = nn.MSELoss()\n",
    "    elif loss_type == \"l1\":\n",
    "        loss_fun = nn.L1Loss()\n",
    "    else:\n",
    "        raise Exception(\"loss_type {} is not valid!\".format(loss_type))\n",
    "\n",
    "    loss_obj = torch.tensor(0., dtype=torch.float32).to(target.device)\n",
    "    for key, w_op in w_op_dict.items():\n",
    "        if pred.shape[1] == 10:\n",
    "            if w_op.shape[1] == 1:\n",
    "                # w_op is a mask:\n",
    "                loss_obj_ele = loss_fun(pred*w_op, target*w_op)\n",
    "            else:\n",
    "                assert w_op.shape[1] == 10\n",
    "                mask = (w_op.argmax(1) != 0)[:, None]\n",
    "                loss_obj_ele = loss_fun(w_op*mask, target*mask)\n",
    "        else:\n",
    "            assert pred.shape[1] == 3 or pred.shape[1] == 2\n",
    "            if w_op.shape[1] == 1:\n",
    "                loss_obj_ele = loss_fun(pred*w_op, target*w_op)\n",
    "            else:\n",
    "                loss_obj_ele = 0\n",
    "        loss_obj = loss_obj + loss_obj_ele\n",
    "    return loss_obj\n",
    "\n",
    "\n",
    "def copy_helper(to_copy_dict, is_share_fun=False, is_copy_module=True, global_attrs=None):\n",
    "    \"\"\"\n",
    "    Deepcopy a dictionary, taking into consideration about torch.nn.Modules and Tensors with grads.\n",
    "\n",
    "    Args:\n",
    "        is_copy_module: if True, will copy torch.nn.Module. Otherwise not copy.\n",
    "        global_attrs: a list of class attribute names that are global dictionaries.\n",
    "\n",
    "    Returns:\n",
    "        copied_dict: The deepcopied dictionary.\n",
    "        global_dicts: the global dictionaries as class attributes, if the arg global_attrs is not None.\n",
    "    \"\"\"\n",
    "    global_dicts = {}\n",
    "    if isinstance(to_copy_dict, torch.Tensor):\n",
    "        if to_copy_dict.requires_grad:\n",
    "            # Copy tensor, detach it from the original computation graph, then allow gradients again\n",
    "            to_copy_dict = to_copy_dict.clone().detach().requires_grad_()\n",
    "        else:\n",
    "            to_copy_dict = to_copy_dict.clone()\n",
    "        return to_copy_dict, global_dicts\n",
    "\n",
    "    copied_dict = {}\n",
    "    if global_attrs is None:\n",
    "        global_attrs = []\n",
    "    if not isinstance(global_attrs, list):\n",
    "        global_attrs = [global_attrs]\n",
    "    if isinstance(to_copy_dict, OrderedDict):\n",
    "        copied_dict = OrderedDict()\n",
    "    for name, value in to_copy_dict.items():\n",
    "        if isinstance(value, Placeholder):\n",
    "            copied_dict[name] = value.copy_with_grad(is_copy_module=is_copy_module, global_attrs=global_attrs)\n",
    "        elif isinstance(value, dict):\n",
    "            if name in global_attrs:\n",
    "                global_dicts[name] = value\n",
    "            else:\n",
    "                copied_dict[name], global_dicts_ele = copy_helper(value, is_copy_module=is_copy_module, global_attrs=global_attrs)\n",
    "        elif isinstance(value, nn.Module) and not isinstance(value, nx.Graph):\n",
    "            if is_copy_module:\n",
    "                other_attr = []\n",
    "                if value.__class__.__name__ == \"ConceptEBM\":\n",
    "                    other_attr += [\"c_repr\", \"c_str\"]\n",
    "                copied_dict[name] = copy_with_model_dict(value, other_attr=other_attr)\n",
    "        elif isinstance(value, torch.Tensor):\n",
    "            if value.requires_grad:\n",
    "                # Copy tensor, detach it from the original computation graph, then allow gradients again\n",
    "                copied_dict[name] = value.clone().detach().requires_grad_()\n",
    "            else:\n",
    "                copied_dict[name] = value.clone()\n",
    "        elif isinstance(value, BaseGraph):\n",
    "            copied_dict[name] = value.copy_with_grad(is_share_fun=is_share_fun, is_copy_module=is_copy_module, global_attrs=global_attrs)\n",
    "        elif isinstance(value, tuple):\n",
    "            copied_dict[name] = tuple(copy_helper(element, is_share_fun=is_share_fun, is_copy_module=is_copy_module, global_attrs=global_attrs)[0] for element in value)\n",
    "        elif not isinstance(value, NodeView):\n",
    "            try:\n",
    "                copied_dict[name] = deepcopy(value)\n",
    "            except Exception as e:\n",
    "                pdb.set_trace()\n",
    "    return copied_dict, global_dicts\n",
    "\n",
    "\n",
    "def init_tensor(placeholder, is_cuda=False):\n",
    "    \"\"\"Initialize PyTorch tensor according to the specs of the placeholder.\n",
    "\n",
    "    placeholder.mode: \"Pos\", \"RelPos\", \"Cat\", \"Bool\"\n",
    "    \"\"\"\n",
    "    concept = CONCEPTS[placeholder.mode]\n",
    "    assert len(concept.nodes) == 1\n",
    "    tensor_spec = concept.get_node_content(placeholder.mode).mode\n",
    "    assert isinstance(tensor_spec, Tensor)\n",
    "    drange = tensor_spec.range if tensor_spec.range is not None else placeholder.range\n",
    "    dshape = tensor_spec.shape if tensor_spec.shape is not None else placeholder.shape\n",
    "    tensor = to_Variable(np.random.rand(*dshape) * (max(drange) - min(drange)) + min(drange), is_cuda=is_cuda)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def check_input_valid(op, *obj_names):\n",
    "    \"\"\"Returns True if the obj_names (e.g. [obj_1:Image, obj_2:Line]) is valid for the op (considering concept inheritance).\"\"\"\n",
    "    is_valid = True\n",
    "    for i, obj_name in enumerate(obj_names):\n",
    "        input_placeholder = Placeholder(op.input_placeholder_nodes[i].split(\":\")[-1])\n",
    "        obj_placeholder = Placeholder(obj_name.split(\":\")[-1])\n",
    "        is_valid = input_placeholder.accepts(obj_placeholder)\n",
    "        if not is_valid:\n",
    "            is_valid = False\n",
    "            break\n",
    "    return is_valid\n",
    "\n",
    "\n",
    "class MyBounds(object):\n",
    "    \"\"\"Used to bound the basinhopping search function implemented below in search()\"\"\"\n",
    "    def __init__(self, xmax=[1.1,1.1], xmin=[-1.1,-1.1]):\n",
    "        self.xmax = np.array(xmax)\n",
    "        self.xmin = np.array(xmin)\n",
    "        \n",
    "    def __call__(self, **kwargs):\n",
    "        x = kwargs[\"x_new\"]\n",
    "        tmax = bool(np.all(x <= self.xmax))\n",
    "        tmin = bool(np.all(x >= self.xmin))\n",
    "        return tmax and tmin\n",
    "\n",
    "\n",
    "def init_ebm_dict(\n",
    "    modes,\n",
    "    ebm_mode,\n",
    "    CONCEPTS=None,\n",
    "    OPERATORS=None,\n",
    "    cache_forward=True,\n",
    "    device=\"cpu\",\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"Initialize the ebm_dict with the given modes.\"\"\"\n",
    "    num_colors = 10\n",
    "    selector = Concept_Pattern(\n",
    "        name=None,\n",
    "        value=Placeholder(Tensor(dtype=\"cat\", range=range(num_colors))),\n",
    "        attr={},\n",
    "        is_all_obj=True,\n",
    "        is_ebm=True,\n",
    "        is_default_ebm=False,\n",
    "        ebm_dict={},\n",
    "        CONCEPTS=CONCEPTS,\n",
    "        OPERATORS=OPERATORS,\n",
    "        device=device,\n",
    "        cache_forward=cache_forward,\n",
    "        **kwargs\n",
    "    )\n",
    "    ebm_dict = selector.ebm_dict\n",
    "    assert len(ebm_dict) == 0\n",
    "    for mode in modes:\n",
    "        selector.init_ebm(\n",
    "            method=\"random\",\n",
    "            mode=mode,\n",
    "            ebm_mode=ebm_mode,\n",
    "            ebm_model_type=\"CEBM\",\n",
    "            CONCEPTS=CONCEPTS,\n",
    "            OPERATORS=OPERATORS,\n",
    "        )\n",
    "    return ebm_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 BaseGraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseGraph(nx.MultiDiGraph, nn.Module):\n",
    "    def __init__(self, G=None, **kwargs):\n",
    "        \"\"\"Backbone of Concept() and Graph() classes:\n",
    "\n",
    "        Contains basic methods for manipulating the graph, and visualization.\n",
    "        \"\"\"\n",
    "        self.is_cuda = kwargs[\"is_cuda\"] if \"is_cuda\" in kwargs else False\n",
    "        self.verbose = kwargs[\"verbose\"] if \"verbose\" in kwargs else True\n",
    "        if G is not None and G.device is not None:\n",
    "            self.device = G.device\n",
    "        else:\n",
    "            self.device = torch.device(self.is_cuda if isinstance(self.is_cuda, str) else \"cuda\" if self.is_cuda else \"cpu\")\n",
    "        BaseGraph.__mro__[-2].__init__(self)  # Obtain the superclass nn.Module\n",
    "        if \"name\" in kwargs:\n",
    "            super(BaseGraph, self).__init__(incoming_graph_data=G, name=kwargs[\"name\"])\n",
    "        else:\n",
    "            super(BaseGraph, self).__init__(incoming_graph_data=G)\n",
    "\n",
    "\n",
    "    def copy_with_grad(self, is_share_fun=False, is_copy_module=True, global_attrs=None):\n",
    "        \"\"\"Return the copy of current instance by detaching tensors which have grad\n",
    "        and deepcopying all other object attributes.\n",
    "\n",
    "        Args:\n",
    "            is_share_fun: if True, the copy will share its torch.nn.Modules with its original.\n",
    "            is_copy_module: if True, will copy torch.nn.Module. Otherwise not copy.\n",
    "            global_attrs: a list of class attribute names that are global dictionaries.\n",
    "\n",
    "        Returns:\n",
    "            G: the copied class instance.\n",
    "        \"\"\"\n",
    "        G_copy = self.__class__()\n",
    "        copied_dict, global_dicts = copy_helper(self.__dict__, is_copy_module=is_copy_module, global_attrs=global_attrs)\n",
    "        G_copy.__dict__.update(copied_dict)\n",
    "        G = self.__class__(G=G_copy)\n",
    "\n",
    "        if global_attrs is not None:\n",
    "            # Set the global dictionaries as attributes of the class:\n",
    "            for key, Dict in global_dicts.items():\n",
    "                setattr(G, key, Dict)\n",
    "        if is_share_fun:\n",
    "            for fun_name in self.funs:\n",
    "                setattr(G, \"fun_\" + fun_name, getattr(self, \"fun_\" + fun_name))\n",
    "                G.set_node_content(getattr(G, \"fun_\" + fun_name), fun_name)\n",
    "        return G\n",
    "\n",
    "\n",
    "    def copy(self, is_share_fun=False):\n",
    "        \"\"\"Return the copy of current instance. Warning: does not work when passing\n",
    "        gradients through graph.\"\"\"\n",
    "        G = deepcopy(self)\n",
    "        if is_share_fun:\n",
    "            for fun_name in self.funs:\n",
    "                setattr(G, \"fun_\" + fun_name, getattr(self, \"fun_\" + fun_name))\n",
    "                G.set_node_content(getattr(G, \"fun_\" + fun_name), fun_name)\n",
    "        return G\n",
    "\n",
    "\n",
    "    def copy_shallow(self):\n",
    "        \"\"\"Return a shallow copy of current instance.\"\"\"\n",
    "        return super(nx.MultiDiGraph, self).copy()\n",
    "\n",
    "\n",
    "    def to(self, device):\n",
    "        super(BaseGraph, self).to(device)\n",
    "        self.device = device\n",
    "        return self\n",
    "\n",
    "\n",
    "    ########################################\n",
    "    # Obtain or modify node content and neighbors:\n",
    "    ########################################\n",
    "    def get_node_name(self, node_name=None):\n",
    "        \"\"\"Get full node_name. If node_name is None (assuming only one node in the graph), \n",
    "        return the unique node_name.\"\"\"\n",
    "        if node_name is None:\n",
    "            node_name = self.name\n",
    "            if self.name is None:\n",
    "                if len(self.nodes) == 1:\n",
    "                    node_name = list(self.nodes)[0]\n",
    "        elif node_name == \"$root\":\n",
    "            node_name = self.name\n",
    "        else:\n",
    "            if \":\" in node_name:\n",
    "                pass\n",
    "            else:\n",
    "                if node_name in self.nodes:\n",
    "                    pass\n",
    "                else:\n",
    "                    is_exist = False\n",
    "                    for node in sorted(self.nodes):\n",
    "                        if node.startswith(node_name + \":\"):\n",
    "                            node_name = node\n",
    "                            is_exist = True\n",
    "                            break\n",
    "                    assert is_exist, \"node '{}' does not exist!\".format(node_name)\n",
    "        return node_name\n",
    "\n",
    "\n",
    "    def get_node_content(self, node_name=None):\n",
    "        \"\"\"Obtain the content of a node. The content can be a placeholder, a Concept(), or a Graph().\"\"\"\n",
    "        node_name = self.get_node_name(node_name)\n",
    "        if \"value\" in self.nodes(data=True)[node_name]:\n",
    "            return self.nodes(data=True)[node_name][\"value\"]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "    def set_node_content(self, content, node_name=None):\n",
    "        \"\"\"Set the content of a node. The content can be a placeholder, a Concept(), or a Graph().\"\"\"\n",
    "        node_name = self.get_node_name(node_name)\n",
    "        self.nodes(data=True)[node_name][\"value\"] = content\n",
    "        return self\n",
    "\n",
    "\n",
    "    def get_node_type(self, node_name=None):\n",
    "        \"\"\"Obtain the type of a node.\"\"\"\n",
    "        node_name = self.get_node_name(node_name)\n",
    "        return self.nodes(data=True)[node_name][\"type\"]\n",
    "\n",
    "\n",
    "    def get_node_repr(self, node_name=None):\n",
    "        \"\"\"Obtain the repr (embedding) of a concept node.\"\"\"\n",
    "        node_name = self.get_node_name(node_name)\n",
    "        if \"repr\" in self.nodes(data=True)[node_name]:\n",
    "            return self.nodes(data=True)[node_name][\"repr\"]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "    def operator_name(self, operator):\n",
    "        \"\"\"Get the name of the operator. If it is an attribute node, preserve the attribute.\"\"\"\n",
    "        if \"^\" in operator or \"input\" in operator or \"concept\" in operator:\n",
    "            # If it is an attribute node:\n",
    "            return operator.split(\":\")[0]\n",
    "        else:\n",
    "            if \"-\" in operator:\n",
    "                return operator.split(\"-\")[0]\n",
    "            else:\n",
    "                return operator.split(\":\")[0]\n",
    "\n",
    "\n",
    "    def get_node_fun(self, node_name=None):\n",
    "        \"\"\"Obtain the fun of a concept node.\"\"\"\n",
    "        node_name = self.get_node_name(node_name)\n",
    "        if \"fun\" in self.nodes(data=True)[node_name]:\n",
    "            return self.nodes(data=True)[node_name][\"fun\"]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "    def get_node_value(self, node_name=None):\n",
    "        \"\"\"Get the value held in the Placeholder of a node, if the content is a Placeholder.\"\"\"\n",
    "        if not isinstance(self.get_node_content(node_name), Placeholder):\n",
    "            return None\n",
    "        else:\n",
    "            node_name = self.get_node_name(node_name)\n",
    "            if \"fun\" in self.nodes(data=True)[node_name] and self.nodes(data=True)[node_name][\"fun\"] is not None and len(self.parent_nodes(node_name)) > 0:\n",
    "                fun = self.nodes(data=True)[node_name][\"fun\"]\n",
    "                value = fun(self.get_node_value(self.parent_nodes(node_name)[0]))\n",
    "                self.nodes(data=True)[node_name][\"value\"].value = value\n",
    "                return value\n",
    "            else:\n",
    "                return self.nodes(data=True)[node_name][\"value\"].value\n",
    "\n",
    "\n",
    "    def set_node_value(self, value, node_name=None):\n",
    "        \"\"\"Set up the value held in the Placeholder of a node, if the content is a Placeholder.\"\"\"\n",
    "        assert isinstance(self.get_node_content(node_name), Placeholder)\n",
    "        node_name = self.get_node_name(node_name)\n",
    "        if not isinstance(value, torch.Tensor):\n",
    "            self.nodes(data=True)[node_name][\"value\"].value = to_Variable(value, is_cuda=self.is_cuda)\n",
    "        else:\n",
    "            self.nodes(data=True)[node_name][\"value\"].value = value\n",
    "        return self\n",
    "\n",
    "\n",
    "    def remove_node_value(self, node_name=None):\n",
    "        \"\"\"Delete the value held in the Placeholder of a node, if the content is a Placeholder.\"\"\"\n",
    "        assert isinstance(self.get_node_content(node_name), Placeholder)\n",
    "        node_name = self.get_node_name(node_name)\n",
    "        self.nodes(data=True)[node_name][\"value\"].value = None\n",
    "        return self\n",
    "\n",
    "\n",
    "    def parent_nodes(self, node_name):\n",
    "        \"\"\"Obtain the parent nodes of a given node.\"\"\"\n",
    "        parent_nodes = []\n",
    "        node_name = self.get_node_name(node_name)\n",
    "        for node, adj in self[node_name].items():\n",
    "            if adj[0][\"type\"].startswith(\"b-\") and \"relation\" not in adj[0][\"type\"]:\n",
    "                parent_nodes.append(node)\n",
    "        return parent_nodes\n",
    "\n",
    "\n",
    "    def child_nodes(self, node_name):\n",
    "        \"\"\"Obtain the child nodes of a given node.\"\"\"\n",
    "        child_nodes = []\n",
    "        node_name = self.get_node_name(node_name)\n",
    "        for node, adj in self[node_name].items():\n",
    "            if not adj[0][\"type\"].startswith(\"b-\") and \"relation\" not in adj[0][\"type\"]:\n",
    "                child_nodes.append(node)\n",
    "        return child_nodes\n",
    "\n",
    "    \n",
    "    def get_edge_type(self, node1, node2):\n",
    "        \"\"\"Get the type of the edge from node1 to node2.\"\"\"\n",
    "        node1 = self.get_node_name(node1)\n",
    "        node2 = self.get_node_name(node2)\n",
    "        return self.edges[(node1, node2, 0)][\"type\"]\n",
    "\n",
    "\n",
    "    def set_edge_type(self, node1, node2, type):\n",
    "        \"\"\"Set the type of the edge from node1 to node2.\"\"\"\n",
    "        node1 = self.get_node_name(node1)\n",
    "        node2 = self.get_node_name(node2)\n",
    "        self.edges[(node1, node2, 0)][\"type\"] = type\n",
    "        self.edges[(node2, node1, 0)][\"type\"] = \"b-{}\".format(type)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def get_attr_source(self, attr_name):\n",
    "        \"\"\"Get the source node of an attribute (either an input node, concept node or fun-out node)\"\"\"\n",
    "        attr_name = self.get_node_name(attr_name)\n",
    "        assert self.get_node_type(attr_name) == \"attr\"\n",
    "        parent_node = attr_name\n",
    "        while self.get_node_type(parent_node) not in [\"input\", \"fun-out\", \"concept\"]:\n",
    "            parent_node = self.parent_nodes(parent_node)\n",
    "            assert len(parent_node) == 1\n",
    "            parent_node = parent_node[0]\n",
    "        return parent_node\n",
    "\n",
    "\n",
    "    def get_path(self, source_node, target_node):\n",
    "        \"\"\"Get a path from the source_node to the target_node.\"\"\"\n",
    "        source_node = self.get_node_name(source_node)\n",
    "        target_node = self.get_node_name(target_node)\n",
    "        try:\n",
    "            path = nx.shortest_path(self, source_node, target_node)\n",
    "            is_connected = True\n",
    "        except:\n",
    "            path = []\n",
    "            is_connected = False\n",
    "        return is_connected, path\n",
    "\n",
    "\n",
    "    def get_path_to_output(self, node_name):\n",
    "        \"\"\"Get the path from the current operator to its output.\"\"\"\n",
    "        node_name = self.get_node_name(node_name)\n",
    "        path = [node_name]\n",
    "        while len(self.child_nodes(node_name)) > 0:\n",
    "            node_name = self.child_nodes(node_name)\n",
    "            if len(node_name) == 0:\n",
    "                return path\n",
    "            if \"inter\" in self[path[-1]][node_name[0]][0][\"type\"]:\n",
    "                return path\n",
    "            if len(node_name) > 1:\n",
    "                print(\"The node {} has more than 1 output. Choose {}.\".format(path[-1], node_name[0]))\n",
    "                path.append(node_name[0])\n",
    "            else:\n",
    "                path.append(node_name[0])\n",
    "            node_name = node_name[0]\n",
    "        return path\n",
    "\n",
    "\n",
    "    def get_to_outnode(self, node_name):\n",
    "        \"\"\"Get the path from the current operator to its output.\"\"\"\n",
    "        node_name = self.get_node_name(node_name)\n",
    "        if self.get_node_type(node_name) == \"fun-out\":\n",
    "            return node_name\n",
    "        while len(self.child_nodes(node_name)) > 0:\n",
    "            node_name_new = self.child_nodes(node_name)\n",
    "            if len(node_name_new) == 0:\n",
    "                return node_name\n",
    "            if 'intra-attr' in self[node_name][node_name_new[0]][0][\"type\"] or \"inter\" in self[node_name][node_name_new[0]][0][\"type\"]:\n",
    "                return node_name\n",
    "            node_name = node_name_new[0]\n",
    "        return node_name\n",
    "\n",
    "\n",
    "    def get_node_neighboring_inter(self, node):\n",
    "        \"\"\"Get the node within the operator or input_placeholder that connect to an inter-edge.\"\"\"\n",
    "        node = self.get_node_name(node)\n",
    "        child_nodes = self.child_nodes(node)\n",
    "        if len(child_nodes) == 0:\n",
    "            return None\n",
    "        for adj, edge in self[node].items():\n",
    "            if \"inter\" in edge[0][\"type\"]:\n",
    "                return node\n",
    "            else:\n",
    "                if not edge[0][\"type\"].startswith(\"b-\"):\n",
    "                    node_cand = self.get_node_neighboring_inter(adj)\n",
    "                    if node_cand is not None:\n",
    "                        return node_cand\n",
    "        return None\n",
    "\n",
    "\n",
    "    def get_ancestors(self, nodes, includes_self=False):\n",
    "        \"\"\"Get all the ancestors of the nodes given.\"\"\"\n",
    "        if not isinstance(nodes, list):\n",
    "            nodes = [nodes]\n",
    "        ancestors = []\n",
    "        forward_graph = self.core_forward_graph_shallow\n",
    "        for node in nodes:\n",
    "            node = self.get_node_name(node)\n",
    "            ancestors += nx.ancestors(forward_graph, node)\n",
    "            if includes_self:\n",
    "                ancestors.append(node)\n",
    "        return remove_duplicates(ancestors)\n",
    "\n",
    "\n",
    "    def get_descendants(self, nodes, includes_self=False):\n",
    "        \"\"\"Get all the descendants of the nodes given.\"\"\"\n",
    "        if not isinstance(nodes, list):\n",
    "            nodes = [nodes]\n",
    "        descendants = []\n",
    "        forward_graph = self.core_forward_graph_shallow\n",
    "        for node in nodes:\n",
    "            node = self.get_node_name(node)\n",
    "            descendants += nx.descendants(forward_graph, node)\n",
    "            if includes_self:\n",
    "                descendants.append(node)\n",
    "        return remove_duplicates(descendants)\n",
    "\n",
    "\n",
    "    def check_available(self, node_name):\n",
    "        \"\"\"Check if any the current node are connected to an inter-edge.\"\"\"\n",
    "        node_name = self.get_node_name(node_name)\n",
    "        assert self.get_node_type(node_name) in [\"attr\", \"input\", \"fun-out\"]\n",
    "        is_available = True\n",
    "        for neighbor_node, info in self[node_name].items():\n",
    "            if \"inter\" in info[0][\"type\"]:\n",
    "                is_available = False\n",
    "                break\n",
    "        return is_available\n",
    "\n",
    "\n",
    "    def check_available_recur(self, node_name):\n",
    "        \"\"\"Check if any the current node and its descendants are connected to an inter-edge.\"\"\"\n",
    "        if not self.check_available(node_name):\n",
    "            return False\n",
    "        for child_node in self.child_nodes(node_name):\n",
    "            if not self.check_available_recur(child_node):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "\n",
    "    def check_dangling(self, node_name):\n",
    "        \"\"\"Check if any of the current node, its ancestors and descendants are connected to an inter-edge.\"\"\"\n",
    "        node_name = self.get_node_name(node_name)\n",
    "        assert self.get_node_type(node_name) in [\"attr\", \"input\", \"fun-out\"]\n",
    "        if not self.check_available_recur(node_name):\n",
    "            return False\n",
    "        # Check all its ancestors:\n",
    "        if self.get_node_type(node_name) == \"attr\":\n",
    "            parent_name = node_name\n",
    "            while self.get_node_type(parent_name) in [\"attr\", \"input\", \"fun-out\"]:\n",
    "                if not self.check_available(parent_name):\n",
    "                    return False\n",
    "                if self.get_node_type(parent_name) in [\"input\", \"fun-out\"]:\n",
    "                    break\n",
    "                parent_name = self.parent_nodes(parent_name)\n",
    "                assert len(parent_name) == 1\n",
    "                parent_name = parent_name[0]\n",
    "        return True\n",
    "\n",
    "\n",
    "    ########################################\n",
    "    # Properties:\n",
    "    ########################################\n",
    "    def forward_graph(self, is_copy_module=True, global_attrs=None):\n",
    "        \"\"\"Get the forward graph.\"\"\"\n",
    "        if is_copy_module is False or global_attrs is not None:\n",
    "            G = self.copy_with_grad(is_copy_module=is_copy_module, global_attrs=global_attrs)\n",
    "        else:\n",
    "            G = self.copy()\n",
    "        backward_edges = []\n",
    "        for ni, no, data in G.edges(data=True):\n",
    "            if data[\"type\"].startswith(\"b-\"):\n",
    "                backward_edges.append((ni, no))\n",
    "        G.remove_edges_from(backward_edges)\n",
    "        return G\n",
    "\n",
    "\n",
    "    @property\n",
    "    def core_graph(self):\n",
    "        \"\"\"Get the graph without relation edges\"\"\"\n",
    "        G = self.copy()\n",
    "        relation_edges = []\n",
    "        for ni, no, data in G.edges(data=True):\n",
    "            if \"relation\" in data[\"type\"]:\n",
    "                relation_edges.append((ni, no))\n",
    "        G.remove_edges_from(relation_edges)\n",
    "        return G\n",
    "\n",
    "\n",
    "    @property\n",
    "    def core_forward_graph_shallow(self):\n",
    "        G = self.copy_shallow()\n",
    "        relation_edges = []\n",
    "        for ni, no, data in G.edges(data=True):\n",
    "            if \"relation\" in data[\"type\"] or data[\"type\"].startswith(\"b-\"):\n",
    "                relation_edges.append((ni, no))\n",
    "        G.remove_edges_from(relation_edges)\n",
    "        return G\n",
    "\n",
    "\n",
    "    @property\n",
    "    def obj_graph(self):\n",
    "        \"\"\"Obtain the graph only containing object nodes.\"\"\"\n",
    "        G = self.get_subgraph(self.obj_names, includes_root=False, includes_descendants=False)\n",
    "        return G\n",
    "\n",
    "\n",
    "    def get_graph(self, allowed_attr):\n",
    "        \"\"\"Get subgraph based on allowed_attr: all (includes all nodes) or obj (includes only object nodes).\"\"\"\n",
    "        if allowed_attr == \"all\":\n",
    "            return self\n",
    "        elif allowed_attr == \"obj\":\n",
    "            return self.obj_graph\n",
    "        else:\n",
    "            raise Exception(\"allowed_attr '{}' is not valid!\".format(allowed_attr))\n",
    "\n",
    "\n",
    "    @property\n",
    "    def backward_graph(self):\n",
    "        \"\"\"Get the backward graph.\"\"\"\n",
    "        G = self.copy()\n",
    "        forward_edges = []\n",
    "        for ni, no, data in G.edges(data=True):\n",
    "            if not data[\"type\"].startswith(\"b-\"):\n",
    "                forward_edges.append((ni, no))\n",
    "        G.remove_edges_from(forward_edges)\n",
    "        return G\n",
    "\n",
    "\n",
    "    def clean_graph(self, is_copy_module=True):\n",
    "        \"\"\"Draw a clean operator graph where the operaoters' input and output nodes connected\n",
    "        by an inter-edge is absorbed.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            G = self.copy_with_grad(is_copy_module=is_copy_module)\n",
    "        except:\n",
    "            assert isinstance(self, Concept)\n",
    "            return self\n",
    "        nodes_to_remove = []\n",
    "        edges_to_remove = []\n",
    "        for node1, node2, edge in G.forward_graph(is_copy_module=is_copy_module).edges(data=True):\n",
    "            if edge[\"type\"].startswith(\"inter\"):\n",
    "                if G.get_node_type(node2) == \"fun-in\":\n",
    "                    node2_parents = G.parent_nodes(node2)\n",
    "                    node2_child = G.child_nodes(node2)[0]\n",
    "                    assert G.get_node_type(node2_child) == \"self\"\n",
    "                    for node2_parent in node2_parents:\n",
    "                        edges_to_remove.append((node2_parent, node2))\n",
    "                        edges_to_remove.append((node2, node2_parent))\n",
    "                        G.add_edge(node2_parent, node2_child, type=self.edges[(node2_parent, node2, 0)][\"type\"])\n",
    "                    nodes_to_remove.append(node2)\n",
    "\n",
    "                if G.get_node_type(node1) == \"fun-out\":\n",
    "                    node1_children = G.child_nodes(node1)\n",
    "                    node1_parent = G.parent_nodes(node1)\n",
    "                    if len(node1_parent) > 0:\n",
    "                        node1_parent = node1_parent[0]\n",
    "                        if G.get_node_type(node1_parent) == \"self\":\n",
    "                            for node1_child in node1_children:\n",
    "                                edges_to_remove.append((node1, node1_child))\n",
    "                                edges_to_remove.append((node1_child, node1))\n",
    "                                G.add_edge(node1_parent, node1_child,\n",
    "                                           type=self.edges[(node1, node1_child, 0)][\"type\"] if (node1, node1_child, 0) in self.edges else edge[\"type\"])\n",
    "                            nodes_to_remove.append(node1)\n",
    "            elif edge[\"type\"] == \"intra-attr\" and G.get_node_type(node1) == \"fun-out\":\n",
    "                # If it is an get-attr node from a fun-out node:\n",
    "                node1_children = G.child_nodes(node1)\n",
    "                node1_parents = G.parent_nodes(node1)\n",
    "                assert len(node1_parents) == 1\n",
    "                node1_parent = node1_parents[0]\n",
    "                for node1_child in node1_children:\n",
    "                    edges_to_remove.append((node1, node1_child))\n",
    "                    edges_to_remove.append((node1_child, node1))\n",
    "                    G.add_edge(node1_parent, node1_child,\n",
    "                               type=self.edges[(node1, node1_child, 0)][\"type\"] if (node1, node1_child, 0) in self.edges else edge[\"type\"])\n",
    "                nodes_to_remove.append(node1)\n",
    "\n",
    "        G.remove_nodes_from(nodes_to_remove)\n",
    "        G.remove_edges_from(edges_to_remove)\n",
    "        G = G.forward_graph(is_copy_module=is_copy_module) if not nx.is_directed_acyclic_graph(G) else G\n",
    "        return G\n",
    "\n",
    "\n",
    "    @property\n",
    "    def topological_sort(self):\n",
    "        \"\"\"Return a list of nodes that are topologically sorted with the full graph.\"\"\"\n",
    "        return list(nx.lexicographical_topological_sort(self.core_forward_graph_shallow))\n",
    "\n",
    "\n",
    "    @property\n",
    "    def node_link_data(self):\n",
    "        \"\"\"Obtain node and link data for web app.\"\"\"\n",
    "        G = self.copy().forward_graph()\n",
    "        for n in G:\n",
    "            G.nodes[n]['id'] = n\n",
    "        data = {\"graph_type\": G.__class__.__name__}\n",
    "        data.update(json_graph.node_link_data(G))\n",
    "        node_reverse_dict = {}\n",
    "\n",
    "        # Nodes:\n",
    "        for i, node in enumerate(data[\"nodes\"]):\n",
    "            node.pop(\"repr\", None)\n",
    "            node.pop(\"fun\", None)\n",
    "            node.pop(\"value\",None)\n",
    "            node_reverse_dict[node[\"id\"]] = i\n",
    "#             if \"value\" in node:\n",
    "#                 if not isinstance(node[\"value\"], BaseGraph):\n",
    "#                     node.pop(\"value\", None)\n",
    "#                 else:\n",
    "#                     node[\"value\"] = node[\"value\"].node_link_data\n",
    "\n",
    "        # Links:\n",
    "        for link in data[\"links\"]:\n",
    "            link[\"source\"] = node_reverse_dict[link[\"source\"]]\n",
    "            link[\"target\"] = node_reverse_dict[link[\"target\"]]\n",
    "        \n",
    "        # Remove other redundent information:\n",
    "        data.pop(\"graph\", None)\n",
    "        data.pop(\"directed\", None)\n",
    "        data.pop(\"multigraph\", None)\n",
    "        return data\n",
    "\n",
    "\n",
    "    ########################################\n",
    "    # Visualization:\n",
    "    ########################################\n",
    "    def draw(self, is_clean_graph=True, layout=\"spring\", filename=None, **kwargs):\n",
    "        \"\"\"Visualize the current graph.\"\"\"\n",
    "        # Only plot the acyclic edges, and by default the forward graph:\n",
    "        if is_clean_graph:\n",
    "            G = self.clean_graph(is_copy_module=False)\n",
    "        else:\n",
    "            G = self.forward_graph(is_copy_module=False) if not nx.is_directed_acyclic_graph(self) else self\n",
    "        # Nodes:\n",
    "        node_color = []\n",
    "        node_sizes = []\n",
    "        for node, info in G.nodes(data=True):\n",
    "            node_size = 1200\n",
    "            if info[\"type\"] == \"input\":\n",
    "                node_color.append(\"#58509d\")\n",
    "            elif info[\"type\"] == \"concept\":\n",
    "                node_color.append(\"#C515E8\")\n",
    "                if self.get_node_value(node) is not None:\n",
    "                    node_size = 1800\n",
    "            elif info[\"type\"] == \"self\":\n",
    "                node_color.append(\"#1f78b4\")\n",
    "            elif info[\"type\"] == \"attr\":\n",
    "                node_color.append(\"#EB8F8F\")\n",
    "            elif info[\"type\"] == \"obj\":\n",
    "                node_color.append(\"#C31B37\")\n",
    "            elif info[\"type\"].startswith(\"fun\"):\n",
    "                if info[\"type\"] == \"fun-out\":\n",
    "                    node_color.append(\"orange\")\n",
    "                else:\n",
    "                    node_color.append(\"g\")\n",
    "            else:\n",
    "                raise\n",
    "            node_sizes.append(node_size)\n",
    "\n",
    "        # Edges:\n",
    "        edge_color = []\n",
    "        for ni, no, data in G.edges(data=True):\n",
    "            if \"intra-relation\" in data[\"type\"]:\n",
    "                edge_color.append(\"purple\")\n",
    "            elif \"inter-concept\" in data[\"type\"]:\n",
    "                edge_color.append(\"#E644F0\")\n",
    "            elif \"intra\" in data[\"type\"]:\n",
    "                edge_color.append(\"k\")\n",
    "            elif \"inter\" in data[\"type\"]:\n",
    "                if data[\"type\"].endswith(\"inter-input\"):\n",
    "                    edge_color.append(\"brown\")\n",
    "                elif data[\"type\"].endswith(\"inter-criteria\"):\n",
    "                    edge_color.append(\"c\")\n",
    "                else:\n",
    "                    raise\n",
    "            elif data[\"type\"].endswith(\"get-attr\"):\n",
    "                    edge_color.append(\"#E815DA\")\n",
    "            else:\n",
    "                raise\n",
    "        # Set up layout:\n",
    "        if layout == \"planar\":\n",
    "            pos = nx.planar_layout(G)\n",
    "        elif layout == \"spring\":\n",
    "            pos = nx.spring_layout(G)\n",
    "        elif layout == \"spectral\":\n",
    "            pos = nx.spectral_layout(G)\n",
    "        elif layout == \"spiral\":\n",
    "            pos = nx.spiral_layout(G)\n",
    "        elif layout == \"shell\":\n",
    "            pos = nx.shell_layout(G)\n",
    "        elif layout == \"random\":\n",
    "            pos = nx.random_layout(G)\n",
    "        elif layout == \"kk\":\n",
    "            pos = nx.kamada_kawai_layout(G)\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "        # Draw:\n",
    "        nx.draw(G, with_labels=True, font_size=10, pos=pos,\n",
    "                node_color=node_color, node_size=node_sizes,\n",
    "                edge_color=edge_color,\n",
    "               )\n",
    "        if filename is not None:\n",
    "            plt.savefig(filename, bbox_inches=\"tight\", **kwargs)\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "            if isinstance(self, Concept) and self.name is not None:\n",
    "                value = self.get_node_value(self.name)\n",
    "                if value is not None:\n",
    "                    if len(value.shape) == 2:\n",
    "                        visualize_matrices([value])\n",
    "                    elif len(value.shape) == 3:\n",
    "                        value_T = deepcopy(value).permute(1,2,0)\n",
    "                        plt.imshow(to_np_array(value_T).astype(int))\n",
    "                        plt.show()\n",
    "\n",
    "            # If certain node's content is a graph, recursively draw it:\n",
    "            for node in self.nodes:\n",
    "                node_content = self.get_node_content(node)\n",
    "                if isinstance(node_content, BaseGraph):\n",
    "                    print(\"\\nDrawing the content of node '{}', which is {}:\".format(node, node_content))\n",
    "                    node_content.draw()\n",
    "\n",
    "            # Draw selectors:\n",
    "            if isinstance(self, Graph):\n",
    "                for op_name in self.operators:\n",
    "                    selector_dict = self.get_selector(op_name)\n",
    "                    if len(selector_dict) > 0:\n",
    "                        for key, selector in selector_dict.items():\n",
    "                            print(\"\\nOp-in '{}' has a selector, with inplace={}:\".format(key, self.get_inplace(key)))\n",
    "                            selector.draw()\n",
    "        return self\n",
    "\n",
    "\n",
    "    def visualize_results(self, results):\n",
    "        for node, value in results.items():\n",
    "            if len(value.shape) == 2:\n",
    "                print(\"{}:\".format(node))\n",
    "                visualize_matrices([value])\n",
    "            elif len(value.shape) == 3:\n",
    "                print(\"{}:\".format(node))\n",
    "                visualize_matrices(value)\n",
    "            else:\n",
    "                print(\"{}: {}\".format(node, value))\n",
    "\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"Save the graph instance to a file.\"\"\"\n",
    "        pickle.dump(self, open(filename, \"wb\"))\n",
    "        return self\n",
    "\n",
    "\n",
    "def load_graph(filename):\n",
    "    \"\"\"Load the graph instance.\"\"\"\n",
    "    return pickle.load(open(filename, \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Graph:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Graph(BaseGraph):\n",
    "    \"\"\"Implement the Graph class where operators, input and programs are its special cases.\n",
    "\n",
    "    A operator is a node with (optional) attributes and (optional) methods. \n",
    "        Attributes and input, output of the methods can be other operators, \n",
    "        and are modeled as nodes.\n",
    "    An input is a Graph with nodes.\n",
    "    A program is a directed acyclic graph (DAG) with overall input nodes and output nodes. \n",
    "        The input nodes can be grounded (connected) to the input graph, and the output \n",
    "        node(s) generates the output of the program. In case where some of the intermediate \n",
    "        input nodes are not grounded, the forward mode of the program will generate multiple \n",
    "        outputs corresponding to the combined ranges of the ungrounded input nodes.\n",
    "    It is inherited from MultiDiGraph for graph manipulation and torch.nn.Module for \n",
    "        gradient-based learning.\n",
    "    \"\"\"\n",
    "    def __init__(self, G=None, **kwargs):\n",
    "        \"\"\"Components of a Graph instance:\n",
    "\n",
    "        (1) Nodes and edges: a node can be a operator body node, an attribute node for a operator, \n",
    "            an input or output node of a operator. Also there is input_placeholder_node.\n",
    "            The content of a node can be a placeholder for concept, a constant concept, a function, or a Graph().\n",
    "        (2) self.operators: contains all the names of the operators, and possibly name of the ground-node.\n",
    "        (3) self.goals: set of operators that ground the input nodes based on criteria.\n",
    "        (4) self.funs: A list of names for PyTorch learnable models.\n",
    "        (5) self.ground_node_name: name of the ground-node. Default as None.\n",
    "        (6) A set of variables for the ungrounded input nodes.\n",
    "\n",
    "        For each node (or Graph), its input and output are OrderedDict() of Concept() instances.\n",
    "        \"\"\"\n",
    "        self.operators = []\n",
    "        self.goals = []\n",
    "        self.funs = []\n",
    "        self.ground_node_name = None\n",
    "        super(Graph, self).__init__(G=G, **kwargs)\n",
    "        if \"name\" in kwargs:\n",
    "            self.add_operator_def(definition=kwargs)\n",
    "\n",
    "        if G is not None:\n",
    "            self.operators = deepcopy(G.operators)\n",
    "            self.goals = deepcopy(G.goals)\n",
    "            self.funs = G.funs\n",
    "            self.ground_node_name = G.ground_node_name\n",
    "            # Set up PyTorch variables and parameters for subgraph:\n",
    "            for variable_name, tensor in G.get_variables().items():\n",
    "                setattr(self, variable_name, nn.Parameter(tensor))\n",
    "            for fun_name in G.funs:\n",
    "                setattr(self, \"fun_{}\".format(fun_name), getattr(G, \"fun_{}\".format(fun_name)))\n",
    "\n",
    "    \n",
    "    def add_operator_def(self, definition):\n",
    "        \"\"\"Initialize the operator node from the operator definition dictionary.\"\"\"\n",
    "        name = definition[\"name\"]\n",
    "        if \"value\" in definition or \"attr\" in definition or \"forward\" in definition:\n",
    "            self.operators.append(name)\n",
    "            # Add body node:\n",
    "            if \"value\" in definition:\n",
    "                # The node itself is a value node (in case of input):\n",
    "                self.add_node(name, value=definition[\"value\"], type=\"self\")\n",
    "            else:\n",
    "                # The node is a operator node:\n",
    "                self.add_node(name, type=\"self\")\n",
    "\n",
    "            if \"repr\" in definition:\n",
    "                self.nodes[name][\"repr\"] = nn.Parameter(definition[\"repr\"])\n",
    "\n",
    "            if \"forward\" in definition:\n",
    "                fun = definition[\"forward\"][\"fun\"]\n",
    "                if isinstance(fun, nn.Module) and not isinstance(fun, BaseGraph):\n",
    "                    self.funs.append(name)\n",
    "                    setattr(self, \"fun_{}\".format(name), fun)\n",
    "                    self.add_node(name, value=getattr(self, \"fun_{}\".format(name)))\n",
    "                else:\n",
    "                    self.add_node(name, value=fun)\n",
    "\n",
    "                # Check if it is a criteria node:\n",
    "                if name.startswith(\"Cri\"):\n",
    "                    self.goals.append(name)\n",
    "                for i, arg in enumerate(definition[\"forward\"][\"args\"]):\n",
    "                    self.add_node(\"{}-{}:{}\".format(name, i + 1, str(arg.mode).split(\"-\")[0]), value=arg, type=\"fun-in\")\n",
    "                    self.add_edge(\"{}-{}:{}\".format(name, i + 1, str(arg.mode).split(\"-\")[0]), name, type=\"intra\")\n",
    "                    self.add_edge(name, \"{}-{}:{}\".format(name, i + 1, str(arg.mode).split(\"-\")[0]), type=\"b-intra\")\n",
    "                self.add_node(\"{}-o:{}\".format(name, str(definition[\"forward\"][\"output\"].mode).split(\"-\")[0]), \n",
    "                              value=definition[\"forward\"][\"output\"], type=\"fun-out\")\n",
    "                self.add_edge(\"{}-o:{}\".format(name, str(definition[\"forward\"][\"output\"].mode).split(\"-\")[0]), name, type=\"b-intra\")\n",
    "                self.add_edge(name, \"{}-o:{}\".format(name, str(definition[\"forward\"][\"output\"].mode).split(\"-\")[0]), type=\"intra\")\n",
    "\n",
    "                # Create and connect input placeholder nodes:\n",
    "                jj = 1\n",
    "                input_type_dict = {}\n",
    "                for placeholder in definition[\"forward\"][\"args\"]:\n",
    "                    if Placeholder(DEFAULT_OBJ_TYPE).accepts(placeholder) or \\\n",
    "                    Placeholder(DEFAULT_BODY_TYPE).accepts(placeholder):\n",
    "                        input_type_dict[jj] = placeholder.mode\n",
    "                        jj += 1\n",
    "                self.connect_input_placeholder_nodes(input_type_dict)\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        repr_str = self.graph[\"name\"] if \"name\" in self.graph else \"Graph\"\n",
    "        # Composing content string:\n",
    "        content_str = \"\"\n",
    "        operator_list = []\n",
    "        for operator_name in self.operators:\n",
    "            if operator_name not in self.goals:\n",
    "                operator_list.append(operator_name)\n",
    "        if len(operator_list) > 0:\n",
    "            content_str += \"operators={}, \".format(operator_list)\n",
    "        if len(self.goals) > 0:\n",
    "            content_str += \"goals={}, \".format(self.goals)\n",
    "        if len(self.input_placeholder_nodes) > 0:\n",
    "            input_str = \"\"\n",
    "            for node in self.input_placeholder_nodes:\n",
    "                tip_node = self.get_node_neighboring_inter(node)\n",
    "                if tip_node is None:\n",
    "                    tip_node = node\n",
    "                input_str += \"{}, \".format(tip_node)\n",
    "            content_str += \"inputs=[{}], \".format(input_str[:-2])\n",
    "        if len(self.constant_concept_nodes) > 0:\n",
    "            content_str += \"concepts={}, \".format(self.constant_concept_nodes)\n",
    "        return '{}({})'.format(repr_str, content_str[:-2])\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        if IS_VIEW and len(self.nodes) > 0:\n",
    "            self.draw()\n",
    "        return self.__str__()\n",
    "    \n",
    "    \n",
    "    def __hash__(self):\n",
    "        return persist_hash(self.get_string_repr())\n",
    "    \n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.get_string_repr() == other.get_string_repr()\n",
    "    \n",
    "    \n",
    "    def get_string_repr(self):\n",
    "        \"\"\"Get 1D string representation of the operator graph\"\"\"\n",
    "        combined_dict = OrderedDict()\n",
    "        if self.name in self.nodes:\n",
    "            combined_dict[self.name + '_value'] = tensor_to_string(self.get_node_value(self.name)) \n",
    "        else:\n",
    "            combined_dict[self.name + '_value'] = \"None\"\n",
    "        for node_name in self.topological_sort:\n",
    "            node_content = self.get_node_content(node_name)\n",
    "            if isinstance(self.get_node_content(node_name), Placeholder):\n",
    "                combined_dict[node_name + '_inplace'] = node_content.get_inplace()\n",
    "                combined_dict[node_name + '_selector'] = str(node_content.get_selector())\n",
    "                combined_dict[node_name + '_value'] = tensor_to_string(self.get_node_value(node_name)) \n",
    "            elif isinstance(self.get_node_content(node_name), Concept) or isinstance(self.get_node_content(node_name), Graph):\n",
    "                # Both Concept and Graph have get_string_repr implemented\n",
    "                combined_dict[node_name] = node_content.get_string_repr()\n",
    "        string = \"\"\n",
    "        for key, item in combined_dict.items():\n",
    "            string += \"%{}!{}\".format(key, item)\n",
    "        return string\n",
    "\n",
    "\n",
    "    def set_ground_node(self, node_name):\n",
    "        \"\"\"Setting up the name for ground node to expect.\"\"\"\n",
    "        self.ground_node_name = node_name\n",
    "        return self\n",
    "\n",
    "\n",
    "    def set_selector(self, selector, node_name=None):\n",
    "        \"\"\"Set selector on a fun-in node.\"\"\"\n",
    "        node_name = self.get_node_name(node_name)\n",
    "        node_type = self.get_node_type(node_name)\n",
    "        if node_type == \"self\":\n",
    "            node_name = self.parent_nodes(node_name)[0]  # Get the first in_node of the operator\n",
    "        assert self.get_node_type(node_name) == \"fun-in\", \"Selector must be put in 'fun-in' node!\"\n",
    "        placeholder = self.get_node_content(node_name)\n",
    "        assert isinstance(placeholder, Placeholder)\n",
    "        placeholder.set_selector(selector)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def get_selector(self, node_name=None):\n",
    "        \"\"\"Get selector on a fun-in node.\"\"\"\n",
    "        node_name = self.get_node_name(node_name)\n",
    "        node_type = self.get_node_type(node_name)\n",
    "        if node_type == \"self\":\n",
    "            node_names = self.parent_nodes(node_name)  # Get the first in_node of the operator\n",
    "        else:\n",
    "            node_names = [node_name]\n",
    "        selector_dict = {}\n",
    "        for node_name in node_names:\n",
    "            assert self.get_node_type(node_name) == \"fun-in\"\n",
    "            placeholder = self.get_node_content(node_name)\n",
    "            assert isinstance(placeholder, Placeholder)\n",
    "            selector = placeholder.get_selector()\n",
    "            if selector is not None:\n",
    "                selector_dict[node_name] = selector\n",
    "        return selector_dict\n",
    "\n",
    "\n",
    "    def get_selectors(self):\n",
    "        \"\"\"Return a dictionary of {op_name: selectors}.\"\"\"\n",
    "        selectors_dict = {}\n",
    "        for op_name in self.operators:\n",
    "            selector_dict = self.get_selector(op_name)\n",
    "            selectors_dict.update(selector_dict)\n",
    "        return selectors_dict\n",
    "\n",
    "\n",
    "    def set_ebm_dict(self, ebm_dict):\n",
    "        \"\"\"Set all the selector's ebm_dict attribute to the given ebm_dict.\"\"\"\n",
    "        from reasoning.experiments.models import to_ebm_models\n",
    "        ebm_dict = to_ebm_models(ebm_dict)\n",
    "        selectors = self.get_selectors()\n",
    "        for key in selectors:\n",
    "            selectors[key].ebm_dict = ebm_dict\n",
    "        return self\n",
    "\n",
    "\n",
    "    def remove_ebm_dict(self):\n",
    "        \"\"\"Remove attribute of ebm_dict from all the selectors.\"\"\"\n",
    "        selectors = self.get_selectors()\n",
    "        for key in selectors:\n",
    "            delattr(selectors[key], \"ebm_dict\")\n",
    "        return self\n",
    "\n",
    "\n",
    "    def set_cache_forward(self, cache_forward):\n",
    "        \"\"\"Set the cache_forward attribute for each selector in the graph.\"\"\"\n",
    "        selectors = self.get_selectors()\n",
    "        for key in selectors:\n",
    "            selectors[key].set_cache_forward(cache_forward)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def add_op_to_selector(self, op_to_add, op_in, *opsc, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            op_to_add: can either be a concept or a relation operator\n",
    "            op_in: fun-in nodes or self nodes that has only one fun-in\n",
    "            *opsc: additional nodes in the selector to connect to/merge with.\n",
    "        \"\"\"\n",
    "        op_in = self.get_node_name(op_in)\n",
    "        op_in_type = self.get_node_type(op_in)\n",
    "        if op_in_type == \"self\":\n",
    "            node_names = self.parent_nodes(op_in)  # Get the first in_node of the operator\n",
    "            assert len(node_names) == 1, \"The op_in given must be a fun-in node or a self node that has only one fun-in node.\"\n",
    "            op_in = node_names[0]\n",
    "        assert self.get_node_type(op_in) == \"fun-in\"\n",
    "        placeholder = self.get_node_content(op_in)\n",
    "        assert isinstance(placeholder, Placeholder)\n",
    "        selector = placeholder.get_selector()\n",
    "        if selector is None:\n",
    "            # The selector is not yet existing:\n",
    "            num_colors = 10\n",
    "            if isinstance(op_to_add, Concept):\n",
    "                # op_to_add is a concept:\n",
    "                selector = Concept_Pattern(\n",
    "                    name=None,\n",
    "                    value=Placeholder(Tensor(dtype=\"cat\", range=range(num_colors))),\n",
    "                    attr={\"obj_0\": Placeholder(op_to_add.name)},\n",
    "                    is_all_obj=True,\n",
    "                    is_ebm=True,\n",
    "                    is_default_ebm=False,\n",
    "                    ebm_dict=kwargs[\"ebm_dict\"],\n",
    "                    CONCEPTS=kwargs[\"CONCEPTS\"],\n",
    "                    OPERATORS=kwargs[\"OPERATORS\"],\n",
    "                    device=self.device,\n",
    "                    cache_forward=kwargs[\"cache_forward\"],\n",
    "                    in_channels=kwargs[\"in_channels\"],\n",
    "                    # EBM specific:\n",
    "                    z_mode=kwargs[\"z_mode\"],\n",
    "                    z_first=kwargs[\"z_first\"],\n",
    "                    z_dim=kwargs[\"z_dim\"],\n",
    "                    w_type=kwargs[\"w_type\"],\n",
    "                    mask_mode=kwargs[\"mask_mode\"],\n",
    "                    aggr_mode=kwargs[\"aggr_mode\"],\n",
    "                    pos_embed_mode=kwargs[\"pos_embed_mode\"],\n",
    "                    is_ebm_share_param=kwargs[\"is_ebm_share_param\"],\n",
    "                    is_relation_z=kwargs[\"is_relation_z\"],\n",
    "                    img_dims=kwargs[\"img_dims\"],\n",
    "                    is_spec_norm=kwargs[\"is_spec_norm\"],\n",
    "                    # Selector specific:\n",
    "                    channel_coef=kwargs[\"selector_channel_coef\"],\n",
    "                    empty_coef=kwargs[\"selector_empty_coef\"],\n",
    "                    obj_coef=kwargs[\"selector_obj_coef\"],\n",
    "                    mutual_exclusive_coef=kwargs[\"selector_mutual_exclusive_coef\"],\n",
    "                    pixel_entropy_coef=kwargs[\"selector_pixel_entropy_coef\"],\n",
    "                    pixel_gm_coef=kwargs[\"selector_pixel_gm_coef\"],\n",
    "                    iou_batch_consistency_coef=kwargs[\"selector_iou_batch_consistency_coef\"],\n",
    "                    iou_concept_repel_coef=kwargs[\"selector_iou_concept_repel_coef\"],\n",
    "                    iou_relation_repel_coef=kwargs[\"selector_iou_relation_repel_coef\"],\n",
    "                    iou_relation_overlap_coef=kwargs[\"selector_iou_relation_overlap_coef\"],\n",
    "                    iou_attract_coef=kwargs[\"selector_iou_attract_coef\"],\n",
    "                    connected_coef=kwargs[\"selector_connected_coef\"],\n",
    "                    SGLD_is_anneal=kwargs[\"selector_SGLD_is_anneal\"],\n",
    "                    SGLD_is_penalize_lower=kwargs[\"selector_SGLD_is_penalize_lower\"],\n",
    "                    SGLD_mutual_exclusive_coef=kwargs[\"selector_SGLD_mutual_exclusive_coef\"],\n",
    "                    SGLD_pixel_entropy_coef=kwargs[\"selector_SGLD_pixel_entropy_coef\"],\n",
    "                    SGLD_pixel_gm_coef=kwargs[\"selector_SGLD_pixel_gm_coef\"],\n",
    "                    SGLD_iou_batch_consistency_coef=kwargs[\"selector_SGLD_iou_batch_consistency_coef\"],\n",
    "                    SGLD_iou_concept_repel_coef=kwargs[\"selector_SGLD_iou_concept_repel_coef\"],\n",
    "                    SGLD_iou_relation_repel_coef=kwargs[\"selector_SGLD_iou_relation_repel_coef\"],\n",
    "                    SGLD_iou_relation_overlap_coef=kwargs[\"selector_SGLD_iou_relation_overlap_coef\"],\n",
    "                    SGLD_iou_attract_coef=kwargs[\"selector_SGLD_iou_attract_coef\"],\n",
    "                    lambd_start=kwargs[\"selector_lambd_start\"],\n",
    "                    lambd=kwargs[\"selector_lambd\"],\n",
    "                    image_value_range=kwargs[\"selector_image_value_range\"],\n",
    "                    w_init_type=kwargs[\"selector_w_init_type\"],\n",
    "                    indiv_sample=kwargs[\"selector_indiv_sample\"],\n",
    "                    step_size=kwargs[\"selector_step_size\"],\n",
    "                    step_size_img=kwargs[\"selector_step_size_img\"],\n",
    "                    step_size_z=kwargs[\"selector_step_size_z\"],\n",
    "                    step_size_zgnn=kwargs[\"selector_step_size_zgnn\"],\n",
    "                    step_size_wtarget=kwargs[\"selector_step_size_wtarget\"],\n",
    "                    connected_num_samples=kwargs[\"selector_connected_num_samples\"],\n",
    "                )\n",
    "                placeholder.set_selector(selector)\n",
    "\n",
    "            elif isinstance(op_to_add, Graph):\n",
    "                # op_to_add is a relation operator:\n",
    "                selector = Concept_Pattern(\n",
    "                    name=None,\n",
    "                    value=Placeholder(Tensor(dtype=\"cat\", range=range(num_colors))),\n",
    "                    attr={\n",
    "                        \"obj_0\": Placeholder(DEFAULT_OBJ_TYPE),\n",
    "                        \"obj_1\": Placeholder(DEFAULT_OBJ_TYPE),\n",
    "                    },\n",
    "                    re={(\"obj_0\", \"obj_1\"): op_to_add.name},\n",
    "                    is_all_obj=True,\n",
    "                    is_ebm=True,\n",
    "                    is_default_ebm=False,\n",
    "                    ebm_dict=kwargs[\"ebm_dict\"],\n",
    "                    CONCEPTS=kwargs[\"CONCEPTS\"],\n",
    "                    OPERATORS=kwargs[\"OPERATORS\"],\n",
    "                    device=self.device,\n",
    "                    cache_forward=kwargs[\"cache_forward\"],\n",
    "                    in_channels=kwargs[\"in_channels\"],\n",
    "                    # EBM specific:\n",
    "                    z_mode=kwargs[\"z_mode\"],\n",
    "                    z_first=kwargs[\"z_first\"],\n",
    "                    z_dim=kwargs[\"z_dim\"],\n",
    "                    w_type=kwargs[\"w_type\"],\n",
    "                    mask_mode=kwargs[\"mask_mode\"],\n",
    "                    aggr_mode=kwargs[\"aggr_mode\"],\n",
    "                    pos_embed_mode=kwargs[\"pos_embed_mode\"],\n",
    "                    is_ebm_share_param=kwargs[\"is_ebm_share_param\"],\n",
    "                    is_relation_z=kwargs[\"is_relation_z\"],\n",
    "                    img_dims=kwargs[\"img_dims\"],\n",
    "                    is_spec_norm=kwargs[\"is_spec_norm\"],\n",
    "                    # Selector specific:\n",
    "                    channel_coef=kwargs[\"selector_channel_coef\"],\n",
    "                    empty_coef=kwargs[\"selector_empty_coef\"],\n",
    "                    obj_coef=kwargs[\"selector_obj_coef\"],\n",
    "                    mutual_exclusive_coef=kwargs[\"selector_mutual_exclusive_coef\"],\n",
    "                    pixel_entropy_coef=kwargs[\"selector_pixel_entropy_coef\"],\n",
    "                    pixel_gm_coef=kwargs[\"selector_pixel_gm_coef\"],\n",
    "                    iou_batch_consistency_coef=kwargs[\"selector_iou_batch_consistency_coef\"],\n",
    "                    iou_concept_repel_coef=kwargs[\"selector_iou_concept_repel_coef\"],\n",
    "                    iou_relation_repel_coef=kwargs[\"selector_iou_relation_repel_coef\"],\n",
    "                    iou_relation_overlap_coef=kwargs[\"selector_iou_relation_overlap_coef\"],\n",
    "                    iou_attract_coef=kwargs[\"selector_iou_attract_coef\"],\n",
    "                    connected_coef=kwargs[\"selector_connected_coef\"],\n",
    "                    SGLD_is_anneal=kwargs[\"selector_SGLD_is_anneal\"],\n",
    "                    SGLD_is_penalize_lower=kwargs[\"selector_SGLD_is_penalize_lower\"],\n",
    "                    SGLD_mutual_exclusive_coef=kwargs[\"selector_SGLD_mutual_exclusive_coef\"],\n",
    "                    SGLD_pixel_entropy_coef=kwargs[\"selector_SGLD_pixel_entropy_coef\"],\n",
    "                    SGLD_pixel_gm_coef=kwargs[\"selector_SGLD_pixel_gm_coef\"],\n",
    "                    SGLD_iou_batch_consistency_coef=kwargs[\"selector_SGLD_iou_batch_consistency_coef\"],\n",
    "                    SGLD_iou_concept_repel_coef=kwargs[\"selector_SGLD_iou_concept_repel_coef\"],\n",
    "                    SGLD_iou_relation_repel_coef=kwargs[\"selector_SGLD_iou_relation_repel_coef\"],\n",
    "                    SGLD_iou_relation_overlap_coef=kwargs[\"selector_SGLD_iou_relation_overlap_coef\"],\n",
    "                    SGLD_iou_attract_coef=kwargs[\"selector_SGLD_iou_attract_coef\"],\n",
    "                    lambd_start=kwargs[\"selector_lambd_start\"],\n",
    "                    lambd=kwargs[\"selector_lambd\"],\n",
    "                    image_value_range=kwargs[\"selector_image_value_range\"],\n",
    "                    w_init_type=kwargs[\"selector_w_init_type\"],\n",
    "                    indiv_sample=kwargs[\"selector_indiv_sample\"],\n",
    "                    step_size=kwargs[\"selector_step_size\"],\n",
    "                    step_size_img=kwargs[\"selector_step_size_img\"],\n",
    "                    step_size_z=kwargs[\"selector_step_size_z\"],\n",
    "                    step_size_zgnn=kwargs[\"selector_step_size_zgnn\"],\n",
    "                    step_size_wtarget=kwargs[\"selector_step_size_wtarget\"],\n",
    "                    connected_num_samples=kwargs[\"selector_connected_num_samples\"],\n",
    "                )\n",
    "                placeholder.set_selector(selector)\n",
    "            else:\n",
    "                raise Exception(\"op_to_add {} must be a Concept or a Graph class.\".format(op_to_add))\n",
    "        else:\n",
    "            if isinstance(op_to_add, Concept):\n",
    "                selector.add_obj(op_to_add, *opsc)\n",
    "            elif isinstance(op_to_add, Graph):\n",
    "                selector.add_relation_manual(op_to_add.name, *opsc)\n",
    "            else:\n",
    "                raise Exception(\"op_to_add {} must be a Concept or a Graph class.\".format(op_to_add))\n",
    "        return self\n",
    "\n",
    "\n",
    "    def set_inplace(self, inplace, node_name=None):\n",
    "        \"\"\"Set 'inplace' attribute on a fun-in node.\"\"\"\n",
    "        node_name = self.get_node_name(node_name)\n",
    "        node_type = self.get_node_type(node_name)\n",
    "        if node_type == \"self\":\n",
    "            node_name = self.parent_nodes(node_name)[0]  # Get the first in_node of the operator\n",
    "        assert self.get_node_type(node_name) == \"fun-in\", \"Selector must be put in 'fun-in' node!\"\n",
    "        placeholder = self.get_node_content(node_name)\n",
    "        assert isinstance(placeholder, Placeholder)\n",
    "        placeholder.set_inplace(inplace)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def get_inplace(self, node_name=None):\n",
    "        \"\"\"Get 'inplace' attribute on a fun-in node.\"\"\"\n",
    "        node_name = self.get_node_name(node_name)\n",
    "        node_type = self.get_node_type(node_name)\n",
    "        if node_type == \"self\":\n",
    "            node_name = self.parent_nodes(node_name)[0]  # Get the first in_node of the operator\n",
    "        assert self.get_node_type(node_name) == \"fun-in\", \"Selector must be put in 'fun-in' node!\"\n",
    "        placeholder = self.get_node_content(node_name)\n",
    "        assert isinstance(placeholder, Placeholder)\n",
    "        return placeholder.get_inplace()\n",
    "\n",
    "\n",
    "    ########################################\n",
    "    # Forward, loss and infer functions:\n",
    "    ########################################\n",
    "    def forward(self, *inputs, **kwargs):\n",
    "        \"\"\"Perform the forward function on the graph, considering all goal nodes.\"\"\"\n",
    "        # No goal_optm nodes:\n",
    "        if len(self.goals_optm) == 0:\n",
    "            return self.forward_simple(*inputs, **kwargs)\n",
    "\n",
    "        # Get the input keys:\n",
    "        inputs, input_keys = broadcast_inputs(inputs)\n",
    "\n",
    "        # Build graph combining main graph and goal nodes:\n",
    "        G, (goal_ctrl_nodes, goal_output_nodes) = self.incorporate_goal_subgraph()\n",
    "\n",
    "        # Prune the parts that are after the latest goal nodes:\n",
    "        G.preserve_subgraph(goal_output_nodes)\n",
    "\n",
    "        # Define loss_fun:\n",
    "        shapes = [CONCEPTS[c.split(\":\")[1]].get_node_content(c.split(\":\")[1]).mode.shape for c in goal_ctrl_nodes]\n",
    "        score_mode = kwargs[\"score_mode\"] if \"score_mode\" in kwargs else \"prod\"\n",
    "        kwargs2 = deepcopy(kwargs)\n",
    "        kwargs2[\"isplot\"] = False\n",
    "\n",
    "        def loss_fun(search_variable_values):\n",
    "            G.set_variable_values(goal_ctrl_nodes, search_variable_values, shapes, input_keys)\n",
    "            results = G.forward_simple(*inputs, is_output_all=True, is_output_tensor=True, **kwargs2)\n",
    "            score = 0\n",
    "            for goal_node in goal_output_nodes:\n",
    "                result = results[goal_node]\n",
    "                score_goal = torch.stack(list(result.values())).float()\n",
    "                if score_mode == \"prod\":\n",
    "                    score_goal = score_goal.prod()\n",
    "                elif score_mode == \"sum\":\n",
    "                    score_goal = score_goal.mean()\n",
    "                else:\n",
    "                    raise\n",
    "                score = score + score_goal\n",
    "            return -score\n",
    "\n",
    "        optms = self.search(goal_ctrl_nodes, loss_fun,\n",
    "                            method=kwargs[\"method\"] if \"method\" in kwargs else \"differential_evolution\",\n",
    "                            example_keys=input_keys,\n",
    "                           )\n",
    "\n",
    "        # Use the best value:\n",
    "        argmin = optms[0]\n",
    "        self.set_variable_values(goal_ctrl_nodes, argmin, shapes, example_keys=input_keys)\n",
    "        return self.forward_simple(*inputs, **kwargs)\n",
    "\n",
    "\n",
    "    def forward_simple(\n",
    "        self,\n",
    "        *inputs,\n",
    "        is_output_all=False,\n",
    "        is_input_all=False,\n",
    "        is_selector_all=False,\n",
    "        is_output_tensor=False,\n",
    "        is_NN_pathway=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Perform the forward function on the main graph (without considering goal nodes).\n",
    "\n",
    "        It first connect the self (program) DAG with the input graph, then according to the topological sort,\n",
    "        performs operation along the DAG, and save the results in all operator outputs to \"result\" dict.\n",
    "        \n",
    "        Args:\n",
    "            inputs: a list of OrderedDict(), where each item of the OrderedDict() is a Concept() graph. \n",
    "                Alternatively, each element of the inputs can be a Concept() graph, which will be assigned {0: Concept()}.\n",
    "            is_output_all: if True, will return all the intermediate concept graph at the outnodes.\n",
    "            is_input_all: if True, will record all the input values for all intermediate nodes.\n",
    "        Returns:\n",
    "            A list of OrderedDict()\n",
    "        \"\"\"\n",
    "        def find_ready_goal_node(results, goal_node_dict, goal_node_inspected):\n",
    "            \"\"\"Check if the current results contains necessary output for the goal\"\"\"\n",
    "            goal_node = None\n",
    "            for node, ancestors in goal_node_dict.items():\n",
    "                if node not in goal_node_inspected:\n",
    "                    if set(ancestors).issubset(list(results.keys())):\n",
    "                        goal_node = node\n",
    "                        break\n",
    "            return goal_node\n",
    "\n",
    "        def filter_preds_by_Bool(preds, goal_preds):\n",
    "            \"\"\"Filter preds by Boolean.\"\"\"\n",
    "            preds_valid = OrderedDict()\n",
    "            for goal_key, goal_value in goal_preds.items():\n",
    "                goal_key_core = (goal_key,) if not isinstance(goal_key, tuple) else goal_key\n",
    "                if isinstance(goal_value, Concept):\n",
    "                    goal_value = goal_value.get_root_value()\n",
    "                if goal_value:\n",
    "                    # Find the pred_key corresponding to the goal_key:\n",
    "                    for pred_key in preds:\n",
    "                        pred_key_core = (pred_key,) if not isinstance(pred_key, tuple) else pred_key\n",
    "                        if pred_key_core == goal_key_core[:len(pred_key_core)]:\n",
    "                            break\n",
    "\n",
    "                    preds_valid[pred_key] = preds[pred_key]\n",
    "            return preds_valid\n",
    "\n",
    "        # Connect the input graph with self (program) DAG:\n",
    "        self.connect_input_placeholder_nodes()\n",
    "        \n",
    "        # If graph is empty, return input:\n",
    "        if len(self.operators_full) == len(self.input_placeholder_nodes):\n",
    "            if is_output_all:\n",
    "                if is_selector_all:\n",
    "                    return OrderedDict([[self.get_node_name(\"input-{}\".format(kk + 1)), input] for kk, input in enumerate(inputs)]), {\"results_selector\": OrderedDict([[self.get_node_name(\"input-{}\".format(kk + 1)), OrderedDict()] for kk, input in enumerate(inputs)])}\n",
    "                else:\n",
    "                    return OrderedDict([[self.get_node_name(\"input-{}\".format(kk + 1)), input] for kk, input in enumerate(inputs)])\n",
    "            else:\n",
    "                if is_selector_all:\n",
    "                    return inputs[0] if len(inputs) == 1 else inputs, {\"results_selector\": {\"results_selector\": OrderedDict([[self.get_node_name(\"input-{}\".format(kk + 1)), OrderedDict()] for kk, input in enumerate(inputs)])}}\n",
    "                else:\n",
    "                    return inputs[0] if len(inputs) == 1 else inputs\n",
    "\n",
    "        # Check if input is in not in the format of dictionary:\n",
    "        is_dict = False\n",
    "        for input_arg in inputs:\n",
    "            if isinstance(input_arg, dict):\n",
    "                is_dict = True\n",
    "                break\n",
    "        if not is_dict:\n",
    "            inputs = [{0: input_arg} for input_arg in inputs]\n",
    "\n",
    "        # Broadcast input keys, and record inputs into results:\n",
    "        results = OrderedDict()\n",
    "        if is_input_all:\n",
    "            results_input = OrderedDict()\n",
    "        if is_selector_all:\n",
    "            results_selector = OrderedDict()\n",
    "        input_key_list_all = []\n",
    "        for i, input_arg in enumerate(inputs):\n",
    "            input_node_name = self.get_node_name(\"input-{}\".format(i + 1))\n",
    "            results[input_node_name] = input_arg\n",
    "            if is_selector_all:\n",
    "                results_selector[input_node_name] = OrderedDict()\n",
    "            if isinstance(input_arg, dict):\n",
    "                input_key_list_all.append(input_arg.keys())\n",
    "            else:\n",
    "                input_key_list_all.append(None)\n",
    "        input_key_dict = broadcast_keys(input_key_list_all)\n",
    "        input_keys = list(input_key_dict.keys())\n",
    "\n",
    "        # Initialize the value in constant concept nodes into the results:\n",
    "        for node_name in self.constant_concept_nodes:\n",
    "            results[node_name] = OrderedDict([[key, self.get_node_content(node_name)] for key in input_keys])\n",
    "\n",
    "        # Initialize variables:\n",
    "        self.init_variable(input_keys)\n",
    "\n",
    "        # Obtain all the operators along the topological sort, excluding goal operators and ground_nodes:\n",
    "        out_nodes_dag = self.get_output_nodes(types=[\"fun-out\", \"attr\"], dangling_mode=\"possible\", allow_goal_node=False)\n",
    "\n",
    "        # Obtain the goal nodes without optm and their ancestors:\n",
    "        goal_node_dict = self.goal_node_ancestors(is_optm=False)\n",
    "        goal_node_inspected = []\n",
    "\n",
    "        # Perform operation along the DAG:\n",
    "        for out_node in out_nodes_dag:\n",
    "            # Obtain the function held in the operator:\n",
    "            parent_node = self.parent_nodes(out_node)\n",
    "            assert len(parent_node) == 1\n",
    "            parent_node = parent_node[0]\n",
    "            if self[parent_node][out_node][0][\"type\"] == \"intra-attr\":\n",
    "                # Obtain the attribute of parent node:\n",
    "                assert parent_node in results, \"The node '{}' must be in the results to get attributes!\".format(parent_node)\n",
    "                results[out_node] = OrderedDict()\n",
    "                for key, concept in results[parent_node].items():\n",
    "                    if not isinstance(concept, Concept):\n",
    "                        concept = CONCEPTS[parent_node.split(\":\")[-1]].copy().set_node_value(concept)\n",
    "                        concept.compute_attr_value()\n",
    "                    out_node_proper_name = get_attr_proper_name(concept, out_node)\n",
    "                    result = concept.get_attr(out_node_proper_name)\n",
    "                    if result is not None:\n",
    "                        results[out_node][key] = result\n",
    "                    else:\n",
    "                        fun = concept.nodes(data=True)[out_node_proper_name][\"fun\"]\n",
    "                        if fun is not None:\n",
    "                            results[out_node][key] = fun(concept.get_root_value())\n",
    "                        else:\n",
    "                            print(\"No function to calculate the {} of {} for {}!\".format(out_node.split(\"^\")[1], parent_node, key))\n",
    "                            results[out_node][key] = None\n",
    "                    if is_selector_all:\n",
    "                        results_selector[out_node] = OrderedDict()\n",
    "\n",
    "            else:\n",
    "                # Use operator to obtain output concept from input concepts:\n",
    "                assert self.get_node_type(parent_node) == \"self\", \"If not getting attribute, the parent node must be a operator!\"\n",
    "                function = self.get_node_content(parent_node)\n",
    "                has_selector = False\n",
    "                \n",
    "                results[out_node] = OrderedDict()\n",
    "                in_nodes = self.parent_nodes(parent_node)  # in_nodes for the operator\n",
    "                in_value_nodes = []  # out_nodes that feed into the in_nodes\n",
    "                key_list_all = []\n",
    "                in_node_mapping = {}\n",
    "                kk = 0\n",
    "                for jj, in_node in enumerate(in_nodes):\n",
    "                    in_value_node = self.parent_nodes(in_node)\n",
    "                    if len(in_value_node) > 0:\n",
    "                        if \"multi*\" not in in_node:\n",
    "                            assert len(in_value_node) == 1\n",
    "                            in_value_nodes.append(in_value_node[0])\n",
    "                            key_list_all.append(results[in_value_node[0]].keys() if isinstance(results[in_value_node[0]], dict) else None)\n",
    "                            in_node_mapping[kk] = jj\n",
    "                            kk += 1\n",
    "                        else:\n",
    "                            in_value_nodes += in_value_node\n",
    "                            for kk_plus, in_value_node_ele in enumerate(in_value_node):\n",
    "                                key_list_all.append(results[in_value_node_ele].keys() if isinstance(results[in_value_node[0]], dict) else None)\n",
    "                                in_node_mapping[kk + kk_plus] = jj\n",
    "                            kk += len(in_value_node)\n",
    "                    else:\n",
    "                        in_value_nodes.append((in_node,))  # There is no in_value_node. Use the ungrounded variable in in_node.\n",
    "                        key_list_all.append(input_keys)\n",
    "                        in_node_mapping[kk] = jj\n",
    "                        kk += 1\n",
    "                key_dict = broadcast_keys(key_list_all)\n",
    "                if is_input_all:\n",
    "                    results_input[out_node] = []\n",
    "                if is_selector_all:\n",
    "                    results_selector[out_node] = OrderedDict()\n",
    "                for expanded_key, key_list in key_dict.items():\n",
    "                    # Obtain input_values:\n",
    "                    input_values = []\n",
    "\n",
    "                    k = 0  # k counts the number of intermediate results (not ungrounded variables)\n",
    "                    for i, (key, in_value_node) in enumerate(zip(key_list, in_value_nodes)):\n",
    "                        if not isinstance(in_value_node, tuple):\n",
    "                            # Get value from results calculated from previous steps:\n",
    "                            if key is None:\n",
    "                                input_value = results[in_value_node]\n",
    "                                if is_input_all:\n",
    "                                    if len(results_input[out_node]) == k:\n",
    "                                        results_input[out_node].append(OrderedDict())\n",
    "                                    results_input[out_node][k] = input_value\n",
    "                            else:\n",
    "                                input_value = results[in_value_node][key]\n",
    "                                # Record input_value:\n",
    "                                if is_input_all:\n",
    "                                    if len(results_input[out_node]) == k:\n",
    "                                        results_input[out_node].append(OrderedDict())\n",
    "                                    results_input[out_node][k][key] = input_value\n",
    "                            k += 1\n",
    "                        else:\n",
    "                            # Get value from ungrounded concepts:\n",
    "                            input_value = getattr(self, \"variable_{}_{}\".format(in_value_node[0], key))\n",
    "\n",
    "                        # If the first in_node has a selector, use the selector to select the input_value:\n",
    "                        selector_innode = self.get_selector(in_nodes[in_node_mapping[i]])\n",
    "                        if len(selector_innode) > 0:\n",
    "                            selector = selector_innode[next(iter(selector_innode))]\n",
    "                            has_selector = True\n",
    "                            if is_NN_pathway:\n",
    "                                input_value, _ = selector.forward_NN(input_value)\n",
    "                            else:\n",
    "                                input_value_ori = deepcopy(input_value)\n",
    "                                inp_name_ori = input_value_ori.get_node_name()\n",
    "                                # If selector applies to a System, must apply the operator to the system concept\n",
    "                                if \"System\" in inp_name_ori:\n",
    "                                    input_value = input_value.get_refer_subconcept(selector)\n",
    "                                else:\n",
    "                                    input_value = input_value.get_refer_nodes(selector)\n",
    "\n",
    "                        # Append:\n",
    "                        input_values.append(input_value)\n",
    "\n",
    "                    # Execute the operator:\n",
    "                    if not has_selector:\n",
    "                        output_value = function(*input_values, is_NN_pathway=is_NN_pathway)\n",
    "                    else:\n",
    "                        # Perform operator on selected objects (either inplace or use new instance.)\n",
    "                        if is_NN_pathway:\n",
    "                            output_value = function(*input_values, is_NN_pathway=is_NN_pathway)\n",
    "                        else:\n",
    "                            obj_trans_dict = {}\n",
    "                            inplace = self.get_inplace(in_nodes[0])\n",
    "                            inp_name_ori = input_value_ori.get_node_name()\n",
    "                            if \"System\" in inp_name_ori:\n",
    "                                output_value = function(input_values[0])\n",
    "                            elif len(input_values[0]) > 0:\n",
    "                                for obj_name, obj in input_values[0].items():\n",
    "                                    obj_trans = function(obj, *input_values[1:], is_NN_pathway=is_NN_pathway)\n",
    "                                    obj_trans_dict[obj_name] = obj_trans\n",
    "                                    if inplace:\n",
    "                                        # Perform operation on the object, but keep the obj_name unchanged:\n",
    "                                        input_value_ori.remove_attr(obj_name, change_root=True)\n",
    "                                        input_value_ori.add_obj(obj_trans, obj_name=obj_name, change_root=True)\n",
    "                                if inplace:\n",
    "                                    output_value = input_value_ori\n",
    "                                else:\n",
    "                                    if obj_name.split(\":\")[-1] == DEFAULT_OBJ_TYPE:\n",
    "                                        output_value, _ = get_comp_obj(obj_trans_dict, CONCEPTS)\n",
    "                                    else:\n",
    "                                        output_value = obj_trans_dict\n",
    "                            else:\n",
    "                                if inplace:\n",
    "                                    output_value = input_value_ori\n",
    "                                else:\n",
    "                                    output_value = CONCEPTS[DEFAULT_OBJ_TYPE].copy().set_node_value(torch.zeros(1, 1)).set_node_value([0, 0, 1, 1], \"pos\")\n",
    "                        # Record selector info:\n",
    "                        if is_selector_all:\n",
    "                            results_selector[out_node][expanded_key] = {\"operated\": list(input_values[0].keys()),\n",
    "                                                                        \"from\": in_value_nodes,\n",
    "                                                                        \"inplace\": inplace}\n",
    "\n",
    "                    # Formatting output:\n",
    "                    if isinstance(output_value, bool):\n",
    "                        output_value = torch.BoolTensor([output_value])[0].to(self.device)\n",
    "                    if is_output_tensor and isinstance(output_value, Concept):\n",
    "                        output_value = output_value.get_root_value()\n",
    "                    # Record output_value:\n",
    "                    if isinstance(output_value, dict):\n",
    "                        for j, output_value_ele in output_value.items():\n",
    "                            new_key = expanded_key + (j,) if isinstance(expanded_key, tuple) else (expanded_key, j)\n",
    "                            results[out_node][new_key] = output_value_ele\n",
    "                    else:\n",
    "                        results[out_node][expanded_key] = output_value\n",
    "                # Make sure that all keys have the same length for all examples:\n",
    "                results[out_node] = canonicalize_keys(results[out_node])\n",
    "\n",
    "                # Check goal_nodes without optm, and reduce number of predictions if determined by goal nodes:\n",
    "                goal_node = find_ready_goal_node(results, goal_node_dict, goal_node_inspected)\n",
    "                if goal_node is not None:\n",
    "                    goal_input_nodes = self.parent_nodes(goal_node)\n",
    "                    goal_in_value_nodes = [self.parent_nodes(node)[0] for node in goal_input_nodes]\n",
    "                    goal_input_values = [results[node] for node in goal_in_value_nodes]\n",
    "                    goal_preds = self.get_node_content(goal_node)(*goal_input_values)\n",
    "                    results[out_node] = filter_preds_by_Bool(results[out_node], goal_preds)\n",
    "\n",
    "        # Return results:\n",
    "        info_all = {}\n",
    "        if is_input_all:\n",
    "            info_all[\"results_input\"] = results_input\n",
    "        if is_selector_all:\n",
    "            info_all[\"results_selector\"] = results_selector\n",
    "        if is_output_all:\n",
    "            if is_input_all or is_selector_all:\n",
    "                return results, info_all\n",
    "            else:\n",
    "                return results\n",
    "        else:\n",
    "            output_mode = out_nodes_dag[-1].split(\":\")[-1]\n",
    "            output_dict = OrderedDict()\n",
    "            for key, item in results[out_nodes_dag[-1]].items():\n",
    "                if is_output_tensor:\n",
    "                    if isinstance(item, Concept):\n",
    "                        item = item.get_root_value()\n",
    "                else:\n",
    "                    if not isinstance(item, Concept):\n",
    "                        item = CONCEPTS[output_mode].copy().set_node_value(item, output_mode)\n",
    "                if \"isplot\" in kwargs and kwargs[\"isplot\"]:\n",
    "                    item_value = item.get_root_value() if isinstance(item, Concept) else item\n",
    "                    if len(item_value.shape) == 2:\n",
    "                        visualize_matrices([item_value])\n",
    "                output_dict[key] = item\n",
    "\n",
    "            if not is_dict and len(output_dict) == 1:\n",
    "                output_dict = output_dict[0]\n",
    "            if is_input_all or is_selector_all:\n",
    "                return output_dict, info_all\n",
    "            else:\n",
    "                return output_dict\n",
    "\n",
    "\n",
    "    def get_score(self, inputs, targets, score_fun, preds=None, variable_dict=None, return_preds=False, **kwargs):\n",
    "        \"\"\"Obtain the score and the corresponding best idx for matching pred and target.\n",
    "        score must be between 0 and 1.\n",
    "        \n",
    "        Args:\n",
    "            inputs: a list of Concept() or OrderedDict() of Concept().\n",
    "            targets: a Concept() or OrderedDict() of Concept()\n",
    "            score_fun: score function to evaluate the score for each pair of pred and target.\n",
    "            variable_dict: a dictionary of {key: variable_tensor}, or a OrderedDict() of such dictionaries.\n",
    "                If None, will use current self's variables.\n",
    "        Returns:\n",
    "            A score (if pred is a Concept()) or OrderedDict() of scores.\n",
    "        \"\"\"\n",
    "        if preds is None:\n",
    "            if not isinstance(inputs, list):\n",
    "                inputs = [inputs]\n",
    "            preds = self(*inputs)\n",
    "        else:\n",
    "            assert inputs is None\n",
    "        if variable_dict is not None and not isinstance(variable_dict[list(variable_dict.keys())[0]], dict):\n",
    "            # Set variable only for the case where variable_dict is a dictionary of {key: variable_tensor}:\n",
    "            self.set_variables(variable_dict)\n",
    "\n",
    "        if not isinstance(preds, dict):\n",
    "            assert not isinstance(targets, dict)\n",
    "            score, _ = score_fun(preds, targets)\n",
    "            return score\n",
    "        else:\n",
    "            score_dict = OrderedDict()\n",
    "            if not isinstance(targets, dict):\n",
    "                if variable_dict is not None:\n",
    "                    key_dict = broadcast_keys([list(preds.keys()), list(variable_dict.keys())])\n",
    "                    preds = []\n",
    "                    for new_key, (pred_key, variable_key) in key_dict.items():\n",
    "                        self.set_variables(variable_dict[variable_key])\n",
    "                        inputs_ele = [OrderedDict([[pred_key, inputs[i][pred_key]]]) for i in range(len(inputs))]\n",
    "                        pred = self(*inputs_ele)\n",
    "                        preds.append(pred)\n",
    "                        score, _ = score_fun(pred, targets)\n",
    "                        score_dict[key] = score\n",
    "                else:\n",
    "                    for key, pred in preds.items():\n",
    "                        score, _ = score_fun(pred, targets)\n",
    "                        score_dict[key] = score\n",
    "            else:\n",
    "                # Both preds and targets are dict:\n",
    "                if variable_dict is not None:\n",
    "                    key_dict = broadcast_keys([list(preds.keys()), list(targets.keys()), list(variable_dict.keys())])\n",
    "                    preds = []\n",
    "                    for new_key, (pred_key, target_key, variable_key) in key_dict.items():\n",
    "                        self.set_variables(variable_dict[variable_key])\n",
    "                        inputs_ele = [inputs[i][pred_key] for i in range(len(inputs))]\n",
    "                        pred = self(*inputs_ele)\n",
    "                        preds.append(pred)\n",
    "                        score, _ = score_fun(pred, targets[target_key])\n",
    "                        score_dict[new_key] = score\n",
    "                else:\n",
    "                    key_dict = broadcast_keys([list(preds.keys()), list(targets.keys())])\n",
    "                    for new_key, (pred_key, target_key) in key_dict.items():\n",
    "                        score, _ = score_fun(preds[pred_key], targets[target_key])\n",
    "                        score_dict[new_key] = score\n",
    "        if return_preds:\n",
    "            return score_dict, preds\n",
    "        else:\n",
    "            return score_dict\n",
    "\n",
    "\n",
    "    def get_loss(self, inputs, targets, score_fun, **kwargs):\n",
    "        \"\"\"Obtain the loss, with the same input and output APIs as get_score, except that loss = 1 - score.\"\"\"\n",
    "        score_dict = self.get_score(inputs, targets, score_fun=score_fun, **kwargs)\n",
    "        if not isinstance(score_dict, dict):\n",
    "            return 1 - score_dict\n",
    "        loss_dict = OrderedDict()\n",
    "        for key, score in score_dict.items():\n",
    "            loss_dict[key] = 1 - score\n",
    "        return loss_dict\n",
    "\n",
    "\n",
    "    def get_loss_NN(self, inputs, targets, loss_fun, reduction=\"mean\"):\n",
    "        \"\"\"Compute the loss in the neural network (NN) pathway, using the given loss_fun.\"\"\"\n",
    "        # Make sure that the key matches:\n",
    "        assert isinstance(inputs, list)\n",
    "        for input in inputs:\n",
    "            assert input.keys() == targets.keys()\n",
    "        # Compute loss for each example:\n",
    "        loss_dict = OrderedDict()\n",
    "        for key, target in targets.items():\n",
    "            pred = self.forward(*[inputs[i][key] for i in range(len(inputs))], is_NN_pathway=True, is_output_tensor=True)\n",
    "            loss_dict[key] = loss_fun(pred, target)\n",
    "        if reduction == \"none\":\n",
    "            return loss_dict\n",
    "        elif reduction == \"mean\":\n",
    "            return torch.stack(list(loss_dict.values())).mean()\n",
    "        elif reduction == \"sum\":\n",
    "            return torch.stack(list(loss_dict.values())).sum()\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "\n",
    "    def search(self, search_variable_list, loss_fun, example_keys=None, method=None):\n",
    "        \"\"\"Find the value of each variable in search_variable_list that minimizes its\n",
    "        corresponding loss function.\n",
    "\n",
    "        Parameters:\n",
    "            search_variable_list: the name of variables whose values should be argmin of \n",
    "                loss_function; e.g. search_variable_list = [\"variable_S-2:Pos\", \"variable_S-2:Color\"]\n",
    "\n",
    "            method: brute-force search: \"brute\"; \n",
    "                    random search: \"basinhopping\";\n",
    "                    heuristic search: \"differential_evolution\"\n",
    "\n",
    "        Returns:\n",
    "            optms: a 2-element list.\n",
    "            optms[0]: A 1-D array containing the coordinates of points at which the objective \n",
    "                      function had its minimum value; \n",
    "            optms[1]: Function value at the optimal point.\n",
    "        \"\"\"\n",
    "\n",
    "        method = \"differential_evolution\" if method is None else method\n",
    "        optms = []\n",
    "        ranges = ()\n",
    "        x0 = []; xmax = []; xmin = []\n",
    "        length = len(example_keys) if example_keys is not None else 1\n",
    "        for variable in search_variable_list:\n",
    "            this_operator = variable.split(\"_\")[0].split(\":\")[1] # obtain the operator in each variable, e.g. \"Pos\"\n",
    "            this_operator_mode = CONCEPTS[this_operator].get_node_content(this_operator).mode\n",
    "            this_shape = int(np.prod(this_operator_mode.shape) * length) # e.g. (4,6) with 2 example keys will become 4 * 6 * 2\n",
    "            this_range = slice(this_operator_mode.range[0], this_operator_mode.range[-1] + 1, 1)\n",
    "            ranges = ranges + tuple([this_range] * this_shape)\n",
    "            x0 = x0 + [1.] * this_shape\n",
    "            xmax = xmax + [this_operator_mode.range[-1] + 1] * this_shape\n",
    "            xmin = xmin + [this_operator_mode.range[0]] * this_shape\n",
    "\n",
    "        if method == \"brute\":\n",
    "            optms = optimize.brute(loss_fun,\n",
    "                                   ranges,\n",
    "                                   full_output=True,\n",
    "                                   finish=optimize.fmin)\n",
    "\n",
    "        elif method == \"basinhopping\":\n",
    "            minimizer_kwargs = {\"method\": \"BFGS\"}\n",
    "            mybounds = MyBounds(xmax, xmin)\n",
    "            vals = optimize.basinhopping(loss_fun, \n",
    "                                         x0, \n",
    "                                         stepsize=1.0, \n",
    "                                         minimizer_kwargs=minimizer_kwargs, \n",
    "                                         niter=2000, accept_test=mybounds)\n",
    "            optms = [vals.x, vals.fun]\n",
    "\n",
    "        elif method == \"differential_evolution\": # best method so far\n",
    "            bounds = list(zip(xmin, xmax))\n",
    "            vals = optimize.differential_evolution(loss_fun, \n",
    "                                                   bounds, \n",
    "                                                   maxiter=2000)\n",
    "            optms = [vals.x, vals.fun]\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"method {} is not supported!\".format(method))\n",
    "\n",
    "        return optms\n",
    "\n",
    "\n",
    "    def infer(self, inputs, targets, score_fun=None, learnable=[\"variable\", \"fun\"], **kwargs):\n",
    "        \"\"\"Infer the best variable values for the dangling nodes, given inputs and targets.\n",
    "\n",
    "        Args:\n",
    "            inputs: a list of Concept() or OrderedDict() of Concept().\n",
    "            targets: a Concept() or OrderedDict() of Concept(). The requirements for \n",
    "                the inputs and targets is that their keys are broadcastable.\n",
    "            score_fun: score function to evaluate the score for each pair of pred and target.\n",
    "            \n",
    "        \n",
    "        Returns:\n",
    "            optm_dict: OrderedDict() of (argmin, score).\n",
    "        \"\"\"\n",
    "        key_list_all = [input_arg.keys() for input_arg in inputs]\n",
    "        key_list_all.append(targets.keys())\n",
    "        key_dict = broadcast_keys(key_list_all)\n",
    "        self.init_variable(key_dict.keys())\n",
    "\n",
    "        optm_dict = OrderedDict()\n",
    "        for expanded_key, key_list in key_dict.items():\n",
    "            search_variable_list = [key for key in self.get_variables(example_key=expanded_key)]\n",
    "            input_item = [input[key_list[i]] for i, input in enumerate(inputs)]\n",
    "            target_item = targets[key_list[-1]]\n",
    "            if len(search_variable_list) == 0:\n",
    "                score = self.get_score(input_item, target_item, score_fun=score_fun)\n",
    "                score_optms = [[], score]\n",
    "                optm_dict[expanded_key] = score_optms\n",
    "\n",
    "            else:\n",
    "                search_variable_list = [\"_\".join(variable.split(\"_\")[1:]) for variable in search_variable_list]\n",
    "                search_operators = [variable.split(\"_\")[0].split(\":\")[1] for variable in search_variable_list]\n",
    "                shapes = [CONCEPTS[c].get_node_content(c).mode.shape for c in search_operators]\n",
    "                kwargs2 = deepcopy(kwargs)\n",
    "                kwargs2[\"isplot\"] = False\n",
    "\n",
    "                def loss_fun(search_variable_values):\n",
    "                    self.set_variable_values(search_variable_list, search_variable_values, shapes)\n",
    "                    loss_dict = self.get_loss(input_item, target_item, score_fun=score_fun, **kwargs2)\n",
    "                    if isinstance(loss_dict, dict):\n",
    "                        loss = torch.stack(list(loss_dict.values())).min()\n",
    "                    else:\n",
    "                        loss = loss_dict\n",
    "                    return loss\n",
    "\n",
    "                optms = self.search(search_variable_list, loss_fun,\n",
    "                                    method=kwargs[\"method\"] if \"method\" in kwargs else \"differential_evolution\",\n",
    "                                   )\n",
    "                self.set_variable_values(search_variable_list, optms[0], shapes)\n",
    "                self.round_variables(expanded_key)\n",
    "                optm_dict[expanded_key] = [self.get_variables(example_key=expanded_key), 1 - optms[1]]\n",
    "                if \"isplot\" in kwargs and kwargs[\"isplot\"]:\n",
    "                    self(*input_item, isplot=True)\n",
    "\n",
    "        return optm_dict\n",
    "\n",
    "        # Note that we can also try heuristic search hill_climbing or genetic search\n",
    "        # and random search https://towardsdatascience.com/hyperparameter-optimization-in-python-part-1-scikit-optimize-754e485d24fe\n",
    "\n",
    "\n",
    "    def accepts(self, inputs, targets):\n",
    "        \"\"\"Whether the current operator's expected input and output types are compatible with the input and target.\"\"\"\n",
    "        key_dict = broadcast_keys([list(inputs[0].keys()), list(targets.keys())])\n",
    "        for _, key_list in key_dict.items():\n",
    "            input_key, target_key = key_list\n",
    "            break\n",
    "        target = targets[target_key]\n",
    "\n",
    "        operator_input_modes = [node.split(\":\")[-1] for node, is_input in self.input_nodes.items() if is_input]\n",
    "        input_modes = [input_arg[input_key].name for input_arg in inputs]\n",
    "        is_valid_input = accepts(operator_input_modes, input_modes, [CONCEPTS, NEW_CONCEPTS], mode=\"exists\")\n",
    "\n",
    "        is_valid_target = False\n",
    "        for operator_target_node in self.get_output_nodes(types=[\"fun-out\"], dangling_mode=True):\n",
    "            target_mode = split_string(target.name)[0]\n",
    "            target_modes_inherit = get_inherit_modes(target_mode, CONCEPTS, type=\"to\")\n",
    "            operator_target_mode = operator_target_node.split(\":\")[-1]\n",
    "            if operator_target_mode in target_modes_inherit:\n",
    "                is_valid_target = True\n",
    "                break\n",
    "        return is_valid_input and is_valid_target\n",
    "\n",
    "\n",
    "    ########################################\n",
    "    # Get items from graph:\n",
    "    ########################################\n",
    "    def get_node_input(self, node_name, mode=None):\n",
    "        \"\"\"Obtain the input node of an operator whose mode is the same as given mode.\"\"\"\n",
    "        if self.get_node_type(node_name) == \"self\":\n",
    "            parent_nodes = self.parent_nodes(node_name)\n",
    "            if mode is None:\n",
    "                return parent_nodes\n",
    "            else:\n",
    "                return [parent_node for parent_node in parent_nodes if canonical(parent_node.split(\":\")[-1]) == mode]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "    def get_node_output(self, node_name, mode=None):\n",
    "        \"\"\"Obtain the output node of an operator whose mode is the same as given mode.\"\"\"\n",
    "        assert self.get_node_type(node_name) in [\"self\"]\n",
    "        child_node = self.child_nodes(node_name)\n",
    "        assert len(child_node) == 1\n",
    "        if mode is None:\n",
    "            return child_node\n",
    "        else:\n",
    "            return [node_name for node_name in child_node if node_name.split(\":\")[-1] == mode]\n",
    "\n",
    "\n",
    "    @property\n",
    "    def core_graph(self):\n",
    "        return self\n",
    "\n",
    "\n",
    "    @property\n",
    "    def core_graph_shallow(self):\n",
    "        return self\n",
    "\n",
    "\n",
    "    @property\n",
    "    def main_graph(self):\n",
    "        \"\"\"Return the subgraph excluding all goal nodes.\"\"\"\n",
    "        G = self.copy()\n",
    "        G.remove_subgraph(self.goals)\n",
    "        return G\n",
    "\n",
    "\n",
    "    @property\n",
    "    def main_forward_graph(self):\n",
    "        \"\"\"Return the subgraph excluding the goal nodes that involves optimization.\"\"\"\n",
    "        G = self.copy()\n",
    "        G.remove_subgraph(self.goals_optm)\n",
    "        return G\n",
    "\n",
    "\n",
    "    @property\n",
    "    def input_nodes(self):\n",
    "        \"\"\"Return input-nodes and whether they are grounded.\"\"\"\n",
    "        input_nodes_dict = OrderedDict()\n",
    "        input_placeholder_nodes = self.input_placeholder_nodes\n",
    "        constant_concept_nodes = self.constant_concept_nodes\n",
    "        for operator in self.operators_core:\n",
    "            nodes = self.parent_nodes(operator)\n",
    "            for node in nodes:\n",
    "                data = self.nodes(data=True)[node]\n",
    "                if len(node.split(\"-\")) > 1 and \"-o\" not in node and \"input-\" not in node and \"concept-\" not in node and \"Ctrl\" not in node: # is an input node\n",
    "                    is_input = True\n",
    "                    # Check if the node is the child of some other nodes:\n",
    "                    for neighbor_node, info in self[node].items():\n",
    "                        if info[0][\"type\"].startswith(\"b-\"):\n",
    "                            is_input = False\n",
    "                            break\n",
    "                    # Check if this node is fed into a goal node for grounding:\n",
    "                    for child_node in self.child_nodes(node):\n",
    "                        if \"Ctrl\" in child_node:\n",
    "                            is_input = False\n",
    "                            break\n",
    "                    input_nodes_dict[node] = is_input\n",
    "        return input_nodes_dict\n",
    "\n",
    "\n",
    "    @property\n",
    "    def dangling_nodes(self):\n",
    "        \"\"\"Return a list of input nodes that does not expect input\n",
    "        (has no arrow fed to it, and does not expect to connect with ground nodes,\n",
    "         thus when calling forward function must initialize a PyTorch variable.)\"\"\"\n",
    "        dangling_nodes = []\n",
    "        for node, is_input in self.input_nodes.items():\n",
    "            variable_name = \"variable_{}\".format(node)\n",
    "            # is_dangling is True if the node does not expect any input:\n",
    "            is_dangling = True if is_input and self.get_node_content(node).mode != self.ground_node_name else False\n",
    "            if is_dangling:\n",
    "                dangling_nodes.append(node)\n",
    "        return dangling_nodes\n",
    "\n",
    "\n",
    "    @property\n",
    "    def input_node_concept(self):\n",
    "        \"\"\"Get all the danging concept input_nodes.\"\"\"\n",
    "        concept_nodes = []\n",
    "        for node, is_input in self.input_nodes.items():\n",
    "            if is_input:\n",
    "                mode = node.split(\":\")[-1]\n",
    "                if mode == \"Concept\":\n",
    "                    concept_nodes.append(node)\n",
    "        return concept_nodes\n",
    "\n",
    "\n",
    "    @property\n",
    "    def is_fully_grounded(self):\n",
    "        \"\"\"Check if the current graph is fully grounded (have no dangling input nodes).\"\"\"\n",
    "        return len(self.dangling_nodes) == 0\n",
    "\n",
    "\n",
    "    @property\n",
    "    def control_nodes(self):\n",
    "        \"\"\"Control nodes: input_nodes that is fed to the 'Ctrl' of a goal node.\"\"\"\n",
    "        control_nodes = []\n",
    "        for node in self.input_nodes:\n",
    "            for child_node in self.child_nodes(node):\n",
    "                if \"Ctrl\" in child_node:\n",
    "                    control_nodes.append(node)\n",
    "        return control_nodes\n",
    "\n",
    "\n",
    "    @property\n",
    "    def goal_nodes(self):\n",
    "        \"\"\"Goal nodes. The item is True when the control node has control variables.\"\"\"\n",
    "        node_dict = OrderedDict()\n",
    "        for node in self.goals:\n",
    "            parent_nodes = self.parent_nodes(node)\n",
    "            if \"Cri-0:Ctrl\" in parent_nodes:\n",
    "                node_dict[node] = True  # Need to optimize\n",
    "            else:\n",
    "                node_dict[node] = False\n",
    "        return node_dict\n",
    "\n",
    "\n",
    "    @property\n",
    "    def goals_optm(self):\n",
    "        return [node for node, is_optm in self.goal_nodes.items() if is_optm]\n",
    "\n",
    "\n",
    "    def goal_node_ancestors(self, node_name=None, is_optm=None):\n",
    "        \"\"\"Get the ancestor output nodes of a goal node.\"\"\"\n",
    "        node_dict = OrderedDict()\n",
    "        output_nodes = self.get_output_nodes(types=[\"fun-out\", \"input\", \"attr\"], dangling_mode=\"possible\")\n",
    "        for node, is_optm_ele in self.goal_nodes.items():\n",
    "            if is_optm is not None and (is_optm_ele is not is_optm):\n",
    "                continue\n",
    "            if node_name is not None:\n",
    "                if node != self.get_node_name(node_name):\n",
    "                    continue\n",
    "            ancestors = self.get_ancestors(\"{}-o\".format(node))\n",
    "            node_dict[node] = list(set(output_nodes).intersection(set(ancestors)))\n",
    "        return node_dict\n",
    "\n",
    "\n",
    "    @property\n",
    "    def output_nodes(self):\n",
    "        return self.get_output_nodes(types=[\"fun-out\", \"attr\", \"input\"], dangling_mode=\"possible\")\n",
    "\n",
    "\n",
    "    def get_output_nodes(self, types=[\"fun-out\", \"attr\", \"input\"], dangling_mode=\"possible\", allow_goal_node=False):\n",
    "        \"\"\"Return output-nodes and whether they feed into other inputs.\"\"\"\n",
    "        assert set(types).issubset({\"fun-out\", \"attr\", \"input\"})\n",
    "        output_nodes = OrderedDict() if isinstance(dangling_mode, dict) else []\n",
    "        for node_name in self.topological_sort:\n",
    "            if self.get_node_type(node_name) in types:\n",
    "                if (not allow_goal_node) and node_name.split(\"-\")[0] in self.goals:\n",
    "                    continue\n",
    "                if dangling_mode == \"possible\":\n",
    "                    output_nodes.append(node_name)\n",
    "                elif dangling_mode == \"available\":\n",
    "                    output_nodes.append(node_name)\n",
    "                elif dangling_mode in [True, False]:\n",
    "                    if self.check_dangling(node_name) is dangling_mode:\n",
    "                        output_nodes.append(node_name)\n",
    "                elif isinstance(dangling_mode, dict):\n",
    "                    if \"possible\" in dangling_mode:\n",
    "                        output_nodes[node_name] = dangling_mode[\"possible\"]\n",
    "                    if \"available\" in dangling_mode and self.check_available(node_name):\n",
    "                        output_nodes[node_name] = dangling_mode[\"available\"]\n",
    "                    if \"dangling\" in dangling_mode and self.check_dangling(node_name):\n",
    "                        output_nodes[node_name] = dangling_mode[\"dangling\"]\n",
    "                else:\n",
    "                    raise\n",
    "        return output_nodes\n",
    "\n",
    "\n",
    "    @property\n",
    "    def input_placeholder_nodes(self):\n",
    "        nodes = []\n",
    "        for node_name in self.nodes:\n",
    "            if self.get_node_type(node_name) == \"input\":\n",
    "                nodes.append(node_name)\n",
    "        return nodes\n",
    "\n",
    "\n",
    "    @property\n",
    "    def constant_concept_nodes(self):\n",
    "        nodes = []\n",
    "        for node_name in self.nodes:\n",
    "            if self.get_node_type(node_name) == \"concept\":\n",
    "                nodes.append(node_name)\n",
    "        return nodes\n",
    "\n",
    "\n",
    "    @property\n",
    "    def operators_full(self):\n",
    "        \"\"\"Returns input_place_holder_nodes, operators, constant concept nodes, and relevant attribute nodes in a sorted list.\n",
    "        Here the operators are the one that can hold concepts.\n",
    "        \"\"\"\n",
    "        input_placeholder_nodes = [self.operator_name(op) for op in self.input_placeholder_nodes]\n",
    "        operators = input_placeholder_nodes\n",
    "        for operator in self.topological_sort:\n",
    "            if len(operator.split(\"-\")) == 1 or self.get_node_type(operator) == \"concept\":\n",
    "                operators.append(operator)\n",
    "        operators_dict = OrderedDict([[operator, [operator]] for operator in operators])\n",
    "        for i, node in enumerate(self.topological_sort):\n",
    "            if \"input\" in node and \"^\" in node:\n",
    "                # input attribute node:\n",
    "                op = node.split(\"^\")[0]\n",
    "                operators_dict[op].append(self.operator_name(node))\n",
    "            elif \"^\" in node:\n",
    "                # attribute node based on intermediate output\n",
    "                op = node.split(\"-\")[0]\n",
    "                operators_dict[op].append(self.operator_name(node))\n",
    "        full_operators = []\n",
    "        for _, operator_expand in operators_dict.items():\n",
    "            full_operators += operator_expand\n",
    "        full_operators = OrderedDict([[operator, i] for i, operator in enumerate(full_operators)])\n",
    "        return full_operators\n",
    "\n",
    "\n",
    "    @property\n",
    "    def operators_core(self):\n",
    "        \"\"\"Returns the operators in a sorted list.\"\"\"\n",
    "        operators = remove_duplicates([operator.split(\"-\")[0] for operator in self.topological_sort if len(operator.split(\"-\")) == 1])\n",
    "        assert set(operators) == set(self.operators)\n",
    "        return operators\n",
    "\n",
    "\n",
    "    @property\n",
    "    def reprs(self):\n",
    "        \"\"\"Return the reprentations for operators in N x REPR_DIM, where N is the number of\n",
    "        operators in the graph, and each row corresponds to the global representation of the operator.\n",
    "        \"\"\"\n",
    "        x = []\n",
    "        for operator in self.operators_core:\n",
    "            operator_repr = self.get_node_repr(operator)\n",
    "            x.append(operator_repr)\n",
    "        x = torch.stack(x, 0)\n",
    "        return x\n",
    "\n",
    "\n",
    "    @property\n",
    "    def edge_index(self):\n",
    "        \"\"\"Return edge_index for operators in COO format, where the operators' index \n",
    "        is according to self.operators_full.\"\"\"\n",
    "        edge_index = []\n",
    "        operators_full = self.operators_full\n",
    "        for i, operator in enumerate(operators_full):\n",
    "            for child_operator in self.child_operators(operator):\n",
    "                j = operators_full[child_operator]\n",
    "                edge_index.append([i, j])\n",
    "        if len(edge_index) > 0:\n",
    "            edge_index = to_Variable(edge_index).long().T.to(self.device)\n",
    "            return edge_index\n",
    "        else:\n",
    "            return torch.zeros(2, 0).long().to(self.device)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def edge_type(self):\n",
    "        \"\"\"\n",
    "        edge attributes:\n",
    "        3-hot: whether it is (0) generic inter-operator edge, (1) attribute edge, or (2) connect to goal node.  \"\"\"\n",
    "        edge_type = []\n",
    "        operators_full = self.operators_full\n",
    "        for i, operator in enumerate(operators_full):\n",
    "            for child_operator in self.child_operators(operator):\n",
    "                j = operators_full[child_operator]\n",
    "                if operator.split(\"-\")[0] == child_operator.split(\"-\")[0]:\n",
    "                    edge_type.append(1)\n",
    "                elif \"Cri\" in child_operator:\n",
    "                    # Connected to goal node:\n",
    "                    edge_type.append(2)\n",
    "                else:\n",
    "                    edge_type.append(0)\n",
    "        if len(edge_type) > 0:\n",
    "            edge_type = torch.LongTensor(edge_type).to(self.device)\n",
    "            return edge_type\n",
    "        else:\n",
    "            return torch.zeros(0).to(self.device)\n",
    "\n",
    "\n",
    "    def get_op_attr(\n",
    "        self,\n",
    "        OPERATORS,\n",
    "        inputs=None,\n",
    "        targets=None,\n",
    "        parse_pair_ele=None,\n",
    "        op_attr_modes=\"0123\",\n",
    "        allowed_attr=\"obj\",\n",
    "        cache_dirname=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Obtain operator attributes amenable for PyG.\n",
    "\n",
    "        Args:\n",
    "            OPERATORS: dictionary of operators\n",
    "            CONCEPTS: dictionary of concepts\n",
    "            inputs: inputs for computing shape and intermediate result\n",
    "            targets: OrderedDict of targets\n",
    "            parse_pair_ele: function parsing the pair of inter_result and target into correspondence of component objects.\n",
    "            op_attr_modes:\n",
    "                0. operator type: |O| + 4 hot (4 is for input-node, concept-node, Do graph and For graph, respectively)\n",
    "                1. operator state: 1-integer, number of dangling input nodes\n",
    "                2. shape: 2-integer vector\n",
    "                3. (intermediate-result, target) in terms of concept graph.\n",
    "            allowed_attr: allowed attributes when computing the concept graph correspondence. Choose from \"obj\" and \"all\"\n",
    "            cache_dirname: if not None, will save the cached files in the cache_dirname for future use.\n",
    "\n",
    "        Covariant encoding of concept graphs:\n",
    "            Each input, intermediate or target concept graph has a covariant encoding, on which the operator on top of that\n",
    "            can refer to any subset of components. This is mainly used in two scenarios: pairing of intermediate result\n",
    "            with the target, as well as building the selector for the downstream operators.\n",
    "\n",
    "        Returns:\n",
    "            op_attr: in a form amenable for PyG.\n",
    "        \"\"\"\n",
    "        length_op = len(OPERATORS)\n",
    "        OPERATOR_KEYS = list(OPERATORS.keys())\n",
    "        operators_full = self.operators_full\n",
    "        op_attr = {}\n",
    "\n",
    "        # Get operator type as one-hot in with length len(OPERATORS) + 4:\n",
    "        if \"0\" in op_attr_modes:\n",
    "            op_types = torch.zeros(len(operators_full), length_op + 4).to(self.device)\n",
    "            for i, op in enumerate(operators_full):\n",
    "                if \"input\" in op:\n",
    "                    idx = length_op\n",
    "                elif \"concept\" in op:\n",
    "                    idx = length_op + 1\n",
    "                elif \"Do\" in op:\n",
    "                    idx = length_op + 2\n",
    "                elif \"For\" in op:\n",
    "                    idx = length_op + 3\n",
    "                else:\n",
    "                    op_core = split_string(op.split(\"-\")[0])[0]\n",
    "                    idx = OPERATOR_KEYS.index(op_core)\n",
    "                op_types[i, idx] = 1\n",
    "            op_attr[\"op_types\"] = op_types\n",
    "\n",
    "        # Get the state the operator:\n",
    "        if \"1\" in op_attr_modes:\n",
    "            op_states = torch.zeros(len(operators_full), 1).to(self.device)\n",
    "            for i, op in enumerate(operators_full):\n",
    "                out_node = self.get_to_outnode(op)\n",
    "                node_name = self.get_node_name(out_node)\n",
    "                if self.get_node_type(node_name) in [\"attr\", \"input\", \"concept\"]:\n",
    "                    op_states[i] = 0\n",
    "                else:\n",
    "                    assert self.get_node_type(op) == \"self\"\n",
    "                    in_nodes = self.parent_nodes(op)\n",
    "                    num_dangling_innodes = 0\n",
    "                    for node in in_nodes:\n",
    "                        if len(self.parent_nodes(node)) == 0:\n",
    "                            num_dangling_innodes += 1\n",
    "                    # To do: deal with grounding using goal node.\n",
    "                    op_states[i] = num_dangling_innodes\n",
    "            op_attr[\"op_states\"] = op_states\n",
    "\n",
    "        # Get the output shape of the operator:\n",
    "        if \"2\" in op_attr_modes and inputs is not None:\n",
    "            input_keys = list(inputs[0].keys())\n",
    "            input_keys_tuple = [(key,) if not isinstance(key, tuple) else key for key in input_keys]\n",
    "            results = self(*inputs, is_output_all=True)\n",
    "            op_shapes = []\n",
    "            for i, op in enumerate(operators_full):\n",
    "                out_node = self.get_to_outnode(op)\n",
    "                node_name = self.get_node_name(out_node)\n",
    "                if \"Cri\" in node_name:\n",
    "                    op_shape = OrderedDict([[key, torch.FloatTensor([1, 0]).to(self.device)] for key in input_keys])\n",
    "                else:\n",
    "                    op_shape = get_op_shape(results[node_name])\n",
    "                op_shapes.append(op_shape)\n",
    "            op_attr[\"op_shapes\"] = op_shapes\n",
    "\n",
    "        if \"3\" in op_attr_modes and inputs is not None and targets is not None and parse_pair_ele is not None:\n",
    "            op_pairs = []\n",
    "            for i, op in enumerate(operators_full):\n",
    "                out_node = self.get_to_outnode(op)\n",
    "                node_name = self.get_node_name(out_node)\n",
    "                inter_results = results[node_name]\n",
    "                pair_data_dict = {}\n",
    "                for key, inter_result in inter_results.items():\n",
    "                    if inter_result.name != DEFAULT_OBJ_TYPE:\n",
    "                        continue\n",
    "                    target = targets[key]\n",
    "                    pair_x, edge_index, edge_attr = get_pair_PyG_data(\n",
    "                        inter_result,\n",
    "                        target,\n",
    "                        OPERATORS,\n",
    "                        parse_pair_ele,\n",
    "                        allowed_attr=allowed_attr,\n",
    "                        cache_dirname=cache_dirname,\n",
    "                    )\n",
    "                    pair_data_dict[key] = {\"x\": pair_x, \"edge_index\": edge_index, \"edge_attr\": edge_attr}\n",
    "                op_pairs.append(pair_data_dict)\n",
    "            op_attr[\"op_pairs\"] = op_pairs\n",
    "        return op_attr\n",
    "\n",
    "\n",
    "    def get_PyG_data(\n",
    "        self,\n",
    "        OPERATORS,\n",
    "        inputs=None,\n",
    "        targets=None,\n",
    "        parse_pair_ele=None,\n",
    "        op_attr_modes=\"0123\",\n",
    "        allowed_attr=\"obj\",\n",
    "        cache_dirname=None,\n",
    "    ):\n",
    "        \"\"\"Get the graph data in PyG format.\n",
    "        Attributes: x: global reprenstation for each operator, where each row is for one operator \n",
    "                                sorted by self.operators_full. See self.get_op_attr() for details.\n",
    "                    edge_index: edge_index for operators in COO format, where the operators' index \n",
    "                                is according to self.operators_full\n",
    "        \"\"\"\n",
    "        from torch_geometric.data import Data\n",
    "        op_attr = self.get_op_attr(\n",
    "            OPERATORS=OPERATORS,\n",
    "            inputs=inputs,\n",
    "            targets=targets,\n",
    "            parse_pair_ele=parse_pair_ele,\n",
    "            op_attr_modes=op_attr_modes,\n",
    "            allowed_attr=allowed_attr,\n",
    "            cache_dirname=cache_dirname,\n",
    "        )\n",
    "        data = Data(x=op_attr, edge_index=self.edge_index, edge_type=self.edge_type)\n",
    "        return data\n",
    "\n",
    "\n",
    "    @property\n",
    "    def operators_dangling(self):\n",
    "        \"\"\"Return operators that are dangling.\"\"\"\n",
    "        dangling_operators = []\n",
    "        for operator in self.operators:\n",
    "            is_dangling = True\n",
    "            op_input_nodes = self.parent_nodes(operator)\n",
    "            for node in op_input_nodes:\n",
    "                if len(self.parent_nodes(node)) > 0:\n",
    "                    is_dangling = False\n",
    "            if is_dangling:\n",
    "                dangling_operators.append(operator)\n",
    "        return dangling_operators\n",
    "\n",
    "\n",
    "    def child_operators(self, operator):\n",
    "        \"\"\"Get the child operators of the current operator.\"\"\"\n",
    "        assert operator in self.operators_full, \"The operator {} does not belong to self.operators_full!\".format(operator)\n",
    "        child_nodes = self.child_nodes(operator)\n",
    "        if len(child_nodes) == 1 and self.get_node_type(child_nodes[0]) == \"fun-out\":\n",
    "            # child_node is an output node:\n",
    "            child_nodes = self.child_nodes(child_nodes[0])\n",
    "        child_operators = [self.operator_name(node) for node in child_nodes]\n",
    "        return child_operators\n",
    "\n",
    "\n",
    "    def parent_operators(self, operator):\n",
    "        \"\"\"Get the child operators of the current operator.\"\"\"\n",
    "        assert operator in self.operators_full, \"The operator {} does not belong to self.operators_full!\".format(operator)\n",
    "        parent_nodes = self.parent_nodes(operator)\n",
    "        if len(parent_nodes) == 1 and (\"^\" in parent_nodes[0] or \"input\" in parent_nodes[0]):\n",
    "            parent_operators = [self.operator_name(parent_nodes[0])]\n",
    "        else:\n",
    "            parent_operators = []\n",
    "            for node1 in parent_nodes:\n",
    "                candidate_nodes = self.parent_nodes(node1)\n",
    "                for node2 in candidate_nodes:\n",
    "                    parent_operators.append(self.operator_name(node2))\n",
    "        return parent_operators\n",
    "\n",
    "\n",
    "    def get_op_embedding(self, op_name, OPERATORS, CONCEPTS):\n",
    "        \"\"\"Get the embedding of a node, used for working memory.\n",
    "\n",
    "        embedding:\n",
    "            op_type_embed: embedding for differentiating different roles in the full corr_graph.\n",
    "            op_name_embed: embedding for the type of the concept or operator\n",
    "            op_API_embed:  embedding for the output type of the op.\n",
    "        \"\"\"\n",
    "        if isinstance(op_name, list):\n",
    "            return torch.cat([self.get_op_embedding(ele, OPERATORS, CONCEPTS) for ele in op_name])\n",
    "        else:\n",
    "            op_mode = split_string(op_name.split(\":\")[-1])[0]\n",
    "            op_type = get_op_type(op_name)\n",
    "            device = self.device\n",
    "            default_embed = torch.zeros(REPR_DIM).to(device)\n",
    "            if op_type == \"op\":\n",
    "                # An operator node:\n",
    "                op_type_embed = torch.FloatTensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]).to(device)\n",
    "                op_name_embed = OPERATORS[op_mode].get_node_repr()\n",
    "                op_outnode_mode = self.get_to_outnode(op_name).split(\":\")[-1]\n",
    "                op_API_embed = CONCEPTS[op_outnode_mode].get_node_repr()\n",
    "            elif op_type == \"op-in\":\n",
    "                # An input or output node:\n",
    "                op_type_embed = torch.FloatTensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]).to(device)\n",
    "                op_name_embed = default_embed\n",
    "                op_API_embed = CONCEPTS[op_mode].get_node_repr()\n",
    "            elif op_type == \"op-attr\":\n",
    "                # An attribute node:\n",
    "                op_type_embed = torch.FloatTensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]).to(device)\n",
    "                op_name_embed = default_embed\n",
    "                op_API_embed = CONCEPTS[op_mode].get_node_repr()\n",
    "            elif op_type == \"op-sc\":\n",
    "                # A concept node in a selector:\n",
    "                op_type_embed = torch.FloatTensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]).to(device)\n",
    "                op_mode = op_name.split(\":\")[-1]\n",
    "                op_name_embed = CONCEPTS[op_mode].get_node_repr()\n",
    "                op_API_embed = default_embed\n",
    "            elif op_type == \"op-so\":\n",
    "                # A relation in a selector:\n",
    "                op_type_embed = torch.FloatTensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]).to(device)\n",
    "                op_mode = op_name.split(\":\")[-1]\n",
    "                op = OPERATORS[op_mode]\n",
    "                op_name_embed = op.get_node_repr()\n",
    "                op_outnode_mode = op.get_to_outnode(op.name).split(\":\")[-1]\n",
    "                op_API_embed = CONCEPTS[op_outnode_mode].get_node_repr()\n",
    "            elif op_type == \"op-op\":\n",
    "                # Operator's inner operator, e.g. \"ForGraph->op$Copy\":\n",
    "                op_type_embed = torch.FloatTensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]).to(device)\n",
    "                op_name_core = split_string(op_name.split(\"$\")[-1])[0]\n",
    "                op_name_embed = OPERATORS[op_name_core].get_node_repr()\n",
    "                op_outnode_mode = self.get_to_outnode(op_name_core).split(\":\")[-1]\n",
    "                op_API_embed = CONCEPTS[op_outnode_mode].get_node_repr()\n",
    "            elif op_type == \"input\":\n",
    "                op_type_embed = torch.FloatTensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]).to(device)\n",
    "                op_name_embed = default_embed\n",
    "                op_API_embed = CONCEPTS[op_mode].get_node_repr()\n",
    "            elif op_type == \"concept\":\n",
    "                op_type_embed = torch.FloatTensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]).to(device)\n",
    "                op_name_embed = default_embed\n",
    "                op_API_embed = CONCEPTS[op_mode].get_node_repr()\n",
    "            elif op_type == \"o\":\n",
    "                op_mode = op_name.split(\"o$\")[-1]\n",
    "                op_type_embed = torch.FloatTensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]).to(device)\n",
    "                op = OPERATORS[op_mode]\n",
    "                op_name_embed = op.get_node_repr()\n",
    "                op_outnode_mode = op.get_to_outnode(op.name).split(\":\")[-1]\n",
    "                op_API_embed = CONCEPTS[op_outnode_mode].get_node_repr()\n",
    "            elif op_type == \"c\":\n",
    "                op_mode = op_name.split(\"c$\")[-1]\n",
    "                op_type_embed = torch.FloatTensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]).to(device)\n",
    "                op = CONCEPTS[op_mode]\n",
    "                op_name_embed = op.get_node_repr()\n",
    "                op_API_embed = default_embed\n",
    "            elif op_type == \"result\":\n",
    "                op_type_embed = torch.FloatTensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]).to(device)\n",
    "                op_name_embed = default_embed\n",
    "                op_API_embed = CONCEPTS[op_mode].get_node_repr()\n",
    "            elif op_type == \"target\":\n",
    "                op_type_embed = torch.FloatTensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]).to(device)\n",
    "                op_name_embed = default_embed\n",
    "                op_API_embed = CONCEPTS[op_mode].get_node_repr()\n",
    "            elif op_type == \"opparse\":\n",
    "                # In the format of \"opparse$Draw->0->obj_1->RotateA\":\n",
    "                op_type_embed = torch.FloatTensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]).to(device)\n",
    "                op_mode_split = op_name.split(\"->\")[-1].split(\"_\")\n",
    "                op_mode = split_string(op_mode_split[0])[0]\n",
    "                op = OPERATORS[op_mode]\n",
    "                op_name_embed = op.get_node_repr()\n",
    "                if len(op_mode_split) > 1 and \"changeColor\" in op_mode_split:\n",
    "                    op_name_embed = op_name_embed + RELATION_EMBEDDING[\"changeColor\"].to(device)\n",
    "                op_outnode_mode = op.get_to_outnode(op.name).split(\":\")[-1]\n",
    "                op_API_embed = CONCEPTS[op_outnode_mode].get_node_repr()\n",
    "            else:\n",
    "                raise Exception(\"op_type '{}' is not supported!\".format(op_type))\n",
    "            embedding = torch.cat([op_type_embed, op_name_embed, op_API_embed])[None, :]\n",
    "            return embedding\n",
    "\n",
    "\n",
    "    def get_neighbors(self, op_name):\n",
    "        \"\"\"Get the neighbor nodes of the op_name given.\n",
    "\n",
    "        Neighbor naming convention:\n",
    "            N-op-parent:    operator's parent op\n",
    "            N-op-child:     operator's child op\n",
    "            N-op-in:        operator's input node\n",
    "            N-opout-c:      operator's output's concept\n",
    "\n",
    "            N-opin-opsc:    from selector root to its consistuent concept\n",
    "            N-opin-opso:    from selector root to its consistuent relations\n",
    "            N-opin-c:       operator's input's concept\n",
    "            N-opin-op:      get the main operator from input.\n",
    "\n",
    "            N-opsc-opin:    from a concept of the selector to the op-in\n",
    "            N-opsc-opso:    from a concept of the selector to a relation in the selector\n",
    "            N-opso-opin:    from a relation of the selector to the op-in\n",
    "            N-opso-opsc:    from a relation of the selector to a concept in the selector\n",
    "\n",
    "            N-attr-c:       get the attribute's concept type\n",
    "            N-attr-parent:  attribute node's parent node\n",
    "            N-attr-attr:    attribute node's child node as its attribute\n",
    "            N-attr-child:   attribute node's child node as a operator node\n",
    "            \n",
    "            N-opop-op:      op-op's container op.\n",
    "\n",
    "            N-input-attr:   get the attribute of input node.\n",
    "            N-input-child:  get the child operator of the input.\n",
    "\n",
    "            N-concept-child: get the child operator of the constant concept.\n",
    "\n",
    "            N-coc:   neighbors in the concept and operator representation.\n",
    "        \"\"\"\n",
    "        op_type = get_op_type(op_name)\n",
    "        neighbors = {}\n",
    "        if op_type == \"op\":\n",
    "            # N-op-parent:\n",
    "            for op_neighbor in self.parent_operators(op_name):\n",
    "                record_data(neighbors, [self.get_node_name(op_neighbor)], [\"N-op-parent\"])\n",
    "            # N-op-child:\n",
    "            for op_neighbor in self.child_operators(op_name):\n",
    "                record_data(neighbors, [self.get_node_name(op_neighbor)], [\"N-op-child\"])\n",
    "            # N-opout-c:\n",
    "            out_node = self.get_to_outnode(op_name)\n",
    "            concept_type = out_node.split(\":\")[-1]\n",
    "            record_data(neighbors, [\"c${}\".format(concept_type)], [\"N-opout-c\"])\n",
    "            # N-op-in:\n",
    "            in_nodes = self.parent_nodes(op_name)\n",
    "            record_data(neighbors, in_nodes, [\"N-op-in\"] * len(in_nodes))\n",
    "\n",
    "        elif op_type == \"op-in\":\n",
    "            # N-opin-c:\n",
    "            concept_type = op_name.split(\":\")[-1]\n",
    "            record_data(neighbors, [\"c${}\".format(concept_type)], [\"N-opin-c\"])\n",
    "\n",
    "            # N-opin-op:\n",
    "            record_data(neighbors, [self.operator_name(op_name)], [\"N-opin-op\"])\n",
    "\n",
    "            # N-opin-opsc/opso:\n",
    "            selector_dict = self.get_selector(op_name)\n",
    "            if len(selector_dict) > 0:\n",
    "                assert len(selector_dict) == 1\n",
    "                op_in_name = next(iter(selector_dict))\n",
    "                selector = selector_dict[op_in_name]\n",
    "                # N-opin-opsc:\n",
    "                for selector_node in selector.nodes:\n",
    "                    selector_node_name = op_in_name + \"->sc$\" + selector_node\n",
    "                    record_data(neighbors, [selector_node_name], [\"N-opin-opsc\"])\n",
    "                # N-opin-opso:\n",
    "                for edge, re_list in selector.get_relations().items():\n",
    "                    for re_name in re_list:\n",
    "                        selector_edge_name = op_in_name + \"->so$\" + f\"({edge[0]},{edge[1]}):{re_name}\"\n",
    "                        record_data(neighbors, [selector_edge_name], [\"N-opin-opso\"])\n",
    "        elif op_type in [\"op-sc\", \"op-so\"]:\n",
    "            # op-sc: e.g. \"Draw-1->sc$obj_0:c0\"\n",
    "            # op-so: e.g. \"Draw-1->so$(obj_0:c0,obj_1:c1):r1\"\n",
    "\n",
    "            # N-opsc/opso-opin:\n",
    "            op_in, ops = op_name.split(\"->\")\n",
    "            if ops.startswith(\"sc$\"):\n",
    "                ops_type = \"opsc\"\n",
    "                ops_neighbor_type = \"opso\"\n",
    "            elif ops.startswith(\"so$\"):\n",
    "                ops_type = \"opso\"\n",
    "                ops_neighbor_type = \"opsc\"\n",
    "            else:\n",
    "                raise\n",
    "            record_data(neighbors, [op_in], [f\"N-{ops_type}-opin\"])\n",
    "\n",
    "            # N-opsc-opso/N-opso-opsc:\n",
    "            selector_dict = self.get_selector(op_in)\n",
    "            selector = selector_dict[next(iter(selector_dict))]\n",
    "            ops_neighbors = selector.get_neighbors(ops.split(\"$\")[-1])\n",
    "            for ops_neighbor in ops_neighbors:\n",
    "                record_data(neighbors, [op_in + f\"->{ops_neighbor_type[2:]}$\" + ops_neighbor], [f\"N-{ops_type}-{ops_neighbor_type}\"])\n",
    "        elif op_type == \"op-attr\":\n",
    "            # N-attr-c:\n",
    "            concept_type = self.get_node_name(op_name).split(\":\")[-1]\n",
    "            record_data(neighbors, [\"c${}\".format(concept_type)], [\"N-attr-c\"])\n",
    "\n",
    "            # N-attr-parent:\n",
    "            attr_parent = self.parent_nodes(op_name)\n",
    "            assert len(attr_parent) == 1\n",
    "            attr_parent = attr_parent[0]\n",
    "            parent_node_type = self.get_node_type(attr_parent)\n",
    "            record_data(neighbors, [self.get_node_name(self.operator_name(attr_parent))], [\"N-attr-parent\"])\n",
    "\n",
    "            # N-attr-child:\n",
    "            attr_children = self.child_nodes(op_name)\n",
    "            for attr_child in attr_children:\n",
    "                child_node_type = self.get_node_type(attr_child)\n",
    "                if child_node_type == \"attr\":\n",
    "                    record_data(neighbors, [attr_child], [\"N-attr-attr\"])\n",
    "                else:\n",
    "                    assert child_node_type == \"fun-in\"\n",
    "                    record_data(neighbors, [self.operator_name(attr_child)], [\"N-attr-child\"])\n",
    "        elif op_type == \"op-op\":\n",
    "            record_data(neighbors, [op_name.split(\"->\")[0]], [\"N-opop-op\"])\n",
    "        elif op_type == \"input\":\n",
    "            input_children = self.child_nodes(op_name)\n",
    "            for input_child in input_children:\n",
    "                input_child_type = self.get_node_type(input_child)\n",
    "                if input_child_type == \"attr\":\n",
    "                    record_data(neighbors, [input_child], [\"N-input-attr\"])\n",
    "                else:\n",
    "                    assert input_child_type == \"fun-in\"\n",
    "                    record_data(neighbors, [self.operator_name(input_child)], [\"N-input-child\"])\n",
    "        elif op_type == \"concept\":\n",
    "            # N-concept-child:\n",
    "            for op_neighbor in self.child_operators(op_name):\n",
    "                record_data(neighbors, [self.operator_name(op_neighbor)], [\"N-concept-child\"])\n",
    "        elif op_type in [\"o\", \"c\", \"result\", \"target\", \"opparse\"]:\n",
    "            pass\n",
    "        else:\n",
    "            raise Exception(\"op_type '{}' is not supported!\".format(op_type))\n",
    "        return neighbors\n",
    "\n",
    "\n",
    "    ########################################\n",
    "    # Operations on PyTorch variables:\n",
    "    ########################################\n",
    "    def init_variable(self, keys, init_all=False):\n",
    "        \"\"\"Initialize PyTorch variables for dangling nodes.\"\"\"\n",
    "        if init_all:\n",
    "            for variable_name in self.get_variables():\n",
    "                delattr(variable_name)\n",
    "        control_nodes = self.control_nodes\n",
    "        if not hasattr(self, \"ground_node_name\"):\n",
    "            self.ground_node_name = DEFAULT_OBJ_TYPE\n",
    "        for node, is_input in self.input_nodes.items():\n",
    "            for key in keys:\n",
    "                variable_name = \"variable_{}_{}\".format(node, key)\n",
    "                # is_dangling is True if the node does not expect any input:\n",
    "                is_dangling = True if is_input and self.get_node_content(node).mode != self.ground_node_name else False\n",
    "                if is_dangling or node in control_nodes:\n",
    "                    if not hasattr(self, variable_name):\n",
    "                        placeholder = self.get_node_content(node)\n",
    "                        tensor = init_tensor(placeholder)\n",
    "                        # For now, don't pass gradients through ungrounded variable\n",
    "                        setattr(self, variable_name, tensor)\n",
    "#                         setattr(self, variable_name, nn.Parameter(tensor))\n",
    "                else:\n",
    "                    if hasattr(self, variable_name):\n",
    "                        delattr(self, variable_name)\n",
    "\n",
    "\n",
    "    def get_variables(self, variable_key=None, example_key=None):\n",
    "        \"\"\"Get all the PyTorch variables (excluding the PyTorch modules).\"\"\"\n",
    "        variable_dict = OrderedDict()\n",
    "        for variable_name, tensor in self.named_parameters():\n",
    "            if variable_name.startswith(\"variable_\"):\n",
    "                if example_key is not None and variable_key is not None:\n",
    "                    key_str = \"_{}_{}\".format(example_key, variable_key)\n",
    "                elif example_key is not None:\n",
    "                    key_str = \"_{}\".format(example_key)\n",
    "                elif variable_key is not None:\n",
    "                    key_str = \"_{}\".format(variable_key)\n",
    "                else:\n",
    "                    key_str = None\n",
    "                if key_str is not None:\n",
    "                    if key_str in variable_name:\n",
    "                        variable_dict[variable_name] = tensor\n",
    "                else:\n",
    "                    variable_dict[variable_name] = tensor\n",
    "        return variable_dict\n",
    "\n",
    "\n",
    "    def get_variables_gen(self, key=None):\n",
    "        \"\"\"Get a generator of all the PyTorch variables (excluding the PyTorch modules).\"\"\"\n",
    "        for variable_name, tensor in self.named_parameters():\n",
    "            if variable_name.startswith(\"variable_\"):\n",
    "                if key is None:\n",
    "                    yield tensor\n",
    "                else:\n",
    "                    if variable_name.endswith(\"_{}\".format(key)):\n",
    "                        yield tensor\n",
    "\n",
    "\n",
    "    def get_targets_from_variables(self):\n",
    "        \"\"\"Obtain a collection of targets from ungrounded variables.\"\"\"\n",
    "        targets_dict = OrderedDict()\n",
    "        for node, is_input in self.input_nodes.items():\n",
    "            if is_input:\n",
    "                variable_dict = self.get_variables(variable_key=node)\n",
    "                if len(variable_dict) > 0:\n",
    "                    concept_name = node.split(\":\")[-1]\n",
    "                    targets = OrderedDict([[try_eval(key.split(\"_\")[-1]), CONCEPTS[concept_name].copy().set_node_value(value.detach())] for key, value in variable_dict.items()])\n",
    "                    targets_dict[node] = targets\n",
    "        return targets_dict\n",
    "\n",
    "\n",
    "    def get_fun_modules(self, key=None):\n",
    "        \"\"\"Get all the PyTorch modules serving as functions.\"\"\"\n",
    "        variable_dict = OrderedDict()\n",
    "        for variable_name, tensor in self.named_parameters():\n",
    "            if variable_name.startswith(\"fun_\"):\n",
    "                if key is not None:\n",
    "                    if variable_name.endswith(\"_{}\".format(key)):\n",
    "                        variable_dict[variable_name] = tensor\n",
    "                else:\n",
    "                    variable_dict[variable_name] = tensor\n",
    "        return variable_dict\n",
    "\n",
    "\n",
    "    def get_fun_modules_gen(self, key=None):\n",
    "        \"\"\"Get a generator of all the PyTorch modules serving as functions.\"\"\"\n",
    "        for variable_name, tensor in self.named_parameters():\n",
    "            if variable_name.startswith(\"fun_\"):\n",
    "                if key is None:\n",
    "                    yield tensor\n",
    "                else:\n",
    "                    if variable_name.endswith(\"_{}\".format(key)):\n",
    "                        yield tensor\n",
    "\n",
    "\n",
    "    def set_variable(self, node_name, tensor, example_keys=None):\n",
    "        \"\"\"Set variable value at node ${node_name}\"\"\"\n",
    "        variable_name = \"variable_{}\".format(node_name) if not node_name.startswith(\"variable_\") else node_name\n",
    "        if not isinstance(tensor, torch.Tensor):\n",
    "            tensor = to_Variable(tensor, is_cuda=self.is_cuda)\n",
    "        if example_keys is None:\n",
    "            if hasattr(self, variable_name):\n",
    "                getattr(self, variable_name).data = tensor\n",
    "            else:\n",
    "                setattr(self, variable_name, nn.Parameter(tensor))\n",
    "        else:\n",
    "            for i, key in enumerate(example_keys):\n",
    "                variable_key = variable_name + \"_{}\".format(key)\n",
    "                if hasattr(self, variable_key):\n",
    "                    getattr(self, variable_key).data = tensor[i]\n",
    "                else:\n",
    "                    setattr(self, variable_key, nn.Parameter(tensor[i]))\n",
    "        return self\n",
    "\n",
    "\n",
    "    def set_variables(self, variable_dict):\n",
    "        \"\"\"Set variable values at given by variable_dict.\"\"\"\n",
    "        for variable_name, tensor in variable_dict.items():\n",
    "            self.set_variable(variable_name, tensor)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def set_variable_values(self, node_names, search_variable_values, shapes, example_keys=None):\n",
    "        \"\"\"Set multiple variable values at once.\"\"\"\n",
    "        start_idx = 0\n",
    "        for idx, variable in enumerate(node_names):\n",
    "            size = int(np.prod(shapes[idx]))\n",
    "            if example_keys is not None:\n",
    "                length = len(example_keys)\n",
    "                shape = (length,) + shapes[idx]\n",
    "            else:\n",
    "                length = 1\n",
    "                shape = shapes[idx]\n",
    "            self.set_variable(variable, \n",
    "                              search_variable_values[start_idx: start_idx + size * length].reshape(shape),\n",
    "                              example_keys=example_keys,\n",
    "                             )\n",
    "            start_idx = start_idx + size * length\n",
    "        return self\n",
    "\n",
    "\n",
    "    def round_variables(self, key=None):\n",
    "        \"\"\"Round all variables whose type is cat or N.\"\"\"\n",
    "        for variable_name, variable in self.get_variables().items():\n",
    "            operator = self.get_node_content(variable_name.split(\"_\")[1]).mode\n",
    "            dtype = CONCEPTS[operator].get_node_content().mode.dtype\n",
    "            if key is None:\n",
    "                if dtype in [\"cat\", \"N\"]:\n",
    "                    self.set_variable(\"_\".join(variable_name.split(\"_\")[1:]), variable.round())\n",
    "                elif dtype in [\"bool\"]:\n",
    "                    self.set_variable(\"_\".join(variable_name.split(\"_\")[1:]), variable.bool())\n",
    "            else:\n",
    "                if variable_name.endswith(\"_{}\".format(key)):\n",
    "                    if dtype in [\"cat\", \"N\"]:\n",
    "                        self.set_variable(\"_\".join(variable_name.split(\"_\")[1:]), variable.round())\n",
    "                    elif dtype in [\"bool\"]:\n",
    "                        self.set_variable(\"_\".join(variable_name.split(\"_\")[1:]), variable.bool())\n",
    "        return self\n",
    "\n",
    "\n",
    "    def remove_variable(self, node_name):\n",
    "        \"\"\"Remove variable held in node ${node_name}\"\"\"\n",
    "        node_name = self.get_node_name(node_name)\n",
    "        variable_dict = self.get_variables(variable_key=node_name)\n",
    "        for variable_name in variable_dict:\n",
    "            delattr(self, variable_name)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def remove_all_variables(self):\n",
    "        \"\"\"Remove all PyTorch variables.\"\"\"\n",
    "        for variable_name in self.get_variables():\n",
    "            delattr(self, variable_name)\n",
    "        return self\n",
    "\n",
    "\n",
    "    ########################################\n",
    "    # Operations on the graph structure:\n",
    "    ########################################\n",
    "    def connect_nodes(self, node1, node2, type=None):\n",
    "        \"\"\"Connect an output node to an arg node.\"\"\"\n",
    "        node_source = self.get_node_name(node1)\n",
    "        node_target = self.get_node_name(node2)\n",
    "        node_source_content = self.get_node_content(node_source)\n",
    "        node_target_content = self.get_node_content(node_target)\n",
    "        if isinstance(node_target_content, Graph):\n",
    "            # Target node is a criteria node:\n",
    "            source_mode = node_source.split(\":\")[-1]\n",
    "            if type is None:\n",
    "                id = len(self.parent_nodes(node_target)) + 1\n",
    "                arg_node_name = \"{}-{}:{}\".format(node_target, id, source_mode)\n",
    "                self.add_node(arg_node_name, type=\"fun-in\")\n",
    "                self.add_edge(node_source, arg_node_name, type=\"inter-criteria\")\n",
    "                self.add_edge(arg_node_name, node_source, type=\"b-inter-criteria\")\n",
    "                self.add_edge(arg_node_name, node_target, type=\"intra\")\n",
    "                self.add_edge(node_target, arg_node_name, type=\"b-intra\")\n",
    "                self.get_node_content(node_target).init_input_placeholder_nodes({id: source_mode})\n",
    "            elif type == \"Ctrl\":\n",
    "                assert self.get_node_type(node_source) == \"fun-in\", \"Only input nodes can be fed into a control node.\"\n",
    "                ctrl_node_name = \"{}-0:Ctrl\".format(node_target)\n",
    "                self.add_node(ctrl_node_name, type=\"fun-in\")\n",
    "                self.add_edge(node_source, ctrl_node_name, type=\"inter-criteria\")\n",
    "                self.add_edge(ctrl_node_name, node_source, type=\"b-inter-criteria\")\n",
    "                self.add_edge(ctrl_node_name, node_target, type=\"intra\")\n",
    "                self.add_edge(node_target, ctrl_node_name, type=\"b-intra\")\n",
    "            else:\n",
    "                raise\n",
    "        else:\n",
    "            self.add_edge(node_source, node_target, type=\"inter-input\")\n",
    "            self.add_edge(node_target, node_source, type=\"b-inter-input\")\n",
    "\n",
    "        if (node_source, node_target) in self.edges:\n",
    "            # Remove ungrounded variable after connection:\n",
    "            if len(self.get_variables(variable_key=node_target)) > 0:\n",
    "                self.remove_variable(node_target)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def sort_nodes(self, *nodes):\n",
    "        \"\"\"Sort the two operators according to the topological sort.\"\"\"\n",
    "        nodes = [self.get_node_name(node) for node in nodes]\n",
    "        nodes_sorted = [node for node in self.topological_sort if node in nodes]\n",
    "        return tuple(nodes_sorted)\n",
    "\n",
    "\n",
    "    def add_connection(self, source_operator, target_operator):\n",
    "        \"\"\"Connect two operators.\"\"\"\n",
    "        source_operator = self.get_node_name(self.operator_name(source_operator))\n",
    "        target_operator = self.get_node_name(self.operator_name(target_operator))\n",
    "        node1 = self.get_to_outnode(source_operator)\n",
    "        mode = node1.split(\":\")[-1]\n",
    "        is_connected = False\n",
    "        in_nodes = self.get_node_input(target_operator, mode=mode)\n",
    "        if in_nodes is None:\n",
    "            return is_connected\n",
    "        for node2 in in_nodes:\n",
    "            if len(self.parent_nodes(node2)) > 0 and \"multi*\" not in node2:\n",
    "                continue\n",
    "            node2_mode = canonical(node2.split(\":\")[-1])\n",
    "            if node2_mode == mode or Placeholder(node2_mode).accepts(Placeholder(mode)):\n",
    "                self.connect_nodes(node1, node2)\n",
    "                is_connected = True\n",
    "                break\n",
    "        if self.verbose >= 1 and not is_connected:\n",
    "            print(\"Fail to connect {} and {}.\".format(source_operator, target_operator))\n",
    "        return is_connected\n",
    "\n",
    "\n",
    "    def is_neighbor_op(self, operator1, operator2):\n",
    "        is_neighbor = False\n",
    "        parent_ops = self.parent_operators(operator1)\n",
    "        child_ops = self.child_operators(operator1)\n",
    "        if self.operator_name(operator2) in parent_ops or self.operator_name(operator2) in child_ops:\n",
    "            is_neighbor = True\n",
    "        return is_neighbor\n",
    "\n",
    "\n",
    "    def break_connection(self, operator1, operator2):\n",
    "        \"\"\"Break the connection between two operators.\"\"\"\n",
    "        source_node, target_node = self.sort_nodes(operator1, operator2)\n",
    "        is_connected, node_name_path = self.get_path(source_node, target_node)\n",
    "\n",
    "        assert is_connected, \"There is no path from '{}' to '{}'\".format(source_node, target_node)\n",
    "        node1, node2 = node_name_path[-3:-1]\n",
    "        assert \"inter\" in self[node1][node2][0][\"type\"], \"The connection between {} and {} is not an inter-operator edge!\".format(node1, node2)\n",
    "        self.remove_edge(node1, node2)\n",
    "        self.remove_edge(node2, node1)\n",
    "        return self\n",
    "\n",
    "\n",
    "    @property\n",
    "    def isolated_operators(self):\n",
    "        strongly_connected_components = sorted(nx.strongly_connected_components(self), key=len)\n",
    "        strongly_connected_operators = [list({node.split(\"-\")[0] for node in strongly_connected_component}) for strongly_connected_component in strongly_connected_components]\n",
    "        operators_isolated = []\n",
    "        if len(strongly_connected_operators) > 1:\n",
    "            for operator_list in strongly_connected_operators:\n",
    "                if len(operator_list) == 1 and \"input\" not in operator_list[0]:\n",
    "                    operators_isolated += operator_list\n",
    "        return operators_isolated\n",
    "\n",
    "\n",
    "    def init_input_placeholder_nodes(self, node_dict=None):\n",
    "        \"\"\"Initialize input_place_holder_nodes, where the node_dict={i: mode} are input-{i}:{mode} pairs.\"\"\"\n",
    "        node_dict = {1: self.ground_node_name if self.ground_node_name is not None else DEFAULT_OBJ_TYPE} if node_dict is None else node_dict\n",
    "        for i, mode in node_dict.items():\n",
    "            try:\n",
    "                self.get_node_name(\"input-{}\".format(i))\n",
    "            except:\n",
    "                self.add_node(\"input-{}:{}\".format(i, mode),\n",
    "                              value=Placeholder(mode), type=\"input\")\n",
    "        return self\n",
    "\n",
    "\n",
    "    def connect_input_placeholder_nodes(self, node_dict=None):\n",
    "        \"\"\"Feed each ground node to the all the ungrounded input nodes that matches.\"\"\"\n",
    "#         if len(self.input_placeholder_nodes) == 0:\n",
    "        self.init_input_placeholder_nodes(node_dict=node_dict)\n",
    "        input_placeholder_node_connected = []\n",
    "        for input_node, is_input in self.input_nodes.items():\n",
    "            if is_input and len(self.get_variables(variable_key=input_node)) == 0:\n",
    "                content = self.nodes(data=True)[input_node]\n",
    "                for i, node in enumerate(self.input_placeholder_nodes):\n",
    "                    if Placeholder(input_node.split(\":\")[-1]).accepts(Placeholder(node.split(\":\")[-1])):\n",
    "                        if node not in input_placeholder_node_connected:\n",
    "                            self.connect_nodes(node, input_node)\n",
    "                            input_placeholder_node_connected.append(node)\n",
    "                            break\n",
    "                        else:\n",
    "                            if i == len(self.input_placeholder_nodes) - 1:\n",
    "                                self.connect_nodes(node, input_node)\n",
    "                                break\n",
    "        return self\n",
    "\n",
    "\n",
    "    def add_get_attr(self, node_name, attr_name):\n",
    "        \"\"\"Add an operation that obtains the attribute of the output at 'node_name'.\"\"\"\n",
    "        node_name = self.get_node_name(node_name)\n",
    "        assert self.get_node_type(node_name) in [\"fun-out\", \"input\", \"attr\"], \"Only output node, attribute node and input-placeholder can get attributes!\"\n",
    "        concepts = combine_dicts([CONCEPTS, NEW_CONCEPTS])\n",
    "        concept = concepts[node_name.split(\":\")[-1]]\n",
    "        is_connected = False\n",
    "        for concept_attr_name in concept.attributes:\n",
    "            if concept_attr_name.split(\":\")[0] == attr_name:\n",
    "                is_connected = True\n",
    "                break\n",
    "        if is_connected:\n",
    "            attr_name = concept.get_node_name(attr_name)\n",
    "            attr_mode = attr_name.split(\":\")[-1]\n",
    "            output_name = \"{}^{}\".format(node_name.split(\":\")[0], attr_name)\n",
    "            self.add_node(output_name, value=Placeholder(attr_mode), type=\"attr\")\n",
    "            self.add_edge(node_name, output_name, type=\"intra-attr\")\n",
    "            self.add_edge(output_name, node_name, type=\"b-intra-attr\")\n",
    "        return is_connected\n",
    "\n",
    "\n",
    "    def remove_get_attr(self, node_name, attr_node_name):\n",
    "        \"\"\"Remove an operation that obtains the attribute of the output at 'node_name'\"\"\"\n",
    "        node_name = self.get_node_name(node_name)\n",
    "        attr_node_name = self.get_node_name(attr_node_name)\n",
    "        assert self.get_node_type(node_name) in [\"fun-out\", \"input\", \"attr\"], \"Only output node, attribute node and input-placeholder can get attributes!\"\n",
    "        assert attr_node_name in self.child_nodes(node_name), \"the attribute node {} must be the child_node of the {}\".format(attr_node_name, node_name)\n",
    "        path = self.get_path_to_output(attr_node_name)\n",
    "        self.remove_nodes_from(path)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def propose_recruit(self, operator_name, includes=[\"input\", \"output\"]):\n",
    "        \"\"\"Find all the possible operators that can connect to the operator in the current Graph.\"\"\"\n",
    "        recruit_candidates = {}\n",
    "\n",
    "        if \"output\" in includes:\n",
    "            # Get output recruit candidates:\n",
    "            output_node = self.child_nodes(operator_name)\n",
    "            assert len(output_node) == 1\n",
    "            output_node = output_node[0]\n",
    "            assert self.get_node_type(output_node) in [\"fun-out\", \"attr\"]\n",
    "            if len(self.child_nodes(output_node)) == 0:\n",
    "                output_mode = output_node.split(\":\")[-1]\n",
    "                recruit_candidates[output_node]= INPUT_MODE_DICT[output_mode]\n",
    "\n",
    "        if \"input\" in includes:\n",
    "            # Get input recruit candidates:\n",
    "            input_nodes = self.parent_nodes(operator_name)\n",
    "            input_nodes_global = self.input_nodes\n",
    "            for input_node in input_nodes:\n",
    "                assert self.get_node_type(input_node) == \"fun-in\"\n",
    "                if input_nodes_global[input_node] is True:\n",
    "                    input_mode = input_node.split(\":\")[-1]\n",
    "                    recruit_candidates[input_node] = OUTPUT_MODE_DICT[input_mode]\n",
    "        return recruit_candidates\n",
    "\n",
    "\n",
    "    def propose_recruit_all(self):\n",
    "        \"\"\"Find all the possible ways the current Graph can recruit a new operator.\"\"\"\n",
    "        recruit_cand_dict = {}\n",
    "        for operator_name in self.operators:\n",
    "            if operator_name not in self.goals:\n",
    "                recruit_cand_dict[operator_name] = self.propose_recruit(\n",
    "                    operator_name, includes=[\"input\", \"output\"])\n",
    "        return recruit_cand_dict\n",
    "\n",
    "\n",
    "    def get_subgraph(self, operator_names, includes_constant=True):\n",
    "        \"\"\"Obtain the subgraph corresponding to the operator_names.\n",
    "        \n",
    "        Args:\n",
    "            operator_names: list of strings indicating the operators to be included in the subgraph.\n",
    "        \"\"\"\n",
    "        if isinstance(operator_names, str):\n",
    "            operator_names = [operator_names]\n",
    "        assert isinstance(operator_names, list)\n",
    "        # Collect the nodes corresponding to operator_names:\n",
    "        nodes = []\n",
    "        for operator_name in operator_names:\n",
    "            assert operator_name in self.operators, \"Operator {} is not in self.operators!\".format(operator_name)\n",
    "            for node in self.nodes:\n",
    "                if node.startswith(\"{}\".format(operator_name)):\n",
    "                    nodes.append(node)\n",
    "                    if includes_constant:\n",
    "                        for parent_node in self.parent_nodes(node):\n",
    "                            if self.get_node_type(parent_node) == \"concept\":\n",
    "                                nodes.append(parent_node)\n",
    "        # Obtain subgraph:\n",
    "        subgraph = Graph(self.subgraph(nodes))\n",
    "        # Set up operators, goals and ground_node_name:\n",
    "        subgraph.operators = operator_names\n",
    "        subgraph.name = subgraph.operators_core[0]\n",
    "        subgraph.goals = [operator_name for operator_name in self.goals if operator_name in operator_names]\n",
    "        subgraph.funs = [operator_name for operator_name in self.funs if operator_name in operator_names]\n",
    "        subgraph.ground_node_name = self.ground_node_name\n",
    "        # Set up PyTorch variables for subgraph:\n",
    "        subgraph.remove_all_variables()\n",
    "        for variable_name, tensor in self.get_variables().items():\n",
    "            for operator_name in operator_names:\n",
    "                if operator_name in variable_name:\n",
    "                    setattr(subgraph, variable_name, nn.Parameter(tensor))\n",
    "        # Set up Pytorch Modules:\n",
    "        for fun_name in self.funs:\n",
    "            for operator_name in operator_names:\n",
    "                if fun_name in operator_names:\n",
    "                    setattr(subgraph, \"fun_{}\".format(fun_name), getattr(self, \"fun_{}\".format(fun_name)))\n",
    "        return subgraph\n",
    "\n",
    "\n",
    "    def add_subgraphs(self, subgraphs, **kwargs):\n",
    "        \"\"\"Add multiple subgraphs into the current graph.\"\"\"\n",
    "        for subgraph in subgraphs:\n",
    "            self.add_subgraph(subgraph, **kwargs)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def add_subgraph(\n",
    "        self,\n",
    "        subgraph,\n",
    "        is_tentative=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Add subgraph. If failing to connect the current graph with the subgraph, revert to original graph.\"\"\"\n",
    "        if not is_tentative:\n",
    "            self.add_subgraph_core(subgraph, **kwargs)\n",
    "            return self\n",
    "        else:\n",
    "            g = deepcopy(self)\n",
    "            is_connected = g.add_subgraph_core(subgraph, **kwargs)\n",
    "            if not is_connected:\n",
    "                return self\n",
    "            else:\n",
    "                # Replace self by g:\n",
    "                self.__dict__.update(g.__dict__)\n",
    "                return self\n",
    "\n",
    "\n",
    "    def add_subgraph_core(\n",
    "        self,\n",
    "        subgraph,\n",
    "        is_constant=False,\n",
    "        add_full_concept=True,\n",
    "        rename_conflict=True,\n",
    "        is_share_fun=False,\n",
    "        is_connect=False,\n",
    "        is_remove_input_placeholder=True,\n",
    "    ):\n",
    "        \"\"\"Add a new subgraph into the current graph.\n",
    "\n",
    "        Args:\n",
    "            subgraph: a Graph() object to be added to the current graph, or a Concept() object to act as input or constant.\n",
    "            is_constant: (only for concept subgraph) if True, the concept added will become a concept node, otherwise it will be an input node.\n",
    "            add_full_concept: (only for concept subgraph) if True, all the properties of the concept will be expanded from the input node.\n",
    "            rename_conflict: (only for operator subgraph) if True, rename conflicting operators in subgraph.\n",
    "                If False, conflicting operators in subgraph will overwrite the ones in self.\n",
    "            is_share_fun: (only for operator subgraph) if True, operators with conflicting names will still share the PyTorch modules\n",
    "            is_connect: if True, will connect the current output of self to the input of newly added subgraph.\n",
    "        \"\"\"\n",
    "        # If it is a Concept node:\n",
    "        if isinstance(subgraph, Concept):\n",
    "            if is_constant:\n",
    "                self.last_added_name = \"concept-{}:{}\".format(len(self.constant_concept_nodes) + 1, subgraph.name)\n",
    "                self.add_node(self.last_added_name, value=subgraph.copy(), type=\"concept\")\n",
    "            else:\n",
    "                if add_full_concept in [True, \"basic\"]:\n",
    "                    base_name = \"input-{}\".format(len(self.input_placeholder_nodes) + 1)\n",
    "                    self.last_added_name = input_name = \"{}:{}\".format(base_name, subgraph.name)\n",
    "                    self.add_node(input_name, type=\"input\", value=Placeholder(subgraph.name), fun=subgraph.get_node_fun())\n",
    "                    if add_full_concept is True:\n",
    "                        nodes_to_add = list(subgraph.nodes)\n",
    "                    elif add_full_concept == \"basic\":\n",
    "                        nodes_to_add = [subgraph.name]\n",
    "                    for node in nodes_to_add:\n",
    "                        if node == subgraph.name:\n",
    "                            node1 = \"{}:{}\".format(base_name, subgraph.name)\n",
    "                        else:\n",
    "                            node1 = \"{}^{}\".format(base_name, node)\n",
    "                        for adj, item in subgraph[node].items():\n",
    "                            if \"b\" not in item[0][\"type\"]:\n",
    "                                node2 = \"{}^{}\".format(base_name, adj)\n",
    "                                if node1 != input_name:\n",
    "                                    self.add_node(node1, type=\"attr\", value=Placeholder(node1.split(\":\")[-1]),\n",
    "                                                  fun=subgraph.get_node_fun(node))\n",
    "                                if node2 != input_name:\n",
    "                                    self.add_node(node2, type=\"attr\" if node2 != input_name else \"input\",\n",
    "                                                  value=Placeholder(adj.split(\":\")[-1]),\n",
    "                                                  fun=subgraph.get_node_fun(adj))\n",
    "                                self.add_edge(node1, node2, type=\"intra-attr\")\n",
    "                                self.add_edge(node2, node1, type=\"b-intra-attr\")\n",
    "                else:\n",
    "                    self.last_added_name = \"input-{}:{}\".format(len(self.input_placeholder_nodes) + 1, subgraph.name)\n",
    "                    self.add_node(self.last_added_name, value=Placeholder(subgraph.name), type=\"input\")\n",
    "            return self\n",
    "\n",
    "        # Rename the operators that have name conflict:\n",
    "        self.rename_mapping = None\n",
    "        if len(self.operators) > 0 and rename_conflict:\n",
    "            mapping = get_rename_mapping(self.operators, subgraph.operators)\n",
    "            if len(mapping) > 0:\n",
    "                subgraph = subgraph.copy(is_share_fun=is_share_fun).rename_operators(mapping)\n",
    "                self.rename_mapping = mapping\n",
    "        \n",
    "        if is_connect:\n",
    "            output_node = self.output_nodes\n",
    "            if len(output_node) > 0:\n",
    "                output_node = output_node[-1]\n",
    "            else:\n",
    "                output_node = None\n",
    "\n",
    "        # Add operator:\n",
    "        self.last_added_name = subgraph.name\n",
    "        operator_copy = deepcopy(subgraph)\n",
    "        if is_remove_input_placeholder:\n",
    "            operator_copy.remove_input_placeholder_nodes()\n",
    "        self.add_nodes_from(operator_copy.nodes(data=True))\n",
    "        self.add_edges_from(operator_copy.edges(data=True))\n",
    "        assert nx.is_directed_acyclic_graph(self.forward_graph(is_copy_module=False))\n",
    "\n",
    "        # Add variables:\n",
    "        for variable_name, tensor in subgraph.get_variables().items():\n",
    "            setattr(self, variable_name, nn.Parameter(tensor))\n",
    "\n",
    "        # Add funs:\n",
    "        self.funs = list(set(self.funs).union(subgraph.funs))\n",
    "        for fun_name in subgraph.funs:\n",
    "            setattr(self, \"fun_{}\".format(fun_name), getattr(subgraph, \"fun_{}\".format(fun_name)))\n",
    "            self.set_node_content(getattr(self, \"fun_{}\".format(fun_name)), fun_name)\n",
    "\n",
    "        # Combine self.operators:\n",
    "        self.operators = remove_duplicates(self.operators + subgraph.operators)\n",
    "\n",
    "        # Combine self.goals:\n",
    "        self.goals = remove_duplicates(self.goals + subgraph.goals)\n",
    "\n",
    "        # Update self.ground_node_name:\n",
    "        if subgraph.ground_node_name is not None:\n",
    "            if self.ground_node_name is None:\n",
    "                self.ground_node_name = subgraph.ground_node_name\n",
    "\n",
    "        # Connect the current output_node with the newly added subgraph:\n",
    "        if is_connect and output_node is not None:\n",
    "            is_connected = self.add_connection(self.operator_name(output_node), self.last_added_name)\n",
    "            return is_connected\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "    def compose(self, subgraph):\n",
    "        \"\"\"Compose a new subgraph into the current graph, where node and edge with the same name are unified.\n",
    "\n",
    "        Args:\n",
    "            subgraph: a Graph() object to be added to the current graph.\n",
    "        \"\"\"\n",
    "        # If it is a Concept node:\n",
    "        assert isinstance(subgraph, Graph)\n",
    "\n",
    "        # Add operator:\n",
    "        subgraph = subgraph.copy()\n",
    "        G = nx.compose(self, subgraph)\n",
    "        assert nx.is_directed_acyclic_graph(G.forward_graph(is_copy_module=False))\n",
    "\n",
    "        # Add variables:\n",
    "        this_variables = self.get_variables()\n",
    "        for variable_name, tensor in subgraph.get_variables().items():\n",
    "            if variable_name not in this_variables:\n",
    "                setattr(G, variable_name, nn.Parameter(tensor))\n",
    "\n",
    "        # Add funs:\n",
    "        G.funs = remove_duplicates(self.funs + subgraph.funs)\n",
    "        for fun_name in subgraph.funs:\n",
    "            setattr(G, \"fun_{}\".format(fun_name), getattr(subgraph, \"fun_{}\".format(fun_name)))\n",
    "\n",
    "        # Combine self.operators:\n",
    "        G.operators = remove_duplicates(self.operators + subgraph.operators)\n",
    "\n",
    "        # Combine self.goals:\n",
    "        G.goals = remove_duplicates(self.goals + subgraph.goals)\n",
    "\n",
    "        # Update self.ground_node_name:\n",
    "        if subgraph.ground_node_name is not None:\n",
    "            if self.ground_node_name is None:\n",
    "                G.ground_node_name = subgraph.ground_node_name\n",
    "        self.__dict__.update(G.__dict__)\n",
    "        return self\n",
    "    \n",
    "     \n",
    "    def incorporate_goal_subgraph(self):\n",
    "        \"\"\"Build a new computation graph that incorporates all goal subgraphs.\"\"\"\n",
    "        G = self.copy()\n",
    "        goal_output_nodes = []\n",
    "        goal_ctrl_nodes = []\n",
    "        for goal_name in self.goals_optm:\n",
    "            goal_output_nodes.append(G.child_nodes(goal_name)[0])\n",
    "            goal_input_nodes = [node for node in G.parent_nodes(goal_name) if not node.endswith(\"-0:Ctrl\")]\n",
    "            arg_nodes = [G.parent_nodes(node)[0] for node in goal_input_nodes]\n",
    "            goal_ctrl = \"{}-0:Ctrl\".format(goal_name)\n",
    "            ctrl_nodes = G.parent_nodes(goal_ctrl)\n",
    "            goal_ctrl_nodes = goal_ctrl_nodes + ctrl_nodes\n",
    "            for arg_node, goal_input_node in zip(arg_nodes, goal_input_nodes):\n",
    "                G.set_edge_type(arg_node, goal_input_node, \"inter-input\")\n",
    "            for ctrl_node in ctrl_nodes:\n",
    "                G.remove_edge(ctrl_node, goal_ctrl)\n",
    "            G.remove_node(goal_ctrl)\n",
    "        G.goals = []\n",
    "        return G, (goal_ctrl_nodes, goal_output_nodes)\n",
    "\n",
    "\n",
    "    def remove_subgraph(self, subgraph, is_rename=True):\n",
    "        \"\"\"Delete the subgraph, including its args and output, from current graph.\n",
    "\n",
    "        Args:\n",
    "            subgraph: a str indicating the name of operator to delete, or a list of strings indicating\n",
    "                the list of names of operators to delete, or a Graph() object to delte.\n",
    "        \"\"\"\n",
    "        nodes_to_remove = []\n",
    "        if isinstance(subgraph, str):\n",
    "            operator_names = [subgraph]\n",
    "        elif isinstance(subgraph, list):\n",
    "            operator_names = subgraph\n",
    "        else:\n",
    "            operator_names = deepcopy(subgraph.operators)\n",
    "\n",
    "        # Remove nodes:\n",
    "        for node in self.nodes:\n",
    "            for operator_name in operator_names:\n",
    "                if node.startswith(\"{}\".format(operator_name)):\n",
    "                    if node.startswith(\"{}-\".format(operator_name)):\n",
    "                        nodes_to_remove.append(node)\n",
    "                    else:\n",
    "                        if node == operator_name:\n",
    "                            nodes_to_remove.append(node)\n",
    "        self.remove_nodes_from(nodes_to_remove)\n",
    "\n",
    "        # Remove operators from self.operators and self.goals:\n",
    "        for operator_name in operator_names:\n",
    "            self.operators.remove(operator_name)\n",
    "            if operator_name in self.goals:\n",
    "                self.goals.remove(operator_name)\n",
    "\n",
    "        # Remove PyTorch variables:\n",
    "        for variable_name in self.get_variables():\n",
    "            for operator_name in operator_names:\n",
    "                if \"variable_{}-\".format(operator_name) in variable_name:\n",
    "                    delattr(self, variable_name)\n",
    "\n",
    "        # Remove Pytorch funs:\n",
    "        funs_to_remove = []\n",
    "        for fun_name in self.funs:\n",
    "            if fun_name in operator_names:\n",
    "                funs_to_remove.append(fun_name)\n",
    "                delattr(self, \"fun_{}\".format(fun_name))\n",
    "        self.funs = list(set(self.funs).difference(set(funs_to_remove)))\n",
    "        \n",
    "        # Canonicalize operators:\n",
    "        if is_rename:\n",
    "            mapping = canonicalize_strings(self.operators)\n",
    "            self.rename_operators(mapping)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def remove_input_placeholder_nodes(self):\n",
    "        \"\"\"Remove input_placeholder_nodes.\"\"\"\n",
    "        nodes_to_remove = self.input_placeholder_nodes\n",
    "        self.remove_nodes_from(nodes_to_remove)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def preserve_subgraph(self, output_nodes, level=\"node\"):\n",
    "        \"\"\"Preserve the minimal subgraph such that all the nodes in output_nodes can be calculated.\"\"\"\n",
    "        if not isinstance(output_nodes, list):\n",
    "            output_nodes = [output_nodes]\n",
    "        nodes_required = self.get_ancestors(output_nodes, includes_self=True)\n",
    "        if level in [\"operator\", \"node\"]:\n",
    "            operators_to_preserve = remove_duplicates([node.split(\"-\")[0] for node in nodes_required])\n",
    "            operators_to_remove = [operator for operator in self.operators if operator not in operators_to_preserve]\n",
    "            self.remove_subgraph(operators_to_remove)\n",
    "        if level == \"node\":  # Remove all unused nodes\n",
    "            nodes_to_remove = [node for node in self.nodes if node not in nodes_required]\n",
    "            for node in nodes_to_remove:\n",
    "                if node in self.nodes:\n",
    "                    self.remove_node(node)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def rename_operators(self, mapping):\n",
    "        \"\"\"Rename operators based on the dictionary of mapping.\"\"\"\n",
    "        # Expand mapping to include all relevant nodes:\n",
    "        mapping_expanded = {}\n",
    "        for node_name in self.nodes:\n",
    "            for operator_name, new_operator_name in mapping.items():\n",
    "                if node_name.startswith(operator_name):\n",
    "                    mapping_expanded[node_name] = new_operator_name + node_name[len(operator_name):]\n",
    "\n",
    "        # Build a new graph:\n",
    "        G = nx.relabel_nodes(self, mapping_expanded)\n",
    "\n",
    "        # Build other properties:\n",
    "        G.operators = [mapping[c] if c in mapping else c for c in self.operators]\n",
    "        G.goals = [mapping[c] if c in mapping else c for c in self.goals]\n",
    "        G.funs = [mapping[c] if c in mapping else c for c in self.funs]\n",
    "        G.ground_node_name = mapping[self.ground_node_name] if self.ground_node_name in mapping else self.ground_node_name\n",
    "        if G.name in mapping:\n",
    "            G.name = mapping[G.name]\n",
    "        for variable_name, variable in self.get_variables().items():\n",
    "            variable_name_new = \"variable_\" + mapping_expanded[variable_name[9:]] if variable_name[9:] in mapping_expanded else variable_name\n",
    "            setattr(G, variable_name_new, nn.Parameter(variable))\n",
    "        for fun_name in self.funs:\n",
    "            fun_name_new = mapping_expanded[fun_name] if fun_name in mapping_expanded else fun_name\n",
    "            setattr(G, \"fun_\" + fun_name_new, getattr(self, \"fun_{}\".format(fun_name)))\n",
    "            G.set_node_content(getattr(G, \"fun_\" + fun_name_new), fun_name_new)\n",
    "        self.__dict__.update(G.__dict__)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def add_And_over_bool(self):\n",
    "        \"\"\"If the number of Boolean output_node is greater than 1, combine them using And.\"\"\"\n",
    "        bool_output_nodes = []\n",
    "        for output_node in self.get_output_nodes(types=[\"fun-out\", \"attr\", \"input\"], dangling_mode=True):\n",
    "            mode = output_node.split(\":\")[-1]\n",
    "            if mode == \"Bool\":\n",
    "                bool_output_nodes.append(output_node)\n",
    "        if len(bool_output_nodes) > 1:\n",
    "            self.add_subgraph(OPERATORS[\"And\"])\n",
    "            for output_node in bool_output_nodes:\n",
    "                self.connect_nodes(output_node, \"And-1\")\n",
    "            bool_output_node = \"And-o:Bool\"\n",
    "        elif len(bool_output_nodes) == 1:\n",
    "            bool_output_node = bool_output_nodes[0]\n",
    "        else:\n",
    "            bool_output_node = None\n",
    "        return bool_output_node\n",
    "\n",
    "\n",
    "    def add_Or_over_bool(self):\n",
    "        \"\"\"If the number of Boolean output_node is greater than 1, combine them using Or.\"\"\"\n",
    "        bool_output_nodes = []\n",
    "        for output_node in self.get_output_nodes(types=[\"fun-out\", \"attr\", \"input\"], dangling_mode=True):\n",
    "            mode = output_node.split(\":\")[-1]\n",
    "            if mode == \"Bool\":\n",
    "                bool_output_nodes.append(output_node)\n",
    "        if len(bool_output_nodes) > 1:\n",
    "            self.add_subgraph(OPERATORS[\"Or\"])\n",
    "        for output_node in bool_output_nodes:\n",
    "            self.connect_nodes(output_node, \"Or-1\")\n",
    "        return self\n",
    "\n",
    "\n",
    "    def abstract(self, node_dict=None):\n",
    "        \"\"\"Turn the graph into a operator node.\"\"\"\n",
    "        g = self.copy()\n",
    "        g.connect_input_placeholder_nodes(node_dict=node_dict)\n",
    "        args = [Placeholder(node_name.split(\":\")[-1]) for node_name in g.input_placeholder_nodes]\n",
    "        output_nodes = self.get_output_nodes(types=[\"attr\", \"fun-out\"], dangling_mode=True)\n",
    "        for node_name in reversed(self.topological_sort):\n",
    "            if node_name in output_nodes:\n",
    "                break\n",
    "        output_mode = node_name.split(\":\")[-1]\n",
    "        G = Graph(name=self.name,\n",
    "                  repr=to_Variable(torch.rand(REPR_DIM), is_cuda=self.is_cuda),\n",
    "                  forward={\"args\": args,\n",
    "                           \"output\": Placeholder(output_mode),\n",
    "                           \"fun\": g,\n",
    "                          })\n",
    "        outer_node_dict = OrderedDict()\n",
    "        for i, arg in enumerate(args):\n",
    "            outer_node_dict[i + 1] = arg.mode\n",
    "        G.remove_input_placeholder_nodes()\n",
    "        return G\n",
    "\n",
    "\n",
    "    def to(self, device):\n",
    "        super(Graph, self).to(device)\n",
    "        selector_dict = self.get_selectors()\n",
    "        for key in selector_dict:\n",
    "            selector_dict[key].to(device)\n",
    "        return self\n",
    "\n",
    "\n",
    "    @property\n",
    "    def DL(self):\n",
    "        \"\"\"Recursively computing DL.\"\"\"\n",
    "        DL = len(self.operators_full) + self.edge_index.shape[1]\n",
    "        for op_name in self.operators_core:\n",
    "            op = self.get_node_content(op_name)\n",
    "            if isinstance(op, BaseGraph):\n",
    "                DL += op.DL\n",
    "        return DL\n",
    "\n",
    "\n",
    "class Do(BaseGraph):\n",
    "    \"\"\"A selector, that selects one of the operator to run. If selector is not given,\n",
    "    output results from all operators.\n",
    "    \"\"\"\n",
    "    def __init__(self, G=None, **kwargs):\n",
    "        self.operators = []\n",
    "        super(Do, self).__init__(G=G, **kwargs)\n",
    "        self.name = \"Do\"\n",
    "\n",
    "\n",
    "    def forward(self, selector, *inputs, **kwargs):\n",
    "        \"\"\"Forward function. \n",
    "        \n",
    "        Args:\n",
    "            selector: A categorical PyTorch Tensor. If selector is None, output all possible outputs.\n",
    "            inputs: the same format as the inputs in the forward() in Graph().\n",
    "            \n",
    "        Returns:\n",
    "            resuls: the same format as the results in the forward() in Graph().\n",
    "        \"\"\"\n",
    "        if isinstance(selector, Concept):\n",
    "            selector = selector.get_root_value()\n",
    "        if selector is not None:   \n",
    "            selector = to_np_array(selector.long())\n",
    "            assert selector < len(self.operators)\n",
    "\n",
    "            operator_chosen = self.operators[selector]\n",
    "            results = operator_chosen(*inputs, **kwargs)\n",
    "        else:\n",
    "            results = OrderedDict()\n",
    "            for i, operator in enumerate(self.operators):\n",
    "                result = operator(*inputs, **kwargs)\n",
    "                if isinstance(result, dict):\n",
    "                    for key, item in result.items():\n",
    "                        if not isinstance(key, tuple):\n",
    "                            key = (key,)\n",
    "                        results[key + (\"Do-{}-{}\".format(i, operator.name),)] = item\n",
    "                else:\n",
    "                    results[\"Do-{}-{}\".format(i, operator.name)] = result\n",
    "        return results\n",
    "\n",
    "\n",
    "    def add_subgraph(self, subgraph):\n",
    "        \"\"\"Add operator to self's self.operators collection.\"\"\"\n",
    "        for operator in self.operators:\n",
    "            assert subgraph.name != operator.name\n",
    "        self.last_added_name = subgraph.name\n",
    "        self.operators.append(subgraph)\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        repr_str = self.graph[\"name\"] if \"name\" in self.graph else \"Graph\"\n",
    "        # Composing content string:\n",
    "        content_str = \"\"\n",
    "        operator_list = []\n",
    "        for operator in self.operators:\n",
    "            operator_list.append(operator.name)\n",
    "            operator.draw()\n",
    "        if len(operator_list) > 0:\n",
    "            content_str += \"operators={}, \".format(operator_list)\n",
    "        return '{}({})'.format(repr_str, content_str[:-2])\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "\n",
    "    def draw(self):\n",
    "        \"\"\"Draw the operator components.\"\"\"\n",
    "        if len(self.operators) > 0:\n",
    "            for operator in self.operators:\n",
    "                print(\"{}:\".format(operator.name))\n",
    "                operator.draw()\n",
    "\n",
    "\n",
    "    @property\n",
    "    def DL(self):\n",
    "        return len(self.operators) + 1  # 1 is for input node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForGraph(Graph):\n",
    "    \"\"\"Perform While Loop on the provided G as actioin and criteria is satisfied, break the loop.\"\"\"\n",
    "    def __init__(self, G=None, criteria=None):\n",
    "        super(ForGraph, self).__init__(G=G)\n",
    "        self.criteria = criteria\n",
    "        self.MAX_ITER = 100\n",
    "\n",
    "    def forward(self, *inputs):\n",
    "        inputs = list(deepcopy(inputs))\n",
    "        outputs = {}\n",
    "        for i in range(self.MAX_ITER):\n",
    "            output_cand = super(ForGraph, self).forward(*inputs)\n",
    "            inputs[0] = output_cand\n",
    "            if self.criteria(*inputs):\n",
    "                break\n",
    "            else:\n",
    "                output = output_cand\n",
    "                outputs[\"obj_{}:Image\".format(i)] = output\n",
    "        output_comp, pos_bounding = get_comp_obj(outputs, CONCEPTS)\n",
    "        output_comp.set_node_value(pos_bounding, \"pos\")\n",
    "        return output_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Execute action that edit the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_action(\n",
    "    graph,\n",
    "    action,\n",
    "    OPERATORS,\n",
    "    CONCEPTS,\n",
    "    max_ops=30,\n",
    "    allowed_modes=\"01234\",\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Action space:\n",
    "        A0: [2 * N + 1]: which graph it is modifying, 0 for the outmost operator graph. (2*0+2, 2*1+2, ... , 2*(N-1)+2) for the \n",
    "            operator graph inside the operator i (as sorted by topological_sort), and (2*0+1, 2*1+1, ... , 2*(N-1)+1) for the \n",
    "            selector inside the operator i.\n",
    "        A5 [2]: whether to stop. 0. continue; 1. stop.\n",
    "        if A0 refers to an operator graph:\n",
    "            A1: [N]: source operator\n",
    "            A2: [|Op| + 4 + N + |A|]: target operator\n",
    "            A3: relation type (here does not matter)\n",
    "            A4: add/delete edge or change operator. \n",
    "                0. add edge from A1 to A2\n",
    "                1. add edge from A2 to A1\n",
    "                2. delete edge between A1 and A2\n",
    "                4. change the operator of A1 to A2\n",
    "        if A0 refers to a selector:\n",
    "            A1: [N]: source object\n",
    "            A2: [|Op| + 4 + N]: target object\n",
    "            A3: relation type\n",
    "            A4: add/delete edge or change operator.\n",
    "                0. add relation from A1 to A2\n",
    "                1. add relation from A2 to A1\n",
    "                2. delete relation from A1 to A2\n",
    "                3. delete relation from A2 to A1\n",
    "\n",
    "    OPERATORS: here the OPERATORS can be relation or manipulation operators, or constant concept.\n",
    "    max_ops: maximum number of operators in terms of operators_full.\n",
    "    allow_modes: a string indicating the allowed action in A3. Use subset of \"01234\".\n",
    "    \"\"\"\n",
    "    assert len(action) == 6, \"action must be multi-discrete with 6 elements!\"\n",
    "    has_effect = False\n",
    "    len_OPERATORS = len(OPERATORS) + 4\n",
    "    OPERATOR_KEYS = list(OPERATORS.keys())\n",
    "    if action[5] == 1:\n",
    "        # stop, do nothing:\n",
    "        if verbose >= 1:\n",
    "            print(\"**Stop.\\n\")\n",
    "        return deepcopy(graph), has_effect\n",
    "    elif action[5] == 0:\n",
    "        # do something:\n",
    "        ops_global = graph.operators_full\n",
    "        ops_global_reverse = {id: op for op, id in ops_global.items()}\n",
    "        if action[0] % 2 == 0:\n",
    "            if action[0] == 0:\n",
    "                # Recursion level: outmost\n",
    "                g = deepcopy(graph)\n",
    "                if verbose >= 1:\n",
    "                    print(\"**Operate on the global graph:\")\n",
    "            else:\n",
    "                # Recursion level: at the inner operator graph in operator with op_id=(action[0] - 2) // 2:\n",
    "                graph_id = (action[0] - 2) // 2\n",
    "                graph_node_name = ops_global_reverse[graph_id]\n",
    "                g = deepcopy(graph.get_node_content(graph_node_name))\n",
    "                if verbose >= 1:\n",
    "                    print(\"**Operate on the component graph of op '{}':\".format(graph_node_name))\n",
    "\n",
    "            ops = g.operators_full\n",
    "            ops_reverse = {id: op for op, id in ops.items()}\n",
    "            source_op_id = int(action[1])\n",
    "            target_op_id = int(action[2])\n",
    "\n",
    "            if source_op_id < len(ops):\n",
    "                source_op = ops_reverse[source_op_id]\n",
    "                if target_op_id < len_OPERATORS:\n",
    "                    # Target operator is new from OPERATORS:\n",
    "                    if target_op_id < len(OPERATORS):\n",
    "                        target_op = OPERATORS[OPERATOR_KEYS[target_op_id]]\n",
    "#                     elif target_op_id == len(OPERATORS) + 2:\n",
    "#                         target_op = Graph(\n",
    "#                             name=\"Do\",\n",
    "#                             forward={\n",
    "#                                 \"args\": [Placeholder(DEFAULT_OBJ_TYPE)],\n",
    "#                                 \"output\": Placeholder(DEFAULT_OBJ_TYPE),\n",
    "#                                 \"fun\": Do(),\n",
    "#                             })\n",
    "                    else:\n",
    "                        return graph, has_effect\n",
    "\n",
    "                    if action_equal(action[4], 0, allowed_modes) or action_equal(action[4], 1, allowed_modes):\n",
    "                        if len(g.operators_full) >= max_ops:\n",
    "                            if verbose >= 1:\n",
    "                                print(\"**Number of operators exceeds max_op={}. Stop.\".format(max_ops))\n",
    "                            return graph, has_effect\n",
    "\n",
    "                        # Add an edge from A1 to A2 (or reverse):\n",
    "                        g.add_subgraph(target_op)\n",
    "                        if action_equal(action[4], 0, allowed_modes):\n",
    "                            is_connected = g.add_connection(source_op, g.last_added_name)\n",
    "                            if verbose >= 1:\n",
    "                                print(\"\\t**Connect the output of op '{}' to a new op '{}'\".format(source_op, g.last_added_name))\n",
    "                        elif action_equal(action[4], 1, allowed_modes):\n",
    "                            is_connected = g.add_connection(g.last_added_name, source_op)\n",
    "                            if verbose >= 1:\n",
    "                                print(\"\\t**Add an op '{}' and connect its output to the input of op '{}'\".format(g.last_added_name, source_op))\n",
    "                        if is_connected:\n",
    "                            has_effect = True\n",
    "                        else:\n",
    "                            # Revert to previous graph if cannot connect:\n",
    "                            g = deepcopy(graph)\n",
    "                            if verbose >= 1:\n",
    "                                print(\"\\t\\t**Fail to connect the newly added op. Revert back.\")\n",
    "                    elif action_equal(action[4], 4, allowed_modes):\n",
    "                        # Change node:\n",
    "                        # Input nodes are not allowed to change, and not switch node with the same operator type.\n",
    "                        if \"input\" not in source_op and split_string(source_op)[0] != target_op.name:\n",
    "                            if \"^\" in source_op:\n",
    "                                # source_op is attribute node:\n",
    "                                parent_operator = g.parent_operators(source_op)[0]\n",
    "                                g.remove_get_attr(g.get_to_outnode(parent_operator), source_op)\n",
    "                                g.add_subgraph(target_op)\n",
    "                                is_connect = g.add_connection(parent_operator, g.last_added_name)\n",
    "                                if verbose >= 1:\n",
    "                                    print(\"\\t**Remove attribute '{}' and replace with op '{}'.\".format(source_op, g.last_added_name))\n",
    "                                if is_connect:\n",
    "                                    has_effect = True\n",
    "                                else:\n",
    "                                    g = deepcopy(graph)\n",
    "                                    if verbose >= 1:\n",
    "                                        print(\"\\t\\t**Fail to connect the newly added op. Revert back.\")\n",
    "                            else:\n",
    "                                parent_operators = g.parent_operators(source_op)\n",
    "                                child_operators = g.child_operators(source_op)\n",
    "                                g.remove_subgraph(source_op)\n",
    "                                g.add_subgraph(target_op)\n",
    "                                is_connect_parent = False\n",
    "                                is_connect_child = False\n",
    "                                for parent_op in parent_operators:\n",
    "                                    is_connect_parent = is_connect_parent or g.add_connection(parent_op, g.last_added_name)\n",
    "                                for child_op in child_operators:\n",
    "                                    if \"^\" in child_op:\n",
    "                                        # Has attribute nodes:\n",
    "                                        # Still need to fix the case where there are multiple get_attr nodes and more depth.\n",
    "                                        g.add_get_attr(g.get_to_outnode(g.last_added_name), child_op.split(\"^\")[-1])\n",
    "                                        is_connect_child = True\n",
    "                                    else:\n",
    "                                        is_connect_child = is_connect_child or g.add_connection(g.last_added_name, child_op)\n",
    "                                if verbose >= 1:\n",
    "                                    print(\"\\t**Replace '{}' with '{}'.\".format(source_op, target_op.name))\n",
    "                                if is_connect_parent or is_connect_child:\n",
    "                                    has_effect = True\n",
    "                                else:\n",
    "                                    g = deepcopy(graph)\n",
    "                                    if verbose >= 1:\n",
    "                                        print(\"\\t\\t**Fail to connect with replaced op. Revert back.\")\n",
    "\n",
    "                elif len_OPERATORS <= target_op_id < len_OPERATORS + len(ops):\n",
    "                    # Target operator is within current graph:\n",
    "                    target_op = ops_reverse[target_op_id - len_OPERATORS]\n",
    "                    if source_op != target_op:\n",
    "                        if action_equal(action[4], 0, allowed_modes) or action_equal(action[4], 1, allowed_modes):\n",
    "                            if action_equal(action[4], 0, allowed_modes):\n",
    "                                is_connect = g.add_connection(source_op, target_op)\n",
    "                                if verbose >= 1:\n",
    "                                    print(\"\\t**Add an edge from '{}' to '{}'\".format(source_op, target_op))\n",
    "                            elif action_equal(action[4], 1, allowed_modes):\n",
    "                                is_connect = g.add_connection(target_op, source_op)\n",
    "                                if verbose >= 1:\n",
    "                                    print(\"\\t**Add an edge from '{}' to '{}'\".format(target_op, source_op))\n",
    "                            has_effect = is_connect\n",
    "                            if not has_effect:\n",
    "                                if verbose >= 1:\n",
    "                                    print(\"\\t\\t**Cannot add edge.\")\n",
    "                        elif action_equal(action[4], 2, allowed_modes):\n",
    "                            # Delete an edge:\n",
    "                            is_neighbor = g.is_neighbor_op(source_op, target_op)\n",
    "                            if is_neighbor:\n",
    "                                if \"^\" in target_op:\n",
    "                                    g.remove_get_attr(source_op, target_op)\n",
    "                                    has_effect = True\n",
    "                                    if verbose >= 1:\n",
    "                                        print(\"\\t**Remove the attribute '{}' from '{}'.\".format(target_op, source_op))\n",
    "                                else:\n",
    "                                    g.break_connection(source_op, target_op)\n",
    "                                    # Remove the target_op if it only has one operator:\n",
    "                                    g.remove_subgraph(g.isolated_operators)\n",
    "                                    has_effect = True\n",
    "                                    if verbose >= 1:\n",
    "                                        print(\"\\t**Remove an edge between '{}' and '{}'.\".format(source_op, target_op))\n",
    "                            else:\n",
    "                                if verbose >= 1:\n",
    "                                    print(\"\\t\\t**'{}' and '{}' are not connected. Cannot delete edge.\".format(source_op, target_op))\n",
    "\n",
    "                elif target_op_id >= len(ops) + len_OPERATORS:\n",
    "                    if action_equal(action[4], 0, allowed_modes):\n",
    "                        # Add an attribute:\n",
    "                        source_op_outnode = g.get_to_outnode(source_op)\n",
    "                        concept_name = source_op_outnode.split(\":\")[-1]\n",
    "                        attributes = CONCEPTS[concept_name].attributes\n",
    "                        k = target_op_id - (len(ops) + len_OPERATORS)\n",
    "                        if verbose >= 1:\n",
    "                            print(\"\\t**Add an attribute '{}' on '{}'\".format(concept_name, source_op))\n",
    "                        if k < len(attributes):\n",
    "                            g.add_get_attr(source_op_outnode, attributes[k])\n",
    "                            has_effect = True\n",
    "                        else:\n",
    "                            if verbose >= 1:\n",
    "                                print(\"\\t\\t**The attribute id exceeds the number of attributes for '{}'\".format(source_op))\n",
    "            # use the g:\n",
    "            if action[0] == 0:\n",
    "                graph = g\n",
    "            else:\n",
    "                graph.set_node_content(g, graph_node_name)\n",
    "            if has_effect:\n",
    "                if verbose >= 2:\n",
    "                    print(\"Plotting the operator graph after the action:\")\n",
    "                    graph.draw()\n",
    "                    print(\"=\" * 100 + \"\\n\")\n",
    "\n",
    "        elif action[0] % 2 == 1:\n",
    "            # Recursion level: at the selector of operator with op_id=(action[0] - 2) // 2:\n",
    "            graph_id = (action[0] - 1) // 2\n",
    "            graph_node_name = ops_global_reverse[graph_id]\n",
    "            selector_dict = deepcopy(graph.get_selector(graph_node_name))\n",
    "            g = selector_dict[next(iter(selector_dict))]\n",
    "            print(\"**Operate on '{}''s selector:\".format(graph_node_name))\n",
    "\n",
    "            ops = g.get_graph(\"obj\").topological_sort\n",
    "            ops_reverse = {id: op for id, op in enumerate(ops)}\n",
    "            source_op_id = int(action[1])\n",
    "            target_op_id = int(action[2])\n",
    "\n",
    "            if source_op_id < len(ops):\n",
    "                source_op = ops_reverse[source_op_id]\n",
    "                relation_id = action[3]\n",
    "                relation_name = OPERATOR_KEYS[relation_id]\n",
    "                if target_op_id < len_OPERATORS:\n",
    "                    if action_equal(action[4], 0, allowed_modes) or action_equal(action[4], 1, allowed_modes):\n",
    "                        if len(ops) >= max_ops:\n",
    "                            if verbose >= 1:\n",
    "                                print(\"**Number of operators exceeds max_op={}. Stop.\".format(max_ops))\n",
    "                            return graph, has_effect\n",
    "\n",
    "                        # Add an edge from A1 to A2 (or reverse):\n",
    "                        obj_name = g.add_obj(CONCEPTS[DEFAULT_OBJ_TYPE].copy(), change_root=False, add_full_concept=False)\n",
    "                        if action_equal(action[4], 0, allowed_modes):\n",
    "                            g.add_relation_manual(relation_name, source_op, obj_name)\n",
    "                            if verbose >= 1:\n",
    "                                print(\"\\t**Add a relation '{}' from '{}' to a new '{}'.\".format(relation_name, source_op, obj_name))\n",
    "                        elif action_equal(action[4], 1, allowed_modes):\n",
    "                            g.add_relation_manual(relation_name, obj_name, source_op)\n",
    "                            if verbose >= 1:\n",
    "                                print(\"\\t**Add a relation '{}' from a new '{}' to '{}'.\".format(relation_name, obj_name, source_op))\n",
    "                        has_effect = True\n",
    "                elif len_OPERATORS <= target_op_id < len_OPERATORS + len(ops):\n",
    "                    target_op = ops_reverse[target_op_id - len_OPERATORS]\n",
    "                    if source_op != target_op:\n",
    "                        if action_equal(action[4], 0, allowed_modes) or action_equal(action[4], 1, allowed_modes):\n",
    "                            # Add a specific relation:\n",
    "                            if action_equal(action[4], 0, allowed_modes):\n",
    "                                g.add_relation_manual(relation_name, source_op, target_op)\n",
    "                                if verbose >= 1:\n",
    "                                    print(\"\\t**Add a relation '{}' from '{}' to '{}'.\".format(relation_name, source_op, target_op))\n",
    "                            elif action_equal(action[4], 1, allowed_modes):\n",
    "                                g.add_relation_manual(relation_name, target_op, source_op)\n",
    "                                if verbose >= 1:\n",
    "                                    print(\"\\t**Add a relation '{}' from '{}' to '{}'.\".format(relation_name, target_op, source_op))\n",
    "                            has_effect = True\n",
    "                        elif action_equal(action[4], 2, allowed_modes) or action_equal(action[4], 3, allowed_modes):\n",
    "                            # Delete a specific relation:\n",
    "                            if action_equal(action[4], 2, allowed_modes):\n",
    "                                has_effect = g.remove_relation_manual(source_op, target_op, relation_name)\n",
    "                                if verbose >= 1:\n",
    "                                    print(\"\\t**Delele the relation '{}' from '{}' to '{}'\".format(relation_name, source_op, target_op))\n",
    "                            elif action_equal(action[4], 3, allowed_modes):\n",
    "                                has_effect = g.remove_relation_manual(target_op, source_op, relation_name)\n",
    "                                if verbose >= 1:\n",
    "                                    print(\"\\t**Delele the relation '{}' from '{}' to '{}'\".format(relation_name, target_op, source_op))\n",
    "            if has_effect:\n",
    "                graph.set_selector(g, graph_node_name)\n",
    "                if verbose >= 2:\n",
    "                    print(\"Plotting the operator graph after the action:\")\n",
    "                    graph.draw()\n",
    "                    print(\"=\" * 100 + \"\\n\")\n",
    "        \n",
    "        if verbose >= 1:\n",
    "            print()\n",
    "        return graph, has_effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Concept:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 Concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Concept(BaseGraph):\n",
    "    \"\"\"Implement the Concept class.\n",
    "\n",
    "    A concept is a node with (optional) attributes and (optional) methods.\n",
    "        Attributes and input, output of the methods can be other concepts,\n",
    "        and are modeled as nodes.\n",
    "    An input is a Graph with nodes.\n",
    "    It is inherited from MultiDiGraph for graph manipulation and torch.nn.Module for\n",
    "        gradient-based learning.\n",
    "    \"\"\"\n",
    "    def __init__(self, G=None, **kwargs):\n",
    "        \"\"\"Components of a Graph instance:\n",
    "\n",
    "        (1) A root node (name is given concept name), harboring a placeholder of the (instantiated)\n",
    "            concept instance.\n",
    "        (2) Its attributes, and arrows from the root node to the attributes. The attribute can be other Concept.\n",
    "        (3) A binary-output function indicating if a given input is an instance of the concept. The function\n",
    "            is a Graph() instance.\n",
    "        \"\"\"\n",
    "        super(Concept, self).__init__(G=G, **kwargs)\n",
    "        if \"name\" in kwargs:\n",
    "            self.add_concept_def(definition=kwargs)\n",
    "            if \"inherit_from\" in kwargs:\n",
    "                self.inherit_from = kwargs[\"inherit_from\"]\n",
    "            if \"inherit_to\" in kwargs:\n",
    "                self.inherit_to = kwargs[\"inherit_to\"]\n",
    "\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        value = self.get_node_value()\n",
    "        if value is not None:\n",
    "            return tuple(value.shape)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "    def add_attr_recur(self, definition, base_node=None, neglect_obj=False):\n",
    "        \"\"\"Recursively construct the concept graph using definition (as a dictionary) or other defined concepts.\"\"\"\n",
    "        # Add attribute nodes:\n",
    "        if not isinstance(definition, Concept):\n",
    "            # Base definition:\n",
    "            if \"attr\" in definition:\n",
    "                base_node = self.name if base_node is None else base_node\n",
    "                for key, value in definition[\"attr\"].items():\n",
    "                    if isinstance(value, tuple):\n",
    "                        value, get_value = value\n",
    "                        is_get_value = True\n",
    "                    else:\n",
    "                        is_get_value = False\n",
    "                    attr_mode = str(value.mode).split(\"-\")[0]\n",
    "                    attr_name = \"{}:{}\".format(key, attr_mode)\n",
    "                    self.add_node(attr_name, value=value, type=\"obj\" if attr_mode==DEFAULT_OBJ_TYPE or (\"is_all_obj\" in definition and definition[\"is_all_obj\"]) else \"attr\", fun=get_value if is_get_value else None)\n",
    "                    if self.name is not None:\n",
    "                        self.add_edge(base_node, attr_name, type=\"intra-attr\")\n",
    "                        # Each edges has a corresponding backward edge (with prefix \"b-\"), to facilitate traversing:\n",
    "                        self.add_edge(attr_name, base_node, type=\"b-intra-attr\")\n",
    "                        self.add_attr_recur(CONCEPTS[attr_mode], base_node=attr_name)\n",
    "        else:\n",
    "            # Recursively adding attributes of attributes:\n",
    "            base_node_key = base_node.split(\":\")[0]\n",
    "            attr_names = definition.attributes\n",
    "            for attr_name in attr_names:\n",
    "                attr_node_name = \"{}^{}\".format(base_node_key, attr_name)\n",
    "                attr_mode = attr_name.split(\":\")[-1]\n",
    "                get_value = definition.nodes(data=True)[attr_name][\"fun\"] if \"fun\" in definition.nodes(data=True)[attr_name] else None\n",
    "                self.add_node(attr_node_name, value=Placeholder(attr_mode), type=\"obj\" if attr_mode==DEFAULT_OBJ_TYPE or (\"is_all_obj\" in definition and definition[\"is_all_obj\"]) else \"attr\", fun=get_value)\n",
    "                attr_value = definition.get_node_value(attr_name)\n",
    "                if attr_value is not None:\n",
    "                    self.set_node_value(attr_value, attr_node_name)\n",
    "                self.add_edge(base_node, attr_node_name, type=\"intra-attr\")\n",
    "                self.add_edge(attr_node_name, base_node, type=\"b-intra-attr\")\n",
    "                if (not neglect_obj) or (definition.get_node_type(attr_name) != \"obj\"):\n",
    "                    self.add_attr_recur(CONCEPTS[attr_mode], base_node=attr_node_name)\n",
    "\n",
    "\n",
    "    def add_concept_def(self, definition):\n",
    "        \"\"\"Initialize the concept node from the concept definition dictionary.\"\"\"\n",
    "        name = definition[\"name\"]\n",
    "        assert \"value\" in definition\n",
    "        if name is not None:\n",
    "            self.add_node(name, value=definition[\"value\"], type=\"concept\")\n",
    "            # Add representation:\n",
    "            if \"repr\" in definition:\n",
    "                self.nodes[name][\"repr\"] = nn.Parameter(definition[\"repr\"])\n",
    "\n",
    "        # Add attribute nodes:\n",
    "        self.add_attr_recur(definition)\n",
    "\n",
    "        # Add relations:\n",
    "        if \"re\" in definition:\n",
    "            for key, relation_name in definition[\"re\"].items():\n",
    "                if len(key) == 2:\n",
    "                    source = self.get_node_name(key[0])\n",
    "                    target = self.get_node_name(key[1])\n",
    "                    self.add_edge(source, target, type=\"intra-relation\", name=relation_name)\n",
    "\n",
    "\n",
    "    def add_obj(\n",
    "        self,\n",
    "        obj,\n",
    "        obj_name=None,\n",
    "        change_root=True,\n",
    "        add_full_concept=True,\n",
    "        loc=None,\n",
    "    ):\n",
    "        \"\"\"Add object attributes relative to the root node.\"\"\"\n",
    "        assert isinstance(obj, Concept)\n",
    "\n",
    "        if obj_name is None:\n",
    "            obj_name = get_next_available_key(self.nodes, \"obj\", suffix=\":Image\", is_underscore=True)\n",
    "        self.add_node(obj_name, value=Placeholder(obj.name), type=\"obj\")\n",
    "        self.set_node_value(obj.get_node_value(), obj_name)\n",
    "        if loc is None:\n",
    "            if self.name is not None:\n",
    "                self.add_edge(self.name, obj_name, type=\"intra-attr\")\n",
    "                self.add_edge(obj_name, self.name, type=\"b-intra-attr\")\n",
    "        else:\n",
    "            loc = self.get_node_name(loc)\n",
    "            self.add_edge(loc, obj_name, type=\"intra-attr\")\n",
    "            self.add_edge(obj_name, loc, type=\"b-intra-attr\")\n",
    "        if add_full_concept:\n",
    "            self.add_attr_recur(obj, base_node=obj_name, neglect_obj=True)\n",
    "        if change_root:\n",
    "            root_tensor = self.get_node_value()\n",
    "            self.set_node_value(set_patch(root_tensor, obj.get_node_value(), obj.get_node_value(\"pos\")))\n",
    "        return obj_name\n",
    "\n",
    "\n",
    "    def add_attr(self, base_name, attr_mode):\n",
    "        \"\"\"Add an attribute with attr_mode on the node base_name.\"\"\"\n",
    "        base_name = self.get_node_name(base_name)\n",
    "        base_op_name = self.operator_name(base_name)\n",
    "        child_nodes = self.child_nodes(base_name)\n",
    "        attr_name = get_next_available_key(child_nodes, \"{}^{}\".format(base_op_name, attr_mode.lower()), suffix=\":{}\".format(attr_mode), is_underscore=False, start_from_null=True)\n",
    "        self.add_node(attr_name, value=Placeholder(attr_mode), type=\"obj\" if attr_mode==DEFAULT_OBJ_TYPE else \"attr\")\n",
    "        self.add_edge(base_name, attr_name, type=\"intra-attr\")\n",
    "        self.add_edge(attr_name, base_name, type=\"b-intra-attr\")\n",
    "        return self, attr_name\n",
    "\n",
    "\n",
    "    def remove_attr_with_value(self, attr, change_root=True):\n",
    "        \"\"\"Remove an attribute and all its descendant attributes.\"\"\"\n",
    "        attr_name_find = None\n",
    "        for attr_name in self.attributes:\n",
    "            attr_value = self.get_node_value(attr_name)\n",
    "            attr_value_given = attr.get_node_value()\n",
    "            if tuple(attr_value.shape) == tuple(attr_value_given.shape) and (attr_value == attr_value_given).all():\n",
    "                attr_name_find = attr_name\n",
    "                break\n",
    "        if attr_name_find is None:\n",
    "            return self\n",
    "        else:\n",
    "            return self.remove_attr(attr_name_find, change_root=change_root)\n",
    "\n",
    "\n",
    "    def remove_attr(self, attr_name, change_root=True):\n",
    "        \"\"\"Remove an attribute and all its descendant attributes.\"\"\"\n",
    "        descendants = self.get_descendants(attr_name, includes_self=True)\n",
    "        if change_root:\n",
    "            root_tensor = self.get_node_value()\n",
    "            for node_name in descendants:\n",
    "                if self.get_node_type(node_name) == \"obj\":\n",
    "                    set_patch(root_tensor,\n",
    "                              self.get_node_value(node_name),\n",
    "                              self.get_node_value(self.operator_name(node_name) + \"^pos\"),\n",
    "                              0)\n",
    "        self.remove_nodes_from(descendants)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def rename_nodes(self, mapping):\n",
    "        \"\"\"Rename nodes.\"\"\"\n",
    "        G = self.__class__(nx.relabel_nodes(self, mapping))\n",
    "        if hasattr(self, \"pivot_node_names\"):\n",
    "            G.pivot_node_names = self.pivot_node_names\n",
    "        if hasattr(self, \"refer_node_names\"):\n",
    "            G.refer_node_names = self.refer_node_names\n",
    "        self.__dict__.update(G.__dict__)\n",
    "        return self\n",
    "        \n",
    "\n",
    "\n",
    "    def combine_objs(self, obj_names):\n",
    "        \"\"\"Add an composite object node as parent for the obj_names.\n",
    "        The obj_names must have the same parent.\n",
    "        \"\"\"\n",
    "        obj_names = [self.get_node_name(name) for name in obj_names]\n",
    "        parent_node = self.parent_nodes(obj_names[0])[0]\n",
    "        # Check that the parent node is the same for all nodes in obj_names:\n",
    "        for obj_name in obj_names[1:]:\n",
    "            parent_node_ele = self.parent_nodes(obj_name)\n",
    "            assert len(parent_node_ele) == 1 and parent_node_ele[0] == parent_node\n",
    "        parent_node_op_name = self.operator_name(parent_node) + \"^\" if parent_node != self.name else \"\"\n",
    "        comp_obj_name = get_next_available_key(self.obj_names, \"{}obj\".format(parent_node_op_name), suffix=\":Image\", is_underscore=True)\n",
    "        comp_obj_op_name = self.operator_name(comp_obj_name)\n",
    "\n",
    "        # Get composite obj:\n",
    "        obj_dict = {obj_name: self.get_attr(obj_name) for obj_name in obj_names}\n",
    "        comp_obj, pos_bounding = get_comp_obj(obj_dict, CONCEPTS)\n",
    "        self.add_node(comp_obj_name, value=Placeholder(DEFAULT_OBJ_TYPE), type=\"obj\")\n",
    "\n",
    "        # Add attribute for the composite obj:\n",
    "        self.add_node(\"{}^pos:Pos\".format(comp_obj_op_name), value=Placeholder(\"Pos\"), type=\"attr\")\n",
    "        self.add_node(\"{}^color:Color\".format(comp_obj_op_name), value=Placeholder(\"Color\"), type=\"attr\", fun=self.get_node_fun(\"color\"))\n",
    "        self.set_node_value(comp_obj.get_node_value(), comp_obj_name)\n",
    "        self.set_node_value(pos_bounding, \"{}^pos:Pos\".format(comp_obj_op_name))\n",
    "        self.add_edge(comp_obj_name, \"{}^pos:Pos\".format(comp_obj_op_name), type=\"intra_attr\")\n",
    "        self.add_edge(comp_obj_name, \"{}^color:Color\".format(comp_obj_op_name), type=\"intra_attr\")\n",
    "        self.add_edge(\"{}^pos:Pos\".format(comp_obj_op_name), comp_obj_name, type=\"b-intra_attr\")\n",
    "        self.add_edge(\"{}^color:Color\".format(comp_obj_op_name), comp_obj_name, type=\"b-intra_attr\")\n",
    "\n",
    "        # Insert comp_obj as parent of obj_names:\n",
    "        self.add_edge(parent_node, comp_obj_name, type=\"intra-attr\")\n",
    "        self.add_edge(comp_obj_name, parent_node, type=\"b-intra-attr\")\n",
    "        for obj_name in obj_names:\n",
    "            self.remove_edge(obj_name, parent_node)\n",
    "            self.remove_edge(parent_node, obj_name)\n",
    "        for obj_name in obj_names:\n",
    "            self.add_edge(comp_obj_name, obj_name, type=\"intra-attr\")\n",
    "            self.add_edge(obj_name, comp_obj_name, type=\"b-intra-attr\")\n",
    "\n",
    "        # Rename:\n",
    "        rename_mapping = {}\n",
    "        attrs_to_rename = self.get_descendants(obj_names, includes_self=True)\n",
    "        for attr_name in attrs_to_rename:\n",
    "            attr_branch_name = attr_name if parent_node == self.name else attr_name.split(parent_node_op_name)[1]\n",
    "            rename_mapping[attr_name] = \"{}^{}\".format(comp_obj_op_name, attr_branch_name)\n",
    "        self.rename_nodes(rename_mapping)\n",
    "        return comp_obj_name\n",
    "\n",
    "\n",
    "    def flatten_obj(self, obj_name):\n",
    "        \"\"\"Remove the obj_name and connect its children directly to its parents.\"\"\"\n",
    "        obj_name = self.get_node_name(obj_name)\n",
    "        child_nodes = self.child_nodes(obj_name)\n",
    "        parent_nodes = self.parent_nodes(obj_name)\n",
    "        assert len(parent_nodes) == 1\n",
    "        parent_node = parent_nodes[0]\n",
    "        rename_mapping = {}\n",
    "        obj_op_name = self.operator_name(obj_name)\n",
    "        for child_node in child_nodes:\n",
    "            if self.get_node_type(child_node) == \"obj\":\n",
    "                descendants = self.get_descendants(child_node, includes_self=True)\n",
    "                self.remove_edge(obj_name, child_node)\n",
    "                self.remove_edge(child_node, obj_name)\n",
    "                self.add_edge(parent_node, child_node, type=\"intra-attr\")\n",
    "                self.add_edge(child_node, parent_node, type=\"b-intra-attr\")\n",
    "                for name in descendants:\n",
    "                    rename_mapping[name] = name.split(obj_op_name + \"^\")[1]\n",
    "            else:\n",
    "                self.remove_node(child_node)\n",
    "        self.remove_node(obj_name)\n",
    "        if hasattr(self, \"pivot_node_names\") and obj_name in self.pivot_node_names:\n",
    "            self.pivot_node_names.remove(obj_name)\n",
    "        if hasattr(self, \"refer_node_names\") and obj_name in self.refer_node_names:\n",
    "            self.refer_node_names.remove(obj_name)\n",
    "        self.rename_nodes(rename_mapping)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def remove_relation_manual(self, obj1_name, obj2_name, relation_name):\n",
    "        \"\"\"Manually remove a specific relation.\"\"\"\n",
    "        obj1_name = self.get_node_name(obj1_name)\n",
    "        obj2_name = self.get_node_name(obj2_name)\n",
    "        has_effect = False\n",
    "        if obj2_name in self[obj1_name]:\n",
    "            for id, edge_info in self[obj1_name][obj2_name].copy().items():\n",
    "                if edge_info[\"type\"] == 'intra-relation' and edge_info[\"name\"] == relation_name:\n",
    "                    self.remove_edge(obj1_name, obj2_name, id)\n",
    "                    has_effect = True\n",
    "        return has_effect\n",
    "\n",
    "\n",
    "    def add_relation_manual(self, relation_name, *opsc, **kwargs):\n",
    "        \"\"\"Manually add a specific relation. If it is a Concept_Pattern and its self.is_ebm is True, will\n",
    "            also add a relation-EBM.\n",
    "\n",
    "        Args:\n",
    "            relation_name: type of the relation\n",
    "            opsc: list of nodes that the relation will connect to in order.\n",
    "        \"\"\"\n",
    "        if len(opsc) == 0:\n",
    "            # Will have two new nodes:\n",
    "            obj1_name = get_next_available_key([node_name.split(\":\")[0] for node_name in self.nodes], \"obj\", is_underscore=True)\n",
    "            obj1_name = \"{}:{}\".format(obj1_name, DEFAULT_OBJ_TYPE)\n",
    "            self.add_node(obj1_name, value=Placeholder(DEFAULT_OBJ_TYPE), type=\"obj\")\n",
    "            obj2_name = get_next_available_key([node_name.split(\":\")[0] for node_name in self.nodes], \"obj\", is_underscore=True)\n",
    "            obj2_name = \"{}:{}\".format(obj2_name, DEFAULT_OBJ_TYPE)\n",
    "            self.add_node(obj2_name, value=Placeholder(DEFAULT_OBJ_TYPE), type=\"obj\")\n",
    "        elif len(opsc) == 1:\n",
    "            # Only provide the first object to connect to the first fun-in of the relation:\n",
    "            obj1_name = self.get_node_name(opsc[0])\n",
    "            obj2_name = get_next_available_key([node_name.split(\":\")[0] for node_name in self.nodes], \"obj\", is_underscore=True)\n",
    "            obj2_name = \"{}:{}\".format(obj2_name, DEFAULT_OBJ_TYPE)\n",
    "            self.add_node(obj2_name, value=Placeholder(DEFAULT_OBJ_TYPE), type=\"obj\")\n",
    "        elif len(opsc) == 2:\n",
    "            if opsc[0] is not None:\n",
    "                obj1_name = self.get_node_name(opsc[0])\n",
    "            else:\n",
    "                # the first opsc[0] is None, meaning that will initialize a new node with default concept:\n",
    "                obj1_name = get_next_available_key([node_name.split(\":\")[0] for node_name in self.nodes], \"obj\", is_underscore=True)\n",
    "                obj1_name = \"{}:{}\".format(obj1_name, DEFAULT_OBJ_TYPE)\n",
    "                self.add_node(obj1_name, value=Placeholder(DEFAULT_OBJ_TYPE), type=\"obj\")\n",
    "            obj2_name = self.get_node_name(opsc[1])\n",
    "        else:\n",
    "            raise Exception(\"The length of ops can only be 0, 1 or 2!\")\n",
    "        if hasattr(self, \"is_ebm\") and self.is_ebm:\n",
    "            if relation_name not in self.ebm_dict:\n",
    "                self.init_ebm(\n",
    "                    method=\"random\",\n",
    "                    mode=relation_name,\n",
    "                    ebm_mode=\"operator\",\n",
    "                    ebm_model_type=\"CEBM\",\n",
    "                    **kwargs\n",
    "                )\n",
    "            placeholder = Placeholder(relation_name).set_ebm_key(relation_name)\n",
    "            self.add_edge(obj1_name, obj2_name, type=\"intra-relation\", name=relation_name, value=placeholder)\n",
    "        else:\n",
    "            self.add_edge(obj1_name, obj2_name, type=\"intra-relation\", name=relation_name)\n",
    "        # Important: reset the forward results cache\n",
    "        if hasattr(self, \"cache_forward\") and self.cache_forward:\n",
    "            self.forward_cache = {}\n",
    "        return self\n",
    "\n",
    "\n",
    "    def add_relation(self, obj1_name, obj2_name, OPERATORS, allowed_types=[\"Bool\"]):\n",
    "        \"\"\"Find all the relations between obj1 and obj2 in both directions.\"\"\"\n",
    "        obj1_name = self.get_node_name(obj1_name)\n",
    "        obj2_name = self.get_node_name(obj2_name)\n",
    "\n",
    "        for key, op in OPERATORS.items():\n",
    "            output_mode = op.get_to_outnode(op.name).split(\":\")[-1]\n",
    "            if \"Bool\" in allowed_types and output_mode == \"Bool\":\n",
    "                if \"obj1\" not in locals():\n",
    "                    obj1 = self.get_attr(obj1_name)\n",
    "                if \"obj2\" not in locals():\n",
    "                    obj2 = self.get_attr(obj2_name)\n",
    "                relations_exist = self.get_relation(obj1_name, obj2_name)\n",
    "\n",
    "                if ((obj1_name, obj2_name) not in relations_exist or key not in relations_exist[(obj1_name, obj2_name)]) and check_input_valid(op, obj1_name, obj2_name):\n",
    "                    is_valid = op(obj1, obj2)\n",
    "                    if is_valid:\n",
    "                        self.add_edge(obj1_name, obj2_name, type=\"intra-relation\", name=key)\n",
    "                if ((obj2_name, obj1_name) not in relations_exist or key not in relations_exist[(obj2_name, obj1_name)]) and check_input_valid(op, obj1_name, obj2_name):\n",
    "                    is_valid = op(obj2, obj1)\n",
    "                    if is_valid:\n",
    "                        self.add_edge(obj2_name, obj1_name, type=\"intra-relation\", name=key)\n",
    "            elif \"Op\" in allowed_types and output_mode != \"Bool\" and len(op.dangling_nodes) == 0 and len(op.input_placeholder_nodes) == 1:\n",
    "                relations_exist = self.get_relation(obj1_name, obj2_name)\n",
    "                if \"obj1\" not in locals():\n",
    "                    obj1 = self.get_attr(obj1_name)\n",
    "                if \"obj2\" not in locals():\n",
    "                    obj2 = self.get_attr(obj2_name)\n",
    "                if ((obj1_name, obj2_name) not in relations_exist or key not in relations_exist[(obj1_name, obj2_name)]) and check_input_valid(op, obj1_name):\n",
    "                    obj1_trans = op(obj1)\n",
    "                    is_valid = obj1_trans == obj2\n",
    "                    if is_valid:\n",
    "                        self.add_edge(obj1_name, obj2_name, type=\"intra-relation\", name=key)\n",
    "                if ((obj2_name, obj1_name) not in relations_exist or key not in relations_exist[(obj2_name, obj1_name)]) and check_input_valid(op, obj2_name):\n",
    "                    obj2_trans = op(obj2)\n",
    "                    is_valid = obj2_trans == obj1\n",
    "                    if is_valid:\n",
    "                        self.add_edge(obj2_name, obj1_name, type=\"intra-relation\", name=key)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def add_relations(self, OPERATORS, allowed_types=[\"Bool\"]):\n",
    "        \"\"\"Get valid relations between all pairs of objects.\"\"\"\n",
    "        node_lst = self.attributes\n",
    "        is_obj = False if len(self.obj_names) == 0 else True\n",
    "        if is_obj:\n",
    "            node_lst = self.obj_names\n",
    "        for i, obj1_name in enumerate(node_lst):\n",
    "            for j, obj2_name in enumerate(node_lst):\n",
    "                if i < j:\n",
    "                    self.add_relation(obj1_name, obj2_name, OPERATORS, \n",
    "                                      allowed_types=allowed_types)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def get_relation(self, obj1_name, obj2_name, bidirectional=True):\n",
    "        \"\"\"Get relation between two objects.\"\"\"\n",
    "        obj1_name = self.get_node_name(obj1_name)\n",
    "        obj2_name = self.get_node_name(obj2_name)\n",
    "        relations = {}\n",
    "        if obj2_name in self[obj1_name]:\n",
    "            for key, item in self[obj1_name][obj2_name].items():\n",
    "                if item[\"type\"] == \"intra-relation\":\n",
    "                    record_data(relations, [item[\"name\"]], [(obj1_name, obj2_name)])\n",
    "        if not bidirectional:\n",
    "            if (obj1_name, obj2_name) in relations:\n",
    "                return relations[(obj1_name, obj2_name)]\n",
    "            else:\n",
    "                return []\n",
    "        else:\n",
    "            if obj1_name in self[obj2_name]:\n",
    "                for key, item in self[obj2_name][obj1_name].items():\n",
    "                    if item[\"type\"] == \"intra-relation\":\n",
    "                        record_data(relations, [item[\"name\"]], [(obj2_name, obj1_name)])\n",
    "            return relations\n",
    "\n",
    "\n",
    "    def get_relations(self):\n",
    "        \"\"\"Get relation between each pair of objects.\"\"\"\n",
    "        node_lst = self.attributes\n",
    "        is_obj = False if len(self.obj_names) == 0 else True\n",
    "        if is_obj:\n",
    "            node_lst = self.obj_names\n",
    "        relations = {}\n",
    "        for i, obj1_name in enumerate(node_lst):\n",
    "            for j, obj2_name in enumerate(node_lst):\n",
    "                if i < j:\n",
    "                    relation = self.get_relation(obj1_name, obj2_name)\n",
    "                    if len(relation) > 0:\n",
    "                        relations.update(relation)\n",
    "        return relations\n",
    "\n",
    "\n",
    "    def get_neighbors(self, op_name):\n",
    "        \"\"\"Obtain the neighbor of an op_name.\n",
    "\n",
    "        Args:\n",
    "            op_name: if op_name is an op-sc, then its neighbors are op-so\n",
    "                     if op_name is an op-so, then its neighbors are op-sc.\n",
    "        \"\"\"\n",
    "        if \")\" not in op_name:\n",
    "            # op_name refers to a concept node, e.g. obj_1:Image:\n",
    "            neighbors = []\n",
    "            node1_name = self.get_node_name(op_name)\n",
    "            for node2_name, edge_info in self[node1_name].items():\n",
    "                for key, item in edge_info.items():\n",
    "                    assert item[\"type\"] == \"intra-relation\"\n",
    "                    neighbors.append(f\"({node1_name},{node2_name}):{item['name']}\")\n",
    "        else:\n",
    "            # op_name refers to a relation edge, e.g. '(obj_1:c1,obj_2:c1):r1':\n",
    "            neighbors = op_name.split(\")\")[0][1:].split(\",\")\n",
    "        return neighbors\n",
    "\n",
    "\n",
    "    def parse_comp_obj(self, comp_obj):\n",
    "        \"\"\"Given an comp_obj, find all the obj_names in self.objs that together forms the comp_obj\"\"\"\n",
    "        comp_obj_value = comp_obj.get_node_value()\n",
    "        comp_obj_pos = comp_obj.get_node_value(\"pos\")\n",
    "        unexplained_comp = comp_obj_value > 0\n",
    "        obj_names_included = []\n",
    "        for obj_name in self.obj_names:\n",
    "            obj_value = self.get_node_value(obj_name)\n",
    "            obj_pos = self.get_node_value(self.operator_name(obj_name) + \"^pos\")\n",
    "            if obj_pos[0] >= comp_obj_pos[0] and obj_pos[1] >= comp_obj_pos[1] and obj_pos[0] + obj_pos[2] <= comp_obj_pos[0] + comp_obj_pos[2] and obj_pos[1] + obj_pos[3] <= comp_obj_pos[1] + comp_obj_pos[3]:\n",
    "                # Object position is inside, then compare value:\n",
    "                relpos = (int(obj_pos[0] - comp_obj_pos[0]), int(obj_pos[1] - comp_obj_pos[1]), int(obj_pos[2]), int(obj_pos[3]))\n",
    "                patch = get_patch(comp_obj_value, relpos)  # The patch corresponding to the position of the obj\n",
    "                mask_obj_g0 = obj_value > 0\n",
    "                is_subset = (obj_value[mask_obj_g0] == patch[mask_obj_g0]).all()\n",
    "                if is_subset:\n",
    "                    obj_names_included.append(obj_name)\n",
    "                    unexplained_comp[relpos[0]: relpos[0] + relpos[2], relpos[1]: relpos[1] + relpos[3]][mask_obj_g0] = 0\n",
    "            else:\n",
    "                intersect_pos = get_pos_intersection(obj_pos, comp_obj_pos)\n",
    "                if intersect_pos is not None:\n",
    "                    relpos = (int(intersect_pos[0] - obj_pos[0]),\n",
    "                              int(intersect_pos[1] - obj_pos[1]),\n",
    "                              int(intersect_pos[2]),\n",
    "                              int(intersect_pos[3]))\n",
    "                    comp_relpos = (int(intersect_pos[0] - comp_obj_pos[0]),\n",
    "                                   int(intersect_pos[1] - comp_obj_pos[1]),\n",
    "                                   int(intersect_pos[2]),\n",
    "                                   int(intersect_pos[3]))\n",
    "                    intersect_patch = get_patch(obj_value, relpos)\n",
    "                    intersect_comp_patch = get_patch(comp_obj_value, comp_relpos)\n",
    "                    is_subset = (intersect_comp_patch == intersect_patch).all()\n",
    "                    if is_subset:\n",
    "                        # Composite object and obj has overlap. Then split the object into intersect_obj and remainder object, \n",
    "                        # and add them as descendants to the obj:\n",
    "                        remainder_obj_value = deepcopy(obj_value)\n",
    "                        set_patch(remainder_obj_value, intersect_patch, relpos, 0)\n",
    "                        remainder_obj_value, remainder_pos = shrink(remainder_obj_value)\n",
    "                        remainder_obj = CONCEPTS[self.name].copy().set_node_value(remainder_obj_value)\n",
    "                        remainder_obj.set_node_value([obj_pos[0] + remainder_pos[0],\n",
    "                                                      obj_pos[1] + remainder_pos[1],\n",
    "                                                      remainder_pos[2],\n",
    "                                                      remainder_pos[3]],\n",
    "                                                     \"pos\")\n",
    "                        intersect_patch, intersect_shrink_pos = shrink(intersect_patch)\n",
    "                        intersect_obj = CONCEPTS[self.name].copy().set_node_value(intersect_patch)\n",
    "                        intersect_obj.set_node_value([intersect_pos[0] + intersect_shrink_pos[0],\n",
    "                                                      intersect_pos[1] + intersect_shrink_pos[1],\n",
    "                                                      intersect_shrink_pos[2],\n",
    "                                                      intersect_shrink_pos[3]], \n",
    "                                                     \"pos\")\n",
    "                        self.remove_attr(obj_name, change_root=False)\n",
    "                        intersect_obj_name = self.add_obj(intersect_obj, change_root=False)\n",
    "                        remainder_obj_name = self.add_obj(remainder_obj, change_root=False)\n",
    "                        obj_names_included.append(intersect_obj_name)\n",
    "                        unexplained_comp[comp_relpos[0]: comp_relpos[0] + comp_relpos[2], comp_relpos[1]: comp_relpos[1] + comp_relpos[3]][intersect_comp_patch > 0] = 0\n",
    "            if unexplained_comp.sum() == 0:\n",
    "                break\n",
    "        return obj_names_included, unexplained_comp\n",
    "\n",
    "\n",
    "    def get_concept_pattern_from_objs(self, comp_objs, is_self_contained=True):\n",
    "        \"\"\"Given a list of comp_objs, return a concept_pattern that together forms the comp_obj\"\"\"\n",
    "        if not isinstance(comp_objs, list):\n",
    "            comp_objs = [comp_objs]\n",
    "        obj_names_all = []\n",
    "        for comp_obj in comp_objs:\n",
    "            obj_names, unexplained_comp = self.parse_comp_obj(comp_obj)\n",
    "            assert unexplained_comp.sum() == 0\n",
    "            obj_names_all += obj_names\n",
    "        if is_self_contained:\n",
    "            # The nodes in concept_pattern only come from the objects constituting the comp_objs:\n",
    "            concept_pattern = self.get_concept_pattern(node_names=obj_names_all)\n",
    "        else:\n",
    "            # Includes all the objects in the graph, and the obj_names_all serves as refer_nodes:\n",
    "            concept_pattern = self.get_concept_pattern(refer_node_names=obj_names_all)\n",
    "        return concept_pattern\n",
    "\n",
    "\n",
    "    def get_concept_pattern(\n",
    "        self,\n",
    "        node_names=None,\n",
    "        pivot_node_names=None,\n",
    "        refer_node_names=None,\n",
    "    ):\n",
    "        \"\"\"Obtain Concept_Pattern instance.\"\"\"\n",
    "        G = deepcopy(self)\n",
    "        node_names = [G.get_node_name(node_name) for node_name in node_names] if node_names is not None else G.nodes\n",
    "        if pivot_node_names is None:\n",
    "            if hasattr(self, \"pivot_node_names\"):\n",
    "                pivot_node_names = self.pivot_node_names\n",
    "        if refer_node_names is None:\n",
    "            if hasattr(self, \"refer_node_names\"):\n",
    "                refer_node_names = self.refer_node_names\n",
    "\n",
    "        concept_pattern = Concept_Pattern(G.subgraph(node_names),\n",
    "                                          pivot_node_names=pivot_node_names,\n",
    "                                          refer_node_names=refer_node_names,\n",
    "                                          parent_root_name=self.name,\n",
    "                                         )\n",
    "        return concept_pattern\n",
    "\n",
    "\n",
    "    def get_matching_mapping(self, subconcept):\n",
    "        \"\"\"Given a subconcept (also an instance of Concept class), find the mapping of subgraph matching\n",
    "        (that also obey the name of the relations), and return a list of mappings,\n",
    "        each of which maps a node in subconcept to a node in self.\n",
    "        \"\"\"\n",
    "        # convert to line graph to get edge-base isomorphism subgraphs\n",
    "        DiGM = isomorphism.DiGraphMatcher(line_graph(DiGraph(self)), line_graph(DiGraph(subconcept)))\n",
    "        p_relations = subconcept.get_relations()\n",
    "        valid_mappings = []\n",
    "        # No edges and only one node:\n",
    "        if len(subconcept.edges) == 0:\n",
    "            assert(len(subconcept.nodes) == 1 or len(subconcept.nodes) == 2)\n",
    "            if len(subconcept.nodes) == 1:\n",
    "                p_node = list(subconcept.nodes)[0]\n",
    "                for node in self.nodes:\n",
    "                    if node.split(\":\")[-1] == p_node.split(\":\")[-1] and node != self.name:\n",
    "                        valid_mappings.append({p_node: node})\n",
    "                return valid_mappings\n",
    "            else:\n",
    "                p_node1, p_node2 = list(subconcept.nodes)[0], list(subconcept.nodes)[1]\n",
    "                for i in range(len(self.nodes)):\n",
    "                    for j in range(i+1, len(self.nodes)):\n",
    "                        node1 = list(self.nodes)[i]\n",
    "                        node2 = list(self.nodes)[j]\n",
    "                        if node1.split(\":\")[-1] == p_node1.split(\":\")[-1] and node1 != self.name \\\n",
    "                         and node2.split(\":\")[-1] == p_node2.split(\":\")[-1] and node2 != self.name:\n",
    "                            valid_mappings.append({p_node1: node1, p_node2: node2})\n",
    "                        elif node2.split(\":\")[-1] == p_node1.split(\":\")[-1] and node2 != self.name \\\n",
    "                         and node1.split(\":\")[-1] == p_node2.split(\":\")[-1] and node1 != self.name:\n",
    "                            valid_mappings.append({p_node1: node2, p_node2: node1})        \n",
    "                return valid_mappings\n",
    "        \n",
    "        # With edges:\n",
    "        for edge_match in DiGM.subgraph_isomorphisms_iter():\n",
    "            # convert edge maps to node maps\n",
    "            mapping = {}\n",
    "            is_valid = True\n",
    "            for c_pair, p_pair in edge_match.items():\n",
    "                if p_pair[0] in mapping:\n",
    "                    if mapping[p_pair[0]] != c_pair[0] or c_pair[0].split(\":\")[-1] != p_pair[0].split(\":\")[-1]:\n",
    "                        # Check if nodes are consistent:\n",
    "                        is_valid = False\n",
    "                        break\n",
    "                elif c_pair[0] == self.name:\n",
    "                    is_valid = False\n",
    "                    break\n",
    "                else:\n",
    "                    mapping[p_pair[0]] = c_pair[0]\n",
    "                if p_pair[1] in mapping:\n",
    "                    if mapping[p_pair[1]] != c_pair[1] or c_pair[1].split(\":\")[-1] != p_pair[1].split(\":\")[-1]:\n",
    "                        is_valid = False\n",
    "                        break\n",
    "                elif c_pair[1] == self.name:\n",
    "                    is_valid = False\n",
    "                    break\n",
    "                else:\n",
    "                    mapping[p_pair[1]] = c_pair[1]\n",
    "\n",
    "                # Check if it is the same edge_type:\n",
    "                p_edge_type = subconcept.get_edge_type(*p_pair)\n",
    "                if p_edge_type != self.get_edge_type(*c_pair):\n",
    "                    is_valid = False\n",
    "                    break\n",
    "\n",
    "                if p_edge_type == \"intra-relation\":\n",
    "                    # Check if relation is a subset:\n",
    "                    p_relation = p_relations[p_pair]\n",
    "                    c_relation = self.get_relation(*c_pair, bidirectional=False)\n",
    "                    if not set(p_relation).issubset(c_relation):\n",
    "                        is_valid = False\n",
    "                        break\n",
    "\n",
    "            if is_valid:\n",
    "                valid_mappings.append(mapping)\n",
    "        return valid_mappings\n",
    "\n",
    "\n",
    "    def query_node_names(self, subconcept, query_node_names, is_match_node=False):\n",
    "        \"\"\"Return the corresponding node names in self as the query_node_names in subconcept.\"\"\"\n",
    "        def get_is_match_node(mapping, refer_node_names):\n",
    "            is_matched = True\n",
    "            for p_node, c_node in mapping.items():\n",
    "                if p_node not in refer_node_names:\n",
    "                    if p_node.split(\":\")[-1] != c_node.split(\":\")[-1]:\n",
    "                        is_matched = False\n",
    "                        break\n",
    "            return is_matched\n",
    "        if not isinstance(query_node_names, list):\n",
    "            query_node_names = [query_node_names]\n",
    "        mappings = self.get_matching_mapping(subconcept)\n",
    "        concept_node_names_list = []\n",
    "        for mapping in mappings:\n",
    "            # Make sure that each pivot node is matched:\n",
    "            is_matched = True\n",
    "            if is_match_node:\n",
    "                is_matched = get_is_match_node(mapping, subconcept.refer_node_names)\n",
    "                if not is_matched:\n",
    "                    continue\n",
    "            if hasattr(subconcept, \"pivot_node_names\") and subconcept.pivot_node_names is not None:\n",
    "                for node_name in subconcept.pivot_node_names:\n",
    "                    value_subconcept = subconcept.get_node_value(node_name)\n",
    "                    node_name_concept = mapping[subconcept.get_node_name(node_name)]\n",
    "                    value_concept = self.get_node_value(node_name_concept)\n",
    "                    if value_subconcept.shape != value_concept.shape or not (value_subconcept == value_concept).all():\n",
    "                        is_matched = False\n",
    "                        break\n",
    "            if not is_matched:\n",
    "                continue\n",
    "            # Obtain the concept_node_names:\n",
    "            concept_node_names_list.append([mapping[subconcept.get_node_name(node_name)] for node_name in query_node_names])\n",
    "        is_same_set = check_same_set(concept_node_names_list)\n",
    "        if is_same_set is None:\n",
    "            return is_same_set, []\n",
    "        elif is_same_set is True:\n",
    "            return is_same_set, concept_node_names_list[0]\n",
    "        else:\n",
    "            union_set = set(concept_node_names_list[0])\n",
    "            for node_names in concept_node_names_list[1:]:\n",
    "                union_set = union_set.union(set(node_names))\n",
    "            return is_same_set, list(union_set)\n",
    "\n",
    "\n",
    "    def get_pivot_node_names(self, concept_pattern, is_match_node=False):\n",
    "        \"\"\"Get pivot_node_names in self as specified by the pivot_node_names in concept_pattern\"\"\"\n",
    "        if concept_pattern.pivot_node_names is not None:\n",
    "            is_same_set, pivot_node_names = self.query_node_names(concept_pattern, concept_pattern.pivot_node_names, is_match_node=is_match_node)\n",
    "            if not is_same_set:\n",
    "                raise Exception(\"Different mappings return different sets of pivot nodes!\")\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "\n",
    "    def get_pivot_nodes(self, concept_pattern, is_match_node=False):\n",
    "        \"\"\"Get pivot_nodes (in terms of concept instance) in self as specified by the pivot_node_names in concept_pattern\"\"\"\n",
    "        pivot_node_names = self.get_pivot_node_names(concept_pattern, is_match_node=is_match_node)\n",
    "        return {node_name: self.get_attr(node_name) for node_name in pivot_node_names}\n",
    "\n",
    "\n",
    "    def get_refer_node_names(self, concept_pattern, is_match_node=False):\n",
    "        \"\"\"Get refer_node_names in self as specified by the refer_node_names in concept_pattern\"\"\"\n",
    "        if concept_pattern.refer_node_names is None:\n",
    "            refer_node_names = list(concept_pattern.nodes)\n",
    "        else:\n",
    "            refer_node_names = concept_pattern.refer_node_names\n",
    "        is_same_set, refer_node_names = self.query_node_names(concept_pattern, refer_node_names, is_match_node=is_match_node)\n",
    "        return refer_node_names\n",
    "\n",
    "\n",
    "    def get_refer_nodes(self, concept_pattern, is_match_node=False):\n",
    "        \"\"\"Get refer_nodes (in terms of concept instance) in self as specified by the refer_node_names in concept_pattern\"\"\"\n",
    "        refer_node_names = self.get_refer_node_names(concept_pattern, is_match_node=is_match_node)\n",
    "        return {node_name: self.get_attr(node_name) for node_name in refer_node_names}\n",
    "\n",
    "\n",
    "    def find_node(self, node_name):\n",
    "        return any([node for node in self.nodes(data=True) if node == node_name])\n",
    "\n",
    "\n",
    "    def get_refer_subconcept(self, concept_pattern):\n",
    "        \"\"\"Given a concept_pattern (also an instance of Concept class), find the mapping of subgraph matching\n",
    "        (that also obey the name of the relations), and return a single concept\n",
    "        that matches the concept_pattern (both nodes and relations)\n",
    "        \"\"\"\n",
    "        p_relations = concept_pattern.get_relations()\n",
    "        refer_nodes = concept_pattern.refer_node_names\n",
    "        if refer_nodes is None:\n",
    "            refer_nodes = list(concept_pattern.nodes)\n",
    "        # Get all valid node mappings as a list of subgraphs\n",
    "        node_mappings = self.get_matching_mapping(concept_pattern)\n",
    "        subconcept = Concept(name=self.name, **self.root_node)\n",
    "        for subgraph_map in node_mappings:\n",
    "            # Pairs of pattern node, concept node\n",
    "            curr_mapping = {}\n",
    "            for p_node, c_node in subgraph_map.items():\n",
    "                # Only keep the concept nodes that are also refer nodes\n",
    "                if p_node in refer_nodes:\n",
    "                    curr_mapping[p_node] = c_node\n",
    "            # Go through all pairs of concepts in this subgraph and add edges\n",
    "            for ind1, p_node1 in enumerate(curr_mapping.keys()):\n",
    "                for ind2, p_node2 in enumerate(curr_mapping.keys()):\n",
    "                    if ind1 < ind2:\n",
    "                        c_node1 = curr_mapping[p_node1]\n",
    "                        c_node2 = curr_mapping[p_node2]\n",
    "                        # Add c_node1 and c_node2 if not already in subconcept\n",
    "                        if c_node1 not in subconcept.nodes:\n",
    "                            subconcept.add_node(c_node1, value = self.get_node_content(c_node1),\n",
    "                                               type = self.get_node_type(c_node1),\n",
    "                                               repr = self.get_node_repr(c_node1))\n",
    "                            subconcept.add_edge(self.name, c_node1, type=\"intra-attr\")\n",
    "                            subconcept.add_edge(c_node1, self.name, type=\"b-intra-attr\")\n",
    "                        if c_node2 not in subconcept.nodes:\n",
    "                            subconcept.add_node(c_node2, value = self.get_node_content(c_node2),\n",
    "                                               type = self.get_node_type(c_node2),\n",
    "                                               repr = self.get_node_repr(c_node2))\n",
    "                            subconcept.add_edge(self.name, c_node2, type=\"intra-attr\")\n",
    "                            subconcept.add_edge(c_node2, self.name, type=\"b-intra-attr\")\n",
    "                        p_relation = p_relations[(p_node1, p_node2)]\n",
    "                        for relation in p_relation:\n",
    "                            subconcept.add_relation_manual(relation, c_node1, c_node2)\n",
    "        print(subconcept.get_relations())\n",
    "        return subconcept\n",
    "\n",
    "\n",
    "    @property\n",
    "    def root_node(self):\n",
    "        \"\"\"Get the content of the root node.\"\"\"\n",
    "        return self.nodes(data=True)[self.name]\n",
    "\n",
    "\n",
    "    @property\n",
    "    def obj_names(self):\n",
    "        \"\"\"Get object names.\"\"\"\n",
    "        nodes_sorted = self.topological_sort\n",
    "        return deepcopy([node for node in nodes_sorted if self.nodes[node][\"type\"] == \"obj\" and node != self.name])\n",
    "\n",
    "\n",
    "    @property\n",
    "    def obj_names_aug(self):\n",
    "        \"\"\"Get object names, and if there is only root object, returns the root object.\"\"\"\n",
    "        obj_names = self.obj_names\n",
    "        if len(obj_names) == 0:\n",
    "            obj_names = [\"$root\"]\n",
    "        return obj_names\n",
    "\n",
    "\n",
    "    @property\n",
    "    def objs(self):\n",
    "        \"\"\"Get a dictionary of {obj_name: obj}.\"\"\"\n",
    "        objs = OrderedDict()\n",
    "        for obj_name in self.obj_names:\n",
    "            objs[obj_name] = self.get_attr(obj_name)\n",
    "        return objs\n",
    "\n",
    "\n",
    "    def draw_objs(self):\n",
    "        \"\"\"Draw objects.\"\"\"\n",
    "        for obj_name, obj in self.objs.items():\n",
    "            print(\"{}:\".format(obj_name))\n",
    "            obj.draw()\n",
    "\n",
    "\n",
    "    @property\n",
    "    def attributes(self):\n",
    "        \"\"\"Get all the attribute names of the concept.\"\"\"\n",
    "        if self.name is not None:\n",
    "            return deepcopy(self.child_nodes(self.name))\n",
    "        else:\n",
    "            return list(self.nodes)\n",
    "\n",
    "\n",
    "    def get_reprs(self, allowed_attr=\"all\"):\n",
    "        \"\"\"Return the reprentations for the attributes in N x REPR_DIM, where N is the number of\n",
    "        nodes in the graph, and each row corresponds to the global representation of the concept.\n",
    "        \"\"\"\n",
    "        x = []\n",
    "        for node in self.get_graph(allowed_attr).topological_sort:\n",
    "            mode = node.split(\":\")[-1]\n",
    "            x.append(CONCEPTS[mode].get_node_repr())\n",
    "        x = torch.stack(x, 0)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def get_edge_index_attr_de(self, OPERATORS, allowed_attr=\"all\", repr_format=\"onehot\"):\n",
    "        \"\"\"Return edge_index for descendant relations in COO format, where the nodes' index\n",
    "            is according to self.topological_sort.\"\"\"\n",
    "        edge_index = []\n",
    "        node_sorted = self.get_graph(allowed_attr).topological_sort\n",
    "        for i, node in enumerate(node_sorted):\n",
    "            for child_node in self.child_nodes(node):\n",
    "                if child_node in node_sorted:\n",
    "                    j = node_sorted.index(child_node)\n",
    "                    edge_index.append([i, j])\n",
    "        if len(edge_index) > 0:\n",
    "            edge_index = to_Variable(edge_index).long().T.to(self.device)\n",
    "        else:\n",
    "            edge_index = torch.zeros(2, 0).long().to(self.device)\n",
    "        if repr_format == \"onehot\":\n",
    "            edge_attr = torch.zeros(edge_index.shape[-1], len(OPERATORS) + 4).to(self.device)\n",
    "        elif repr_format == \"embedding\":\n",
    "            edge_attr = torch.zeros(edge_index.shape[-1], REPR_DIM).to(self.device)\n",
    "        else:\n",
    "            raise Exception(\"repr_format {} is not valid!\".format(repr_format))\n",
    "        return edge_index, edge_attr\n",
    "\n",
    "\n",
    "    def get_edge_index_attr_re(self, OPERATORS, allowed_attr=\"all\", repr_format=\"onehot\"):\n",
    "        \"\"\"Return edge_index for intra-attribute relations in COO format, where the nodes' index \n",
    "            is according to self.topological_sort.\"\"\"\n",
    "        edge_index = []\n",
    "        edge_attr = []\n",
    "        node_sorted = self.get_graph(allowed_attr).topological_sort\n",
    "        relations_dict = self.get_relations()\n",
    "        for (source, target), relations in relations_dict.items():\n",
    "            source_id = node_sorted.index(source)\n",
    "            target_id = node_sorted.index(target)\n",
    "            for relation in relations:\n",
    "                edge_index.append([source_id, target_id])\n",
    "                # Relation 0 is preserved for descendant relations\n",
    "                if repr_format == \"onehot\":\n",
    "                    edge_attr_tensor = torch.zeros(len(OPERATORS) + 4)\n",
    "                    op_id = list(OPERATORS.keys()).index(relation)\n",
    "                    edge_attr_tensor[op_id] = 1\n",
    "                elif repr_format == \"embedding\":\n",
    "                    edge_attr_tensor = OPERATORS[relation].get_node_repr()\n",
    "                else:\n",
    "                    raise Exception(\"repr_format {} is not valid!\".format(repr_format))\n",
    "                edge_attr.append(edge_attr_tensor)\n",
    "        if len(edge_index) > 0:\n",
    "            edge_index = to_Variable(edge_index).long().T.to(self.device)\n",
    "            edge_attr = torch.stack(edge_attr).to(self.device)\n",
    "        else:\n",
    "            edge_index = torch.zeros(2, 0).long().to(self.device)\n",
    "            edge_attr = torch.zeros(0, len(OPERATORS) + 4 if repr_format == \"onehot\" else REPR_DIM).float().to(self.device)\n",
    "        return edge_index, edge_attr\n",
    "\n",
    "\n",
    "    def get_PyG_data(self, OPERATORS=None, allowed_attr=\"all\", repr_format=\"onehot\"):\n",
    "        \"\"\"Get the graph data in PyG format.\n",
    "        Args:\n",
    "            OPERATORS: if not None, the edge_index and edge_attr will include the relational edge data.\n",
    "            allowed_attr: choose from \"all\" (allowing all attribute nodes) and \"obj\" (only allowing object nodes).\n",
    "\n",
    "        Attributes: x: global reprenstation for each operator, where each row is for one operator \n",
    "                                sorted by self.operators_core.\n",
    "                    edge_index: edge_index for operators in COO format, where the operators' index \n",
    "                                is according to self.operators_core\n",
    "                    edge_attr: edge_attr with shape [N, 1]. 0 means descendant relation. Other integers indicate\n",
    "                                the index of relation in OPERATORS.\n",
    "        \"\"\"\n",
    "        from torch_geometric.data import Data\n",
    "        edge_index_de, edge_attr_de = self.get_edge_index_attr_de(OPERATORS, allowed_attr=allowed_attr, repr_format=repr_format)\n",
    "        edge_index_re, edge_attr_re = self.get_edge_index_attr_re(OPERATORS, allowed_attr=allowed_attr, repr_format=repr_format)\n",
    "        edge_index = torch.cat([edge_index_de, edge_index_re], -1)\n",
    "        edge_attr = torch.cat([edge_attr_de, edge_attr_re])\n",
    "        data = Data(x=self.get_reprs(allowed_attr), edge_index=edge_index, edge_attr=edge_attr)\n",
    "        return data\n",
    "\n",
    "\n",
    "    def get_root_value(self):\n",
    "        \"\"\"Get the value of the root node.\"\"\"\n",
    "        return self.get_node_value(self.name)\n",
    "\n",
    "\n",
    "    def get_subgraph(self, nodes, includes_root=False, includes_descendants=True):\n",
    "        \"\"\"Get a subgraph using nodes and their descendants.\"\"\"\n",
    "        if includes_descendants:\n",
    "            nodes_to_preserve = []\n",
    "            for node in nodes:\n",
    "                nodes_to_preserve += self.get_descendants(node, includes_self=True)\n",
    "        else:\n",
    "            nodes_to_preserve = nodes\n",
    "        if includes_root:\n",
    "            nodes_to_preserve.append(self.name)\n",
    "        nodes_to_preserve = list(set(nodes_to_preserve))\n",
    "        G = Concept(self.subgraph(nodes_to_preserve))\n",
    "        G.name = self.name if includes_root else None\n",
    "        return G\n",
    "\n",
    "\n",
    "    def get_attr(self, attr_name):\n",
    "        \"\"\"Get the full attribute (preserving concept form and all its descendants).\"\"\"\n",
    "        if attr_name == \"$root\":\n",
    "            return self\n",
    "        G = self.get_subgraph(self.get_descendants(attr_name, includes_self=True), includes_root=False)\n",
    "\n",
    "        # Rename root node:\n",
    "        attr_prefix = attr_name.split(\":\")[0]\n",
    "        length = len(attr_prefix)\n",
    "        mapping = {}\n",
    "        for node_name in G.nodes:\n",
    "            mapping[node_name] = node_name[length + 1:]\n",
    "\n",
    "        G = nx.relabel_nodes(G, mapping)\n",
    "        G.name = attr_name.split(\":\")[-1]\n",
    "        return G\n",
    "\n",
    "\n",
    "    def get_attr_value(self, attr_name):\n",
    "        \"\"\"Get the value of an attribute node.\"\"\"\n",
    "        return self.get_node_value(attr_name)\n",
    "\n",
    "\n",
    "    def set_node_value(self, value, node_name=None):\n",
    "        \"\"\"Set up the value held in the Placeholder of a node, if the content is a Placeholder.\n",
    "        Then compute all the attributes belonging to its descendants\n",
    "        \"\"\"\n",
    "        assert isinstance(self.get_node_content(node_name), Placeholder)\n",
    "        node_name = self.get_node_name(node_name)\n",
    "        if not isinstance(value, torch.Tensor):\n",
    "            self.nodes(data=True)[node_name][\"value\"].value = to_Variable(value, is_cuda=self.is_cuda)\n",
    "        else:\n",
    "            self.nodes(data=True)[node_name][\"value\"].value = value\n",
    "        self.compute_attr_value(node_name=node_name)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def compute_attr_value(self, node_name=None):\n",
    "        \"\"\"Calculate the attribute values of the descendants of node_name\"\"\"\n",
    "        node_name = self.get_node_name(node_name)\n",
    "        node_value = self.get_node_value(node_name)\n",
    "        if node_value is not None:\n",
    "            for child_node in self.child_nodes(node_name):\n",
    "                fun = self.get_node_fun(child_node)\n",
    "                if fun is not None:\n",
    "                    if self.get_node_value(child_node) is None:\n",
    "                        self.set_node_value(fun(node_value), child_node)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def draw(self, is_clean_graph=True, layout=\"spring\", filename=None, **kwargs):\n",
    "        \"\"\"Visualize the current graph.\"\"\"\n",
    "        # Only plot the acyclic edges, and by default the forward graph:\n",
    "        import logging\n",
    "        logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "        if is_clean_graph:\n",
    "            G = self.clean_graph(is_copy_module=False)\n",
    "        else:\n",
    "            G = self.forward_graph(is_copy_module=False) if not nx.is_directed_acyclic_graph(self) else self\n",
    "        # Nodes:\n",
    "        node_color = []\n",
    "        node_sizes = []\n",
    "        node_list = []\n",
    "        attr_node_list = []\n",
    "\n",
    "        for node, info in G.nodes(data=True):\n",
    "            node_size = 1200\n",
    "            if info[\"type\"] == \"attr\" and self.__class__.__name__ != \"Concept_Pattern\":\n",
    "                attr_node_list.append(node)\n",
    "                continue\n",
    "            node_list.append(node)\n",
    "            if info[\"type\"] == \"input\":\n",
    "                node_color.append(\"#58509d\")\n",
    "            elif info[\"type\"] == \"concept\":\n",
    "                node_color.append(\"#C515E8\")\n",
    "                if self.get_node_value(node) is not None:\n",
    "                    node_size = 1800\n",
    "            elif info[\"type\"] == \"self\":\n",
    "                node_color.append(\"#1f78b4\")\n",
    "            elif info[\"type\"] == \"obj\":\n",
    "                if hasattr(self, \"pivot_node_names\") and self.pivot_node_names is not None and node in self.pivot_node_names:\n",
    "                    node_color.append(\"#FA49E2\")\n",
    "                elif hasattr(self, \"refer_node_names\") and self.refer_node_names is not None and node in self.refer_node_names:\n",
    "                    node_color.append(\"#FA9968\")\n",
    "                else:\n",
    "                    node_color.append(\"#C31B37\")\n",
    "            elif info[\"type\"] == \"attr\":\n",
    "                node_color.append(\"#EB8F8F\")\n",
    "            elif info[\"type\"].startswith(\"fun\"):\n",
    "                if info[\"type\"] == \"fun-out\":\n",
    "                    node_color.append(\"orange\")\n",
    "                else:\n",
    "                    node_color.append(\"g\")\n",
    "            else:\n",
    "                raise\n",
    "            node_sizes.append(node_size)\n",
    "\n",
    "        # Edges:\n",
    "        edge_color = []\n",
    "        edge_list = []\n",
    "        for ni, no, data in G.edges(data=True):\n",
    "            if no in attr_node_list:\n",
    "                continue\n",
    "            edge_list.append((ni, no))\n",
    "            if \"intra-relation\" in data[\"type\"]:\n",
    "                edge_color.append(\"purple\")\n",
    "            elif \"intra\" in data[\"type\"]:\n",
    "                edge_color.append(\"k\")\n",
    "            elif \"inter\" in data[\"type\"]:\n",
    "                if data[\"type\"].endswith(\"inter-input\"):\n",
    "                    edge_color.append(\"brown\")\n",
    "                elif data[\"type\"].endswith(\"inter-criteria\"):\n",
    "                    edge_color.append(\"c\")\n",
    "                else:\n",
    "                    raise\n",
    "            elif data[\"type\"].endswith(\"get-attr\"):\n",
    "                edge_color.append(\"#E815DA\")\n",
    "            else:\n",
    "                raise\n",
    "        # Set up layout:\n",
    "        if layout == \"planar\":\n",
    "            pos = nx.planar_layout(G)\n",
    "        elif layout == \"spring\":\n",
    "            pos = nx.spring_layout(G)\n",
    "        elif layout == \"spectral\":\n",
    "            pos = nx.spectral_layout(G)\n",
    "        elif layout == \"spiral\":\n",
    "            pos = nx.spiral_layout(G)\n",
    "        elif layout == \"shell\":\n",
    "            pos = nx.shell_layout(G)\n",
    "        elif layout == \"random\":\n",
    "            pos = nx.random_layout(G)\n",
    "        elif layout == \"kk\":\n",
    "            pos = nx.kamada_kawai_layout(G)\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "        # Draw:\n",
    "        G.remove_nodes_from(attr_node_list)\n",
    "        nx.draw(G, with_labels=True, font_size=10, pos=pos, alpha=0.8 if isinstance(self, Concept_Pattern) else 1,\n",
    "                nodelist=node_list, node_color=node_color, node_size=node_sizes,\n",
    "                edgelist=edge_list, edge_color=edge_color,\n",
    "               )\n",
    "\n",
    "        # Draw edge labels:\n",
    "        nx.draw_networkx_edge_labels(\n",
    "            G,\n",
    "            pos,\n",
    "            edge_labels={edge: \",\".join(relation_list) for edge, relation_list in self.get_relations().items()},\n",
    "            font_color='red',\n",
    "        )\n",
    "\n",
    "        # Recursively draw composite concepts:\n",
    "        if filename is not None:\n",
    "            plt.savefig(filename, bbox_inches=\"tight\", **kwargs)\n",
    "        else:\n",
    "            plt.show()\n",
    "            if isinstance(self, Concept) and self.name is not None:\n",
    "                value = self.get_node_value(self.name)\n",
    "                if value is not None:\n",
    "                    if len(value.shape) == 2:\n",
    "                        visualize_matrices([value])\n",
    "                    elif len(value.shape) == 3:\n",
    "                        value_T = deepcopy(value).permute(1,2,0)\n",
    "                        plt.imshow(to_np_array(value_T).astype(int))\n",
    "                        plt.show()\n",
    "\n",
    "            # If certain node's content is a graph, recursively draw it:\n",
    "            for node in self.nodes:\n",
    "                node_content = self.get_node_content(node)\n",
    "                if isinstance(node_content, BaseGraph):\n",
    "                    print(\"\\nDrawing the content of node '{}', which is {}:\".format(node, node_content))\n",
    "                    node_content.draw()\n",
    "        return self\n",
    "\n",
    "\n",
    "    def parse(self, input, Sobj, abs_pos=(0,0), score_mode=\"mean\", **kwargs):\n",
    "        \"\"\"Parse an input image (or CONCEPTS[DEFAULT_OBJ_TYPE]), and see if it satisfies\n",
    "        all the relations required by the current concept.\n",
    "        \n",
    "        Args:\n",
    "            input: A Concept() instance with its attribute of absolute position,\n",
    "                or a PyTorch Tensor for the value of the concept, in which case \n",
    "                the abs_pos must be given.\n",
    "            Sobj: function to select objects from a scene.\n",
    "            abs_pos: the absolute position of the input, in the case where input\n",
    "                is a PyTorch tensor.\n",
    "        \"\"\"\n",
    "        def assign_key_to_concept_inst(key_dict, concept_inst_dict):\n",
    "            \"\"\"\n",
    "            Assign concept instances to corresponding keys.\n",
    "\n",
    "            Example:\n",
    "                key_dict:          {\"Line\": [\"line1:Line\", \"line2:Line\"]}\n",
    "                concept_inst_dict: {\"Line\": [Line(), Line()]}\n",
    "\n",
    "            Return:\n",
    "                all_dictL          {\"line1:Line\": Line(), \"line2:Line\": Line()}\n",
    "            \"\"\"\n",
    "            all_dict = OrderedDict()\n",
    "            assert set(key_dict) == set(concept_inst_dict)\n",
    "            for key, key_ele_list in key_dict.items():\n",
    "                concept_inst_list = concept_inst_dict[key]\n",
    "                if len(key_ele_list) > len(concept_inst_list):\n",
    "                    return all_dict\n",
    "                for key_ele, concept_inst in zip(key_ele_list, concept_inst_list):\n",
    "                    all_dict[key_ele] = concept_inst\n",
    "            return all_dict\n",
    "\n",
    "        if isinstance(input, Concept):\n",
    "            abs_pos = to_np_array(input.get_node_value(\"pos\")[:2]).round().astype(int) if input.get_node_value(\"pos\") is not None else (0, 0)\n",
    "            input = input.get_root_value()\n",
    "            # The input is a Tensor.\n",
    "        score_dict = {}\n",
    "        is_valid = True\n",
    "        pos_all_dict = OrderedDict()\n",
    "        if not hasattr(self, \"re\"):\n",
    "            is_valid = False\n",
    "        else:\n",
    "            for key, relation in self.re.items():\n",
    "                if isinstance(key, tuple):\n",
    "                    # The relation concerns two or more attributes:\n",
    "                    concept_inst_dict = OrderedDict()\n",
    "                    key_dict = OrderedDict()\n",
    "                    for key_ele in key:\n",
    "                        key_ele = self.get_node_name(key_ele)\n",
    "                        mode = key_ele.split(\":\")[-1]\n",
    "                        if mode not in key_dict:\n",
    "                            key_dict[mode] = [key_ele]\n",
    "                        else:\n",
    "                            key_dict[mode].append(key_ele)\n",
    "                        if mode not in concept_inst_dict:\n",
    "                            concept_instances = Sobj(input, CONCEPTS[mode], abs_pos=abs_pos)\n",
    "                            # Get pos of all the concept_instances:\n",
    "                            if not isinstance(concept_instances, dict):\n",
    "                                concept_instances = OrderedDict([[\"Sobj-0\", concept_instances]])\n",
    "                            # Save to concept_inst_dict:\n",
    "                            concept_inst_dict[mode] = list(concept_instances.values())\n",
    "                    assigned_dict = assign_key_to_concept_inst(key_dict, concept_inst_dict)\n",
    "                    pos_all_dict.update(OrderedDict([[key_part.split(\":\")[0], instance.get_node_value(\"pos\")] for key_part, instance in assigned_dict.items()]))\n",
    "                    if len(assigned_dict) == 0:\n",
    "                        return False, score_dict, None\n",
    "                    score = relation(*[assigned_dict[self.get_node_name(key_ele)] for key_ele in key])\n",
    "                    if isinstance(score, Concept):\n",
    "                        score = score.get_root_value()\n",
    "                else:\n",
    "                    # The relation concerns only one attribute or itself:\n",
    "                    if key == \"self\":\n",
    "                        score = relation(input)\n",
    "                        pos_all_dict[\"self\"] = (abs_pos[0], abs_pos[1], input.shape[0], input.shape[1])\n",
    "                    else:\n",
    "                        score = relation(kwargs[key])\n",
    "                if isinstance(score, Concept):\n",
    "                    score = to_np_array(score.get_root_value())\n",
    "                score_dict[key] = score\n",
    "                is_valid = is_valid and (score >= 1.)\n",
    "        if is_valid:\n",
    "            if len(pos_all_dict) == 0:\n",
    "                pos_all = [[abs_pos[0], abs_pos[1], input.shape[0], input.shape[1]]]  # In case that it is a single image.\n",
    "            else:\n",
    "                pos_all = list(pos_all_dict.values())\n",
    "            G = self.copy()\n",
    "            pos_union = combine_pos(*pos_all)\n",
    "            G.set_node_value(pos_union, \"pos:Pos\")\n",
    "            G.set_node_value(input[pos_union[0] - abs_pos[0]: pos_union[0] + pos_union[2] - abs_pos[0], \n",
    "                                   pos_union[1] - abs_pos[1]: pos_union[1] + pos_union[3] - abs_pos[1]], G.name)\n",
    "            if \"assigned_dict\" in locals():\n",
    "                for key, item in assigned_dict.items():\n",
    "                    G.set_node_value(item.get_root_value(), key)\n",
    "            for key, pos in pos_all_dict.items():\n",
    "                if key != \"self\":\n",
    "                    G.set_node_value(pos, \"{}^pos\".format(key))\n",
    "            for key, item in kwargs.items():\n",
    "                G.set_node_value(item, key)\n",
    "        else:\n",
    "            G = None\n",
    "        score = get_generalized_mean(list(score_dict.values()), cumu_mode=score_mode)\n",
    "        return is_valid, (score, score_dict), G\n",
    "\n",
    "\n",
    "    @property\n",
    "    def DL(self):\n",
    "        \"\"\"Description length of the concept.\"\"\"\n",
    "        return len(self.attributes) + 1 + self.edge_index.shape[1]\n",
    "\n",
    "\n",
    "    # Overloading mathematical operations:\n",
    "    def __bool__(self):\n",
    "        \"\"\"True (or False)\"\"\"\n",
    "        return self.get_node_content().__bool__()\n",
    "\n",
    "\n",
    "    def __gt__(self, concept2):\n",
    "        \"\"\"Greater than (>). Both the self and concept2 must have the same root shape. Return a BoolTensor of the same shape.\"\"\"\n",
    "        tensor_self = self.get_node_value()\n",
    "        tensor_other = concept2.get_node_value() if isinstance(concept2, Concept) else concept2\n",
    "        tensor_gt = tensor_self > tensor_other\n",
    "        G = self.copy()\n",
    "        G.set_node_value(tensor_gt)\n",
    "        for obj_name in self.obj_names:\n",
    "            tensor_obj_other = concept2.get_node_value(obj_name) if isinstance(concept2, Concept) else concept2\n",
    "            tensor_obj_gt = self.get_node_value(obj_name) > tensor_obj_other\n",
    "            G.set_node_value(tensor_obj_gt, obj_name)\n",
    "        return G\n",
    "\n",
    "\n",
    "    def __lt__(self, concept2):\n",
    "        \"\"\"Less than (<). Both the self and concept2 must have the same root shape. Return a BoolTensor of the same shape.\"\"\"\n",
    "        tensor_self = self.get_node_value()\n",
    "        tensor_other = concept2.get_node_value() if isinstance(concept2, Concept) else concept2\n",
    "        tensor_lt = tensor_self < tensor_other\n",
    "        G = self.copy()\n",
    "        G.set_node_value(tensor_lt)\n",
    "        for obj_name in self.obj_names:\n",
    "            tensor_obj_other = concept2.get_node_value(obj_name) if isinstance(concept2, Concept) else concept2\n",
    "            tensor_obj_lt = self.get_node_value(obj_name) < tensor_obj_other\n",
    "            G.set_node_value(tensor_obj_lt, obj_name)\n",
    "        return G\n",
    "\n",
    "\n",
    "    def __eq__(self, concept2):\n",
    "        \"\"\"Equal to (==). Return a Bool(True/False) concept.\"\"\"\n",
    "        tensor_self = self.get_node_value()\n",
    "        tensor_other = concept2.get_node_value() if isinstance(concept2, Concept) else concept2\n",
    "        if tuple(tensor_self.shape) == tuple(tensor_other.shape) and (tensor_self == tensor_other).all():\n",
    "            tensor_eq = torch.BoolTensor([True])[0]\n",
    "        else:\n",
    "            tensor_eq = torch.BoolTensor([False])[0]\n",
    "        C = CONCEPTS[\"Bool\"].copy().set_node_value(tensor_eq)\n",
    "        return C\n",
    "\n",
    "\n",
    "    # Printing:\n",
    "    def __str__(self):\n",
    "        repr_str = self.graph[\"name\"] if \"name\" in self.graph else \"Concept\"\n",
    "        # Composing content string:\n",
    "        content_str = \"\"\n",
    "        attr_node_list = self.obj_names\n",
    "        if len(attr_node_list) > 0:\n",
    "            for attr_node in attr_node_list:\n",
    "                content_str += \"{}, \".format(attr_node)\n",
    "            content_str = content_str[:-2]\n",
    "        if repr_str == \"Bool\":\n",
    "            content_str = str(to_np_array(self.get_root_value()))\n",
    "        return '{}({})'.format(repr_str, content_str)\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        if IS_VIEW:\n",
    "            if len(self.nodes) > 0:\n",
    "                self.draw()\n",
    "            if hasattr(self, \"re\"):\n",
    "                for key, relation in self.re.items():\n",
    "                    if isinstance(relation, Graph):\n",
    "                        print(\"The relation on {} is {}, as follows:\".format(key, relation))\n",
    "                        relation.draw()\n",
    "        return self.__str__()\n",
    "\n",
    "\n",
    "    def get_string_repr(self, mode=\"obj\"):\n",
    "        \"\"\"Get 1D string representation of the concept.\"\"\"\n",
    "        if mode == \"obj\":\n",
    "            combined_dict = OrderedDict()\n",
    "            combined_dict[self.name] = tensor_to_string(self.get_node_value())\n",
    "            if self.name.split(\":\")[-1] == DEFAULT_OBJ_TYPE:\n",
    "                combined_dict[\"pos\"] = tensor_to_string(self.get_node_value(\"pos\"))\n",
    "            for obj_name in self.obj_names:\n",
    "                combined_dict[obj_name] = tensor_to_string(self.get_node_value(obj_name))\n",
    "                pos_obj = self.operator_name(obj_name) + \"^pos\"\n",
    "                combined_dict[pos_obj] = tensor_to_string(self.get_node_value(pos_obj))\n",
    "            string = \"\"\n",
    "            for key, item in combined_dict.items():\n",
    "                string += \"#{}${}\".format(key, item)\n",
    "        else:\n",
    "            raise Exception(\"mode {} is not valid!\".format(mode))\n",
    "        return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 Concept_Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Concept_Pattern(Concept):\n",
    "    \"\"\"Concept_Pattern (inherited from Concept) for refering to a subset of objects in a concept graph. In addition to Concept, it has \n",
    "    a pivot node(s) and pivot edges, and a subset of refer_nodes. The pivot node(edge) is for identifying\n",
    "    and pivotting the Concept_Pattern within a larger concept graph, and refer nodes are the nodes being referred to.\n",
    "    All edges are pivot edges.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        G=None,\n",
    "        pivot_node_names=None,\n",
    "        refer_node_names=None,\n",
    "        parent_root_name=None,\n",
    "        is_all_obj=False,\n",
    "        is_ebm=False,\n",
    "        is_default_ebm=False,\n",
    "        is_selector_gnn=False,\n",
    "        ebm_dict=None,\n",
    "        gnn=None,\n",
    "        CONCEPTS=None,\n",
    "        OPERATORS=None,\n",
    "        cache_forward=False,\n",
    "        in_channels=10,\n",
    "        # EBM specific:\n",
    "        z_mode=\"None\",\n",
    "        z_first=2,\n",
    "        z_dim=4,\n",
    "        w_type=\"image+mask\",\n",
    "        mask_mode=\"concat\",\n",
    "        aggr_mode=\"max\",\n",
    "        pos_embed_mode=\"None\",\n",
    "        is_ebm_share_param=True,\n",
    "        is_relation_z=False,\n",
    "        img_dims=2,\n",
    "        is_spec_norm=True,\n",
    "        act_name=\"leakyrelu0.2\",\n",
    "        normalization_type=\"None\",\n",
    "        # Selector specific:\n",
    "        channel_coef=None,\n",
    "        empty_coef=None,\n",
    "        obj_coef=None,\n",
    "        mutual_exclusive_coef=None,\n",
    "        pixel_entropy_coef=None,\n",
    "        pixel_gm_coef=None,\n",
    "        iou_batch_consistency_coef=None,\n",
    "        iou_concept_repel_coef=None,\n",
    "        iou_relation_repel_coef=None,\n",
    "        iou_relation_overlap_coef=None,\n",
    "        iou_attract_coef=None,\n",
    "        SGLD_is_anneal=None,\n",
    "        SGLD_is_penalize_lower=None,\n",
    "        SGLD_mutual_exclusive_coef=None,\n",
    "        SGLD_pixel_entropy_coef=None,\n",
    "        SGLD_pixel_gm_coef=None,\n",
    "        SGLD_iou_batch_consistency_coef=None,\n",
    "        SGLD_iou_concept_repel_coef=None,\n",
    "        SGLD_iou_relation_repel_coef=None,\n",
    "        SGLD_iou_relation_overlap_coef=None,\n",
    "        SGLD_iou_attract_coef=None,\n",
    "        lambd_start=None,\n",
    "        lambd=None,\n",
    "        image_value_range=None,\n",
    "        w_init_type=None,\n",
    "        indiv_sample=None,\n",
    "        step_size=None,\n",
    "        step_size_img=None,\n",
    "        step_size_z=None,\n",
    "        step_size_zgnn=None,\n",
    "        step_size_wtarget=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Concept_Pattern as a partially instantiated Concept, for selecting object(s) within a Concept instance.\n",
    "\n",
    "        Args:\n",
    "            is_default_ebm: if True, the DEFAULT_OBJ_TYPE will also has its EBM.\n",
    "        \"\"\"\n",
    "        super(Concept_Pattern, self).__init__(G=G, is_all_obj=is_all_obj, **kwargs)\n",
    "        if parent_root_name is not None:\n",
    "            rename_mapping = {}\n",
    "            for node_name in self.nodes:\n",
    "                if node_name.startswith(\"{}^\".format(parent_root_name)):\n",
    "                    rename_mapping[node_name] = node_name.split(\"{}^\".format(parent_root_name))[1]\n",
    "            G = nx.relabel_nodes(self, rename_mapping)\n",
    "            self.__dict__.update(G.__dict__)\n",
    "        self.name = None\n",
    "        if G is not None and G.device is not None:\n",
    "            self.device = G.device\n",
    "        else:\n",
    "            self.device = kwargs[\"device\"] if \"device\" in kwargs else torch.device(\"cpu\")\n",
    "        for node_name in self.nodes:\n",
    "            if \"fun\" in self.nodes(data=True)[node_name]:\n",
    "                self.nodes(data=True)[node_name]['fun'] = None\n",
    "        if pivot_node_names is not None:\n",
    "            self.set_pivot_nodes(pivot_node_names)\n",
    "        else:\n",
    "            self.pivot_node_names = pivot_node_names\n",
    "        if refer_node_names is not None:\n",
    "            self.set_refer_nodes(refer_node_names)\n",
    "        else:\n",
    "            self.refer_node_names = refer_node_names\n",
    "        self.in_channels = in_channels\n",
    "        # EBM specific:\n",
    "        self.z_mode = z_mode\n",
    "        self.z_first = z_first\n",
    "        self.z_dim = z_dim\n",
    "        self.w_type = w_type\n",
    "        self.mask_mode = mask_mode\n",
    "        self.aggr_mode = aggr_mode\n",
    "        self.pos_embed_mode = pos_embed_mode\n",
    "        self.is_ebm_share_param = is_ebm_share_param\n",
    "        self.is_relation_z = is_relation_z\n",
    "        self.img_dims = img_dims\n",
    "        self.is_spec_norm = is_spec_norm\n",
    "        self.act_name = act_name\n",
    "        self.normalization_type = normalization_type\n",
    "        # selector specific:\n",
    "        self.channel_coef = channel_coef\n",
    "        self.empty_coef = empty_coef\n",
    "        self.obj_coef = obj_coef\n",
    "        self.mutual_exclusive_coef = mutual_exclusive_coef\n",
    "        self.pixel_entropy_coef = pixel_entropy_coef\n",
    "        self.pixel_gm_coef = pixel_gm_coef\n",
    "        self.iou_batch_consistency_coef = iou_batch_consistency_coef\n",
    "        self.iou_concept_repel_coef = iou_concept_repel_coef\n",
    "        self.iou_relation_repel_coef = iou_relation_repel_coef\n",
    "        self.iou_relation_overlap_coef = iou_relation_overlap_coef\n",
    "        self.iou_attract_coef = iou_attract_coef\n",
    "        self.SGLD_is_anneal = SGLD_is_anneal\n",
    "        self.SGLD_is_penalize_lower = SGLD_is_penalize_lower\n",
    "        self.SGLD_mutual_exclusive_coef = SGLD_mutual_exclusive_coef\n",
    "        self.SGLD_pixel_entropy_coef = SGLD_pixel_entropy_coef\n",
    "        self.SGLD_pixel_gm_coef = SGLD_pixel_gm_coef\n",
    "        self.SGLD_iou_batch_consistency_coef = SGLD_iou_batch_consistency_coef\n",
    "        self.SGLD_iou_concept_repel_coef = SGLD_iou_concept_repel_coef\n",
    "        self.SGLD_iou_relation_repel_coef = SGLD_iou_relation_repel_coef\n",
    "        self.SGLD_iou_relation_overlap_coef = SGLD_iou_relation_overlap_coef\n",
    "        self.SGLD_iou_attract_coef = SGLD_iou_attract_coef\n",
    "        self.lambd_start = lambd_start\n",
    "        self.lambd = lambd\n",
    "        self.image_value_range = image_value_range\n",
    "        self.w_init_type = w_init_type\n",
    "        self.indiv_sample = indiv_sample\n",
    "        self.step_size = step_size\n",
    "        self.step_size_img = step_size_img\n",
    "        self.step_size_z = step_size_z\n",
    "        self.step_size_zgnn = step_size_zgnn\n",
    "        self.step_size_wtarget = step_size_wtarget\n",
    "\n",
    "        # When cache_forward is True, store a dictionary of input image hash to\n",
    "        # forward_NN results\n",
    "        self.cache_forward = cache_forward\n",
    "        if G is not None:\n",
    "            self.forward_cache = G.forward_cache\n",
    "        else:\n",
    "            self.forward_cache = {} if self.cache_forward else None\n",
    "\n",
    "        self.is_all_obj = is_all_obj\n",
    "        self.is_ebm = is_ebm\n",
    "        self.is_default_ebm = is_default_ebm\n",
    "        self.is_selector_gnn = is_selector_gnn\n",
    "        if self.is_ebm:\n",
    "            if ebm_dict is not None:\n",
    "                if self.is_ebm_share_param:\n",
    "                    if len(ebm_dict) == 0:\n",
    "                        self.ebm_dict = Shared_Param_Dict(is_relation_z=self.is_relation_z)\n",
    "                    else:\n",
    "                        self.ebm_dict = ebm_dict\n",
    "                else:\n",
    "                    self.ebm_dict = Combined_Dict(ebm_dict).set_is_relation_z(self.is_relation_z)\n",
    "            self.CONCEPTS = CONCEPTS\n",
    "            self.OPERATORS = OPERATORS\n",
    "            self.init_ebms(\n",
    "                method=\"random\",\n",
    "                ebm_model_type=\"CEBM\",\n",
    "                CONCEPTS=self.CONCEPTS,\n",
    "                OPERATORS=self.OPERATORS,\n",
    "                **kwargs\n",
    "            )\n",
    "            if self.is_selector_gnn:\n",
    "                self.gnn = gnn\n",
    "                self.zgnn_dim = gnn.zgnn_dim\n",
    "                self.edge_attr_size = gnn.edge_attr_size\n",
    "\n",
    "\n",
    "    def copy_with_grad(self, is_share_fun=False, is_copy_module=True, global_attrs=None):\n",
    "        \"\"\"Return the copy of current instance by detaching tensors which have grad\n",
    "        and deepcopying all other object attributes.\n",
    "\n",
    "        Args:\n",
    "            is_share_fun: if True, the copy will share its torch.nn.Modules with its original.\n",
    "            is_copy_module: if True, will copy torch.nn.Module. Otherwise not copy.\n",
    "            global_attrs: a list of class attribute names that are global dictionaries.\n",
    "\n",
    "        Returns:\n",
    "            G: the copied class instance.\n",
    "        \"\"\"\n",
    "        G_copy = self.__class__()\n",
    "        copied_dict, global_dicts = copy_helper(self.__dict__, is_copy_module=is_copy_module, global_attrs=global_attrs)\n",
    "        G_copy.__dict__.update(copied_dict)\n",
    "        G = self.__class__(\n",
    "            G=G_copy,\n",
    "            pivot_node_names=deepcopy(self.pivot_node_names),\n",
    "            refer_node_names=deepcopy(self.refer_node_names),\n",
    "            is_all_obj=self.is_all_obj,\n",
    "            is_ebm=self.is_ebm,\n",
    "            is_default_ebm=self.is_default_ebm,\n",
    "            is_selector_gnn=self.is_selector_gnn,\n",
    "            gnn=self.gnn,\n",
    "            ebm_dict=self.ebm_dict,\n",
    "            CONCEPTS=self.CONCEPTS,\n",
    "            OPERATORS=self.OPERATORS,\n",
    "            cache_forward=self.cache_forward,\n",
    "            in_channels=self.in_channels,\n",
    "            z_mode=self.z_mode,\n",
    "            z_first=self.z_first,\n",
    "            z_dim=self.z_dim,\n",
    "            w_type=self.w_type,\n",
    "            mask_mode=self.mask_mode,\n",
    "            aggr_mode=self.aggr_mode,\n",
    "            pos_embed_mode=self.pos_embed_mode,\n",
    "            is_ebm_share_param=self.is_ebm_share_param,\n",
    "            is_relation_z=self.is_relation_z,\n",
    "            img_dims=self.img_dims,\n",
    "            is_spec_norm=self.is_spec_norm,\n",
    "            act_name=self.act_name,\n",
    "            normalization_type=self.normalization_type,\n",
    "            channel_coef=self.channel_coef,\n",
    "            empty_coef=self.empty_coef,\n",
    "            obj_coef=self.obj_coef,\n",
    "            mutual_exclusive_coef=self.mutual_exclusive_coef,\n",
    "            pixel_entropy_coef=self.pixel_entropy_coef,\n",
    "            pixel_gm_coef=self.pixel_gm_coef,\n",
    "            iou_batch_consistency_coef=self.iou_batch_consistency_coef,\n",
    "            iou_concept_repel_coef=self.iou_concept_repel_coef,\n",
    "            iou_relation_repel_coef=self.iou_relation_repel_coef,\n",
    "            iou_relation_overlap_coef=self.iou_relation_overlap_coef,\n",
    "            iou_attract_coef=self.iou_attract_coef,\n",
    "            SGLD_is_anneal=self.SGLD_is_anneal,\n",
    "            SGLD_is_penalize_lower=self.SGLD_is_penalize_lower,\n",
    "            SGLD_mutual_exclusive_coef=self.SGLD_mutual_exclusive_coef,\n",
    "            SGLD_pixel_entropy_coef=self.SGLD_pixel_entropy_coef,\n",
    "            SGLD_pixel_gm_coef=self.SGLD_pixel_gm_coef,\n",
    "            SGLD_iou_batch_consistency_coef=self.SGLD_iou_batch_consistency_coef,\n",
    "            SGLD_iou_concept_repel_coef=self.SGLD_iou_concept_repel_coef,\n",
    "            SGLD_iou_relation_repel_coef=self.SGLD_iou_relation_repel_coef,\n",
    "            SGLD_iou_relation_overlap_coef=self.SGLD_iou_relation_overlap_coef,\n",
    "            SGLD_iou_attract_coef=self.SGLD_iou_attract_coef,\n",
    "            lambd_start=self.lambd_start,\n",
    "            lambd=self.lambd,\n",
    "            image_value_range=self.image_value_range,\n",
    "            w_init_type=self.w_init_type,\n",
    "            indiv_sample=self.indiv_sample,\n",
    "            step_size=self.step_size,\n",
    "            step_size_img=self.step_size_img,\n",
    "            step_size_z=self.step_size_z,\n",
    "            step_size_zgnn=self.step_size_zgnn,\n",
    "            step_size_wtarget=self.step_size_wtarget,\n",
    "        )\n",
    "\n",
    "        if global_attrs is not None:\n",
    "            # Set the global dictionaries as attributes of the class:\n",
    "            for key, Dict in global_dicts.items():\n",
    "                setattr(G, key, Dict)\n",
    "        return G\n",
    "\n",
    "\n",
    "    def set_cache_forward(self, cache_forward):\n",
    "        \"\"\"Set the cache_forward attribute.\"\"\"\n",
    "        self.cache_forward = cache_forward\n",
    "        if cache_forward is True:\n",
    "            if self.forward_cache is None:\n",
    "                self.forward_cache = {}\n",
    "        elif cache_forward is False:\n",
    "            self.forward_cache = None\n",
    "        else:\n",
    "            raise Exception(\"cache_forward must be True or False!\")\n",
    "\n",
    "\n",
    "    def init_ebms(\n",
    "        self,\n",
    "        method=\"random\",\n",
    "        ebm_model_type=\"CEBM\",\n",
    "        CONCEPTS=None,\n",
    "        OPERATORS=None,\n",
    "        ebm_dict=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Initialize the EBMs for the Concept_Pattern.\n",
    "\n",
    "        Args:\n",
    "            mode: choose from \"trained\" (loading from best trained EBMs), \n",
    "                and \"random\" (initialize from random parameters or load from ebm_dict)\n",
    "            ebm_model_type: model_type for the EBMs. Choose from \"CEBM\", \"ConjEBM\".\n",
    "            ebm_dict: if not None, will contain some already-existing EBMs, whose key are the concepts/relation/operator type.\n",
    "                Only effective if mode==\"random\".\n",
    "            kwargs: init parameters for the EBM model if mode == \"random\".\n",
    "\n",
    "        Returns:\n",
    "            ebm_dict: A dictionary of EBMs, whose keys are the concept/relation/operator's type.\n",
    "        \"\"\"\n",
    "\n",
    "        # Update the self.ebm_dict with the gievn ebm_dict. If not exist, create one:\n",
    "        self.is_ebm = True\n",
    "        if ebm_dict is None:\n",
    "            if not hasattr(self, \"ebm_dict\"):\n",
    "                self.ebm_dict = Shared_Param_Dict() if self.is_ebm_share_param else Combined_Dict()\n",
    "        else:\n",
    "            assert hasattr(self, \"ebm_dict\")\n",
    "            self.ebm_dict.update(ebm_dict)\n",
    "\n",
    "        # Setting up the EBMs for concept nodes:\n",
    "        for node in self.nodes:\n",
    "            placeholder = self.get_node_content(node)\n",
    "            placeholder.ebm_key = self.init_ebm(\n",
    "                method=method,\n",
    "                mode=placeholder.mode,\n",
    "                ebm_mode=\"concept\",\n",
    "                ebm_model_type=ebm_model_type,\n",
    "                CONCEPTS=CONCEPTS if CONCEPTS is not None else self.CONCEPTS,\n",
    "                **kwargs\n",
    "            )\n",
    "\n",
    "        # Setting up the EBMs for the relation nodes:\n",
    "        for node_source, node_target, info in self.edges(data=True):\n",
    "            assert info[\"type\"] == \"intra-relation\"\n",
    "            relation_name = info[\"name\"]\n",
    "            placeholder = Placeholder(mode=relation_name)\n",
    "            info[\"value\"] = placeholder\n",
    "            placeholder.ebm_key = self.init_ebm(\n",
    "                method=method,\n",
    "                mode=placeholder.mode,\n",
    "                ebm_mode=\"operator\",\n",
    "                ebm_model_type=ebm_model_type,\n",
    "                OPERATORS=OPERATORS if OPERATORS is not None else self.OPERATORS,\n",
    "                **kwargs\n",
    "            )\n",
    "        return self\n",
    "\n",
    "\n",
    "    def init_ebm(self, method, mode, ebm_mode, ebm_model_type, CONCEPTS=None, OPERATORS=None, **kwargs):\n",
    "        \"\"\"Initialize the EBMs for the Concept_Pattern.\n",
    "\n",
    "        Args:\n",
    "            method: choose from \"trained\" (loading from best trained EBMs),\n",
    "                and \"random\" (initialize from random parameters or load from ebm_dict)\n",
    "            mode: concept type or relation type.\n",
    "            ebm_model_type: model_type for the EBMs. Choose from \"CEBM\", \"ConjEBM\".\n",
    "            ebm_dict: if not None, will contain some already-existing EBMs, whose key are the concepts/relation/operator type.\n",
    "                Only effective if mode==\"random\".\n",
    "            kwargs: init parameters for the EBM model if mode == \"random\".\n",
    "\n",
    "        Returns:\n",
    "            ebm_dict: A dictionary of EBMs, whose keys are the concept/relation/operator's type.\n",
    "        \"\"\"\n",
    "\n",
    "        if not self.is_default_ebm and mode == DEFAULT_OBJ_TYPE:\n",
    "            return\n",
    "\n",
    "        from reasoning.experiments.models import ConceptEBM, load_model_atom\n",
    "        # Initialize the parameters:\n",
    "        channel_base = kwargs[\"channel_base\"] if \"channel_base\" in kwargs else 128\n",
    "        two_branch_mode = kwargs[\"two_branch_mode\"] if \"two_branch_mode\" in kwargs else \"concat\"\n",
    "        c_repr_mode = kwargs[\"c_repr_mode\"] if \"c_repr_mode\" in kwargs else \"c2\"\n",
    "        c_repr_first = kwargs[\"c_repr_first\"] if \"c_repr_first\" in kwargs else 2\n",
    "        if method == \"trained\":\n",
    "            self.ebm_dict[mode] = load_model_atom(model_atom_str=mode, model_type=ebm_model_type, device=self.device)\n",
    "        elif method == \"random\":\n",
    "            if mode not in self.ebm_dict:\n",
    "                if ebm_model_type == \"CEBM\":\n",
    "                    if ebm_mode == \"concept\":\n",
    "                        Dict = CONCEPTS if CONCEPTS is not None else self.CONCEPTS\n",
    "                    elif ebm_mode == \"operator\":\n",
    "                        Dict = OPERATORS if OPERATORS is not None else self.OPERATORS\n",
    "                    else:\n",
    "                        raise\n",
    "                    if (not self.is_ebm_share_param) or (self.is_ebm_share_param and not self.ebm_dict.is_model_exist(ebm_mode)):\n",
    "                        self.ebm_dict[mode] = ConceptEBM(\n",
    "                            mode=ebm_mode,\n",
    "                            in_channels=self.in_channels,\n",
    "                            repr_dim=REPR_DIM,\n",
    "                            channel_base=channel_base,\n",
    "                            two_branch_mode=two_branch_mode,\n",
    "                            c_repr_mode=c_repr_mode,\n",
    "                            c_repr_first=c_repr_first,\n",
    "                            z_mode=\"None\" if self.is_relation_z is False and ebm_mode==\"operator\" else self.z_mode,\n",
    "                            z_first=self.z_first,\n",
    "                            z_dim=self.z_dim,\n",
    "                            w_type=self.w_type,\n",
    "                            mask_mode=self.mask_mode,\n",
    "                            pos_embed_mode=self.pos_embed_mode,\n",
    "                            aggr_mode=self.aggr_mode,\n",
    "                            img_dims=self.img_dims,\n",
    "                            is_spec_norm=self.is_spec_norm,\n",
    "                            act_name=self.act_name,\n",
    "                            normalization_type=self.normalization_type,\n",
    "                        ).set_c(c_repr=Dict[mode].get_node_repr()[None], c_str=mode).to(self.device)\n",
    "                    else:\n",
    "                        self.ebm_dict.add_c_repr(\n",
    "                            c_repr=Dict[mode].get_node_repr()[None].to(self.device),\n",
    "                            c_str=mode,\n",
    "                            ebm_mode=ebm_mode,\n",
    "                        )\n",
    "                else:\n",
    "                    raise Exception(\"model_type '{}' is not valid!\".format(model_type))\n",
    "        else:\n",
    "            raise Exception(\"method '{}' is not valid!\".format(method))\n",
    "        return mode\n",
    "\n",
    "\n",
    "    ## EBM-related functions:\n",
    "    def get_ebm(self, src):\n",
    "        \"\"\"Get individual EBMs from concept node or relations.\n",
    "\n",
    "        Args:\n",
    "            src: for obtaining the EBM for concept node, use node_name. For relation, use \n",
    "                a tuple of (source_node_name, target_node_name).\n",
    "\n",
    "        Returns:\n",
    "            self_ebm_dict: A dictionary of {mode: ebm_model}.\n",
    "        \"\"\"\n",
    "        self_ebm_dict = {}\n",
    "        if isinstance(src, str):\n",
    "            placeholder = self.get_node_content(src)\n",
    "            ebm_key = placeholder.ebm_key\n",
    "            if ebm_key is not None:\n",
    "                self_ebm_dict[ebm_key] = self.ebm_dict[ebm_key]\n",
    "        elif isinstance(src, tuple):\n",
    "            k = 0\n",
    "            src = (self.get_node_name(src[0]), self.get_node_name(src[1]))\n",
    "            for edge in self.edges:\n",
    "                if src[0] == edge[0] and src[1] == edge[1]:\n",
    "                    info = self.edges[(src[0], src[1], k)]\n",
    "                    placeholder = info[\"value\"]\n",
    "                    ebm_key = placeholder.ebm_key\n",
    "                    self_ebm_dict[ebm_key] = self.ebm_dict[ebm_key]\n",
    "                    k += 1\n",
    "        else:\n",
    "            raise Exception(\"src must be a str (for concept) or tuple (for relation)!\")\n",
    "        if len(self_ebm_dict) == 0:\n",
    "            self_ebm_dict = None\n",
    "        return self_ebm_dict\n",
    "\n",
    "\n",
    "    def get_ebms(self, ebm_type=\"all\"):\n",
    "        \"\"\"Get all the EBM keys.\n",
    "\n",
    "        Args:\n",
    "            ebm_type: type of the EBMs included. Choose from \"all\" (all types), \"concept\" and \"relation\".\n",
    "\n",
    "        Returns:\n",
    "            self_ebm_dict_all: A dictionary with key being the src, and value being a dictionary of {mode: ebm_model}.\n",
    "        \"\"\"\n",
    "        self_ebm_dict_all = {}\n",
    "        if ebm_type in [\"concept\", \"all\"]:\n",
    "            for node in self.nodes:\n",
    "                self_ebm_dict = self.get_ebm(node)\n",
    "                if self_ebm_dict is not None:\n",
    "                    self_ebm_dict_all[node] = self_ebm_dict\n",
    "        if ebm_type in [\"relation\", \"all\"]:\n",
    "            for edge in self.edges:\n",
    "                edge_tuple = tuple(edge[:2])\n",
    "                self_ebm_dict_all[edge_tuple] = self.get_ebm(edge_tuple)\n",
    "        return self_ebm_dict_all\n",
    "\n",
    "\n",
    "    def get_ebm_loc(self, ebm_type=\"all\"):\n",
    "        \"\"\"Obtain the dictionary of what masks each EBM refers to.\n",
    "        Has the format of {ebm_key: [list of obj_names or obj_pairs]}.\"\"\"\n",
    "        ebm_loc_dict = {}\n",
    "        if ebm_type in [\"concept\", \"all\"]:\n",
    "            for node in self.nodes:\n",
    "                self_ebm_dict = self.get_ebm(node)\n",
    "                if self_ebm_dict is not None:\n",
    "                    record_data(ebm_loc_dict, [node]*len(self_ebm_dict), list(self_ebm_dict.keys()))\n",
    "        if ebm_type in [\"relation\", \"all\"]:\n",
    "            for edge in self.edges:\n",
    "                edge_tuple = tuple(edge[:2])\n",
    "                self_ebm_dict = self.get_ebm(edge_tuple)\n",
    "                record_data(ebm_loc_dict, [edge_tuple]*len(self_ebm_dict), list(self_ebm_dict.keys()))\n",
    "        return ebm_loc_dict\n",
    "\n",
    "\n",
    "    def get_z_mode_dict(self):\n",
    "        \"\"\"\n",
    "        z_mode_dict: E.g.\n",
    "            OrderedDict([('obj_0:c0', 'c2'),\n",
    "                 ('obj_1:c1', 'c2'),\n",
    "                 ('obj_2:c2', 'c2'),\n",
    "                 ('obj_3:c3', 'c2'),\n",
    "                 ('obj_4:c4', 'c2'),\n",
    "                 ('obj_5:c5', 'c2'),\n",
    "                 (('obj_6:Image', 'obj_7:Image'), 'None'),\n",
    "                 (('obj_8:Image', 'obj_9:Image'), 'None'),\n",
    "                 (('obj_10:Image', 'obj_11:Image'), 'None')])\n",
    "        \"\"\"\n",
    "        ebm_dict = self.get_ebms()\n",
    "        z_mode_dict = OrderedDict()\n",
    "        for key in ebm_dict:\n",
    "            if isinstance(key, str):\n",
    "                z_mode_dict[key] = self.z_mode\n",
    "            else:\n",
    "                assert isinstance(key, tuple)\n",
    "                z_mode_dict[key] = self.z_mode if self.is_relation_z else \"None\"\n",
    "        return z_mode_dict\n",
    "\n",
    "\n",
    "    def get_mask_info(self):\n",
    "        \"\"\"\n",
    "        mask_info: a dictionary containing information about the masks. E.g.\n",
    "            {\n",
    "                id_to_type: {0: (\"concept\", 1), 1: (\"relation\", 0), 2: (\"concept\", 0), ...},  \n",
    "                    # The number are chosen from {0,1}, which indicates the number of object slot this mask occupies, for computing mutual_exclusive loss.\n",
    "                id_same_relation: [(1,3), (4,6), ...],\n",
    "            }\n",
    "        \"\"\"\n",
    "        key_to_id = {key: i for i, key in enumerate(self.nodes)}\n",
    "        id_to_type = {}\n",
    "        id_same_relation = []\n",
    "        for ebm_key, item in self.get_ebm_loc(\"concept\").items():\n",
    "            assert len(item) == 1\n",
    "            obj_slot_value = int(item[0].split(\":\")[-1] != DEFAULT_OBJ_TYPE)\n",
    "            id_to_type[key_to_id[item[0]]] = (\"concept\", obj_slot_value)\n",
    "        for ebm_key, item in self.get_ebm_loc(\"relation\").items():\n",
    "            assert len(item) == 1\n",
    "            obj_slot_value_0 = int(item[0][0].split(\":\")[-1] != DEFAULT_OBJ_TYPE)\n",
    "            obj_slot_value_1 = int(item[0][1].split(\":\")[-1] != DEFAULT_OBJ_TYPE)\n",
    "            id_to_type[key_to_id[item[0][0]]] = (\"relation\", obj_slot_value_0)\n",
    "            id_to_type[key_to_id[item[0][1]]] = (\"relation\", obj_slot_value_1)\n",
    "            id_same_relation.append((key_to_id[item[0][0]], key_to_id[item[0][1]]))\n",
    "        mask_info = {\n",
    "            \"id_to_type\": id_to_type,\n",
    "            \"id_same_relation\": id_same_relation,\n",
    "            \"z_mode_dict\": self.get_z_mode_dict(),\n",
    "        }\n",
    "        return mask_info\n",
    "\n",
    "\n",
    "    def get_masks_ebms_status(self, w_op_dict, threshold=0.5, active_n_pixel_threshold=3):\n",
    "        mask_active = {}\n",
    "        for key, mask in w_op_dict.items():\n",
    "            is_active = ((mask>=threshold).float().sum((1,2,3)) > active_n_pixel_threshold).all().item()\n",
    "            mask_active[key] = is_active\n",
    "        ebm_loc_dict = self.get_ebm_loc()\n",
    "        ebm_active = {}\n",
    "        for ebm_key in self.ebm_dict:\n",
    "            if ebm_key not in ebm_loc_dict:\n",
    "                continue\n",
    "            for keys in ebm_loc_dict[ebm_key]:\n",
    "                # key: a mask name or tuple of mask names\n",
    "                if isinstance(key, tuple):\n",
    "                    is_ebm_active = mask_active[key[0]] and mask_active[key[1]]\n",
    "                else:\n",
    "                    is_ebm_active = mask_active[key]\n",
    "                ebm_active[ebm_key] = is_ebm_active\n",
    "        return mask_active, ebm_active\n",
    "\n",
    "\n",
    "    def forward(self, input, w, c_repr=None, z=None, zgnn=None, wtarget=None, batch_shape=None, is_E_tensor=False):\n",
    "        \"\"\"\n",
    "        Computes the energy given the input and the objects.\n",
    "\n",
    "        Args:\n",
    "            input:  input image, [B, C, H, W]\n",
    "            w:  if the EBM has type of \"CEBM\", then w is a list (with length len(self.nodes)) of masks, each with shape [B, 1, H, W], where the list is ordered by self.nodes;\n",
    "                if the EBM has type of \"ConjEBM\", then w is a list (with length len(self.nodes)) of objects, each with shape [B, channel_size, H, W]\n",
    "            c_repr: embedding, , act as a placeholder\n",
    "            z:  list of latent representation. If self.z_mode != \"None\", then it is\n",
    "                a list of length len(self.get_ebms()).\n",
    "\n",
    "        Returns:\n",
    "            energy: the energy for the given inputs and objects (w).\n",
    "        \"\"\"\n",
    "        assert len(self.nodes) == len(w)\n",
    "        self.info = {}\n",
    "        ebm_dict_all = self.get_ebms()\n",
    "        nodes = list(self.topological_sort)\n",
    "        E_all_list = []\n",
    "        if self.is_selector_gnn:\n",
    "            c_all_list = []\n",
    "        for kk, (src, ebm_dict) in enumerate(ebm_dict_all.items()):\n",
    "            if ebm_dict is None:\n",
    "                continue\n",
    "            if isinstance(src, str):\n",
    "                # concept-EBM:\n",
    "                idx = nodes.index(src)\n",
    "                for mode, ebm in ebm_dict.items():\n",
    "                    # The ebm_dict here is a dict of {mode: ebm}\n",
    "                    z_ele = (z[kk],) if self.z_mode != \"None\" else None\n",
    "                    energy_ele = ebm(input, (w[idx],), z=z_ele)\n",
    "                    # The keys have the format of e.g. 'obj_2:c1->c1', \"('obj_0:c0', 'obj_1:c1')->r0\":\n",
    "                    E_all_list.append(energy_ele)\n",
    "                    self.info[\"{}->{}\".format(src, mode)] = to_np_array(energy_ele)\n",
    "                    if self.is_selector_gnn:\n",
    "                        c_all_list.append(ebm.c_repr)\n",
    "            elif isinstance(src, tuple):\n",
    "                # relation-EBM:\n",
    "                w_ele = tuple(w[nodes.index(src_ele)] for src_ele in src)\n",
    "                for mode, ebm in ebm_dict.items():\n",
    "                    z_ele = (z[kk],) if self.z_mode != \"None\" and z[kk] is not None else None\n",
    "                    if not (z_ele is None and w_ele[0] is None and w_ele[1] is None):\n",
    "                        energy_ele = ebm((input, input), w_ele, z=z_ele)\n",
    "                        E_all_list.append(energy_ele)\n",
    "                        self.info[\"{}->{}\".format(src, mode)] = to_np_array(energy_ele)\n",
    "                        if self.is_selector_gnn:\n",
    "                            c_all_list.append(ebm.c_repr)\n",
    "            else:\n",
    "                raise Exception(\"src must be a str or a 2-tuple!\")\n",
    "        E_all_list = torch.stack(E_all_list)  # [n_ebms, B_task * B_example, 1]\n",
    "        energy = E_all_list.sum(0)\n",
    "        if self.is_selector_gnn and zgnn is not None:\n",
    "            energy_gnn = self.gnn(w, z, tuple(c_all_list), zgnn, wtarget, batch_shape=batch_shape, x=input if self.z_mode==\"None\" else None)  # [B_task, B_example, 1]\n",
    "            energy_gnn = energy_gnn.view(-1, 1)   # [B_task * B_example, 1]\n",
    "            energy = energy + energy_gnn\n",
    "        if is_E_tensor:\n",
    "            self.info[\"E_all\"] = E_all_list\n",
    "            if self.is_selector_gnn:\n",
    "                self.info[\"E_gnn\"] = energy_gnn\n",
    "        return energy\n",
    "\n",
    "\n",
    "    def forward_NN_op(\n",
    "        self,\n",
    "        input,\n",
    "        op_name,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Performs the forward operation using the EBM on the node.\n",
    "        \"\"\"\n",
    "        # E.g. op_name = \"Identity-1:Image->sc$obj_0:c1\", node_name = \"obj_0:c1\":\n",
    "        node_name = op_name.split(\"->\")[-1].split(\"$\")[-1]\n",
    "        node_mode = node_name.split(\":\")[-1]\n",
    "        ebm = self.get_ebm(node_name)[node_mode].to(input.device)\n",
    "\n",
    "        # Get hyperparameters:\n",
    "        lambd_start = kwargs[\"lambd_start\"] if \"lambd_start\" in kwargs and kwargs[\"lambd_start\"] is not None else self.lambd_start if self.lambd_start is not None else 0.1\n",
    "        lambd = kwargs[\"lambd\"] if \"lambd\" in kwargs and kwargs[\"lambd\"] is not None else self.lambd if self.lambd is not None else 0.005\n",
    "        image_value_range = kwargs[\"image_value_range\"] if \"image_value_range\" in kwargs and kwargs[\"image_value_range\"] is not None else self.image_value_range if self.image_value_range is not None else \"0,1\"\n",
    "        step_size_start = kwargs[\"step_size_start\"] if \"step_size_start\" in kwargs else -1\n",
    "        step_size = kwargs[\"step_size\"] if \"step_size\" in kwargs and kwargs[\"step_size\"] is not None else self.step_size if self.step_size is not None else 20\n",
    "        step_size_img = kwargs[\"step_size_img\"] if \"step_size_img\" in kwargs and kwargs[\"step_size_img\"] is not None else self.step_size_img if self.step_size_img is not None else -1\n",
    "        step_size_zgnn = kwargs[\"step_size_zgnn\"] if \"step_size_zgnn\" in kwargs and kwargs[\"step_size_zgnn\"] is not None else self.step_size_zgnn if self.step_size_zgnn is not None else 2\n",
    "        step_size_wtarget = kwargs[\"step_size_wtarget\"] if \"step_size_wtarget\" in kwargs and kwargs[\"step_size_wtarget\"] is not None else self.step_size_wtarget if self.step_size_wtarget is not None else -1\n",
    "        SGLD_is_anneal = kwargs[\"SGLD_is_anneal\"] if \"SGLD_is_anneal\" in kwargs and kwargs[\"SGLD_is_anneal\"] is not None else self.SGLD_is_anneal if self.SGLD_is_anneal is not None else True\n",
    "        SGLD_is_penalize_lower = kwargs[\"SGLD_is_penalize_lower\"] if \"SGLD_is_penalize_lower\" in kwargs and kwargs[\"SGLD_is_penalize_lower\"] is not None else self.SGLD_is_penalize_lower if self.SGLD_is_penalize_lower is not None else True\n",
    "        SGLD_mutual_exclusive_coef = kwargs[\"SGLD_mutual_exclusive_coef\"] if \"SGLD_mutual_exclusive_coef\" in kwargs and kwargs[\"SGLD_mutual_exclusive_coef\"] is not None else self.SGLD_mutual_exclusive_coef if self.SGLD_mutual_exclusive_coef is not None else 0\n",
    "        SGLD_pixel_entropy_coef = kwargs[\"SGLD_pixel_entropy_coef\"] if \"SGLD_pixel_entropy_coef\" in kwargs and kwargs[\"SGLD_pixel_entropy_coef\"] is not None else self.SGLD_pixel_entropy_coef if self.SGLD_pixel_entropy_coef is not None else 0\n",
    "        SGLD_pixel_gm_coef = kwargs[\"SGLD_pixel_gm_coef\"] if \"SGLD_pixel_gm_coef\" in kwargs and kwargs[\"SGLD_pixel_gm_coef\"] is not None else self.SGLD_pixel_gm_coef if self.SGLD_pixel_gm_coef is not None else 0\n",
    "        SGLD_object_exceed_coef = kwargs[\"SGLD_object_exceed_coef\"] if \"SGLD_object_exceed_coef\" in kwargs else 0\n",
    "        # For selector discovery:\n",
    "        SGLD_iou_batch_consistency_coef = kwargs[\"SGLD_iou_batch_consistency_coef\"] if \"SGLD_iou_batch_consistency_coef\" in kwargs and kwargs[\"SGLD_iou_batch_consistency_coef\"] is not None else self.SGLD_iou_batch_consistency_coef if self.SGLD_iou_batch_consistency_coef is not None else 0\n",
    "        SGLD_iou_concept_repel_coef = kwargs[\"SGLD_iou_concept_repel_coef\"] if \"SGLD_iou_concept_repel_coef\" in kwargs and kwargs[\"SGLD_iou_concept_repel_coef\"] is not None else self.SGLD_iou_concept_repel_coef if self.SGLD_iou_concept_repel_coef is not None else 0\n",
    "        SGLD_iou_relation_repel_coef = kwargs[\"SGLD_iou_relation_repel_coef\"] if \"SGLD_iou_relation_repel_coef\" in kwargs and kwargs[\"SGLD_iou_relation_repel_coef\"] is not None else self.SGLD_iou_relation_repel_coef if self.SGLD_iou_relation_repel_coef is not None else 0\n",
    "        SGLD_iou_relation_overlap_coef = kwargs[\"SGLD_iou_relation_overlap_coef\"] if \"SGLD_iou_relation_overlap_coef\" in kwargs and kwargs[\"SGLD_iou_relation_overlap_coef\"] is not None else self.SGLD_iou_relation_overlap_coef if self.SGLD_iou_relation_overlap_coef is not None else 0\n",
    "        SGLD_iou_attract_coef = kwargs[\"SGLD_iou_attract_coef\"] if \"SGLD_iou_attract_coef\" in kwargs and kwargs[\"SGLD_iou_attract_coef\"] is not None else self.SGLD_iou_attract_coef if self.SGLD_iou_attract_coef is not None else 0\n",
    "\n",
    "        # Other settings:\n",
    "        sample_step = kwargs[\"sample_step\"] if \"sample_step\" in kwargs else 60\n",
    "        ensemble_size = kwargs[\"ensemble_size\"] if \"ensemble_size\" in kwargs else 1\n",
    "        w_type = kwargs[\"w_type\"] if \"w_type\" in kwargs else \"image+mask\"\n",
    "        w_init_type = kwargs[\"w_init_type\"] if \"w_init_type\" in kwargs and kwargs[\"w_init_type\"] is not None else self.w_init_type if self.w_init_type is not None else \"random\"\n",
    "        assert w_init_type in [\"random\", \"input\", \"input-mask\", \"input-gaus\"] or w_init_type.startswith(\"k-means\")\n",
    "\n",
    "        args = get_pdict()(\n",
    "            lambd_start=lambd_start,\n",
    "            lambd=lambd,\n",
    "            image_value_range=image_value_range,\n",
    "            step_size_start=step_size_start,\n",
    "            step_size=step_size,\n",
    "            step_size_img=step_size_img,\n",
    "            step_size_zgnn=step_size_zgnn,\n",
    "            step_size_wtarget=step_size_wtarget,\n",
    "            image_size=input.shape[-2:],\n",
    "            w_type=w_type,\n",
    "            SGLD_is_anneal=SGLD_is_anneal,\n",
    "            SGLD_is_penalize_lower=SGLD_is_penalize_lower,\n",
    "            SGLD_object_exceed_coef=SGLD_object_exceed_coef,\n",
    "            SGLD_mutual_exclusive_coef=SGLD_mutual_exclusive_coef,\n",
    "            SGLD_pixel_entropy_coef=SGLD_pixel_entropy_coef,\n",
    "            SGLD_pixel_gm_coef=SGLD_pixel_gm_coef,\n",
    "            # Selector:\n",
    "            SGLD_iou_batch_consistency_coef=SGLD_iou_batch_consistency_coef,\n",
    "            SGLD_iou_concept_repel_coef=SGLD_iou_concept_repel_coef,\n",
    "            SGLD_iou_relation_repel_coef=SGLD_iou_relation_repel_coef,\n",
    "            SGLD_iou_relation_overlap_coef=SGLD_iou_relation_overlap_coef,\n",
    "            SGLD_iou_attract_coef=SGLD_iou_attract_coef,\n",
    "        )\n",
    "        with torch.enable_grad():\n",
    "            (img_ensemble_sorted, neg_mask_ensemble_sorted, z_ensemble_sorted, _, _), neg_out_ensemble_sorted = ebm.ground(\n",
    "                input,\n",
    "                args=args,\n",
    "                ensemble_size=ensemble_size,\n",
    "                topk=-1,\n",
    "                w_init_type=w_init_type,\n",
    "                sample_step=60,\n",
    "                isplot=False,\n",
    "            )\n",
    "        # Using the w with the lowest energy in the ensemble:\n",
    "        # For now, repeat the mask channel 10 times to mimic the full object:\n",
    "        w_grounded = neg_mask_ensemble_sorted[0][:, 0]\n",
    "        return w_grounded\n",
    "\n",
    "\n",
    "    def forward_NN(\n",
    "        self,\n",
    "        input,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Performs the forward operation for the full selector with EBM.\n",
    "\n",
    "        Two scenarios:\n",
    "        (1) \"obj\" in w_type:\n",
    "            The 1st SGLD can obtain the w_op_dict\n",
    "\n",
    "        (2) \"mask\" in w_type and z_mode != \"None\":\n",
    "            The 1st SGLD obtain the mask and z, and\n",
    "            the 2nd SGLD reconstruct the img based on the inferred mask and z.\n",
    "\n",
    "        Returns:\n",
    "            pred: For (1), will simply be combination from w_op_dict.\n",
    "                    For (2), will be reconstruction based on the mask from 1st SGLD and the reconstructed img from the 2nd SGLD.\n",
    "                    The recons has the same shape as the input.\n",
    "            w_op_dict: will always be the mask/obj from the 1st SGLD\n",
    "        \"\"\"\n",
    "        def squeeze_batch(tensor, is_squeeze):\n",
    "            if is_squeeze:\n",
    "                tensor = tensor.squeeze(0)\n",
    "            return tensor\n",
    "\n",
    "        inp_hash = persist_hash(str(input))\n",
    "        if self.cache_forward and inp_hash in self.forward_cache:\n",
    "            return self.forward_cache[inp_hash]\n",
    "\n",
    "        # If there is an additional task-batch dimension, combine that with the example-batch:\n",
    "        if len(input.shape) == 5:\n",
    "            batch_dims = input.shape[:2]\n",
    "            batch_shape = tuple(batch_dims)\n",
    "        else:\n",
    "            assert len(input.shape) == 4\n",
    "            batch_dims = input.shape[:1]\n",
    "            batch_shape = None\n",
    "        input = input.view(-1, *input.shape[-3:])\n",
    "\n",
    "        # Setting up the hyperparameters:\n",
    "        lambd_start = kwargs[\"lambd_start\"] if \"lambd_start\" in kwargs and kwargs[\"lambd_start\"] is not None else self.lambd_start if self.lambd_start is not None else 0.1\n",
    "        lambd = kwargs[\"lambd\"] if \"lambd\" in kwargs and kwargs[\"lambd\"] is not None else self.lambd if self.lambd is not None else 0.005\n",
    "        image_value_range = kwargs[\"image_value_range\"] if \"image_value_range\" in kwargs and kwargs[\"image_value_range\"] is not None else self.image_value_range if self.image_value_range is not None else \"0,1\"\n",
    "        SGLD_is_anneal = kwargs[\"SGLD_is_anneal\"] if \"SGLD_is_anneal\" in kwargs and kwargs[\"SGLD_is_anneal\"] is not None else self.SGLD_is_anneal if self.SGLD_is_anneal is not None else True\n",
    "        SGLD_is_penalize_lower = kwargs[\"SGLD_is_penalize_lower\"] if \"SGLD_is_penalize_lower\" in kwargs and kwargs[\"SGLD_is_penalize_lower\"] is not None else self.SGLD_is_penalize_lower if self.SGLD_is_penalize_lower is not None else True\n",
    "        SGLD_mutual_exclusive_coef = kwargs[\"SGLD_mutual_exclusive_coef\"] if \"SGLD_mutual_exclusive_coef\" in kwargs and kwargs[\"SGLD_mutual_exclusive_coef\"] is not None else self.SGLD_mutual_exclusive_coef if self.SGLD_mutual_exclusive_coef is not None else 0\n",
    "        SGLD_pixel_entropy_coef = kwargs[\"SGLD_pixel_entropy_coef\"] if \"SGLD_pixel_entropy_coef\" in kwargs and kwargs[\"SGLD_pixel_entropy_coef\"] is not None else self.SGLD_pixel_entropy_coef if self.SGLD_pixel_entropy_coef is not None else 0\n",
    "        SGLD_pixel_gm_coef = kwargs[\"SGLD_pixel_gm_coef\"] if \"SGLD_pixel_gm_coef\" in kwargs and kwargs[\"SGLD_pixel_gm_coef\"] is not None else self.SGLD_pixel_gm_coef if self.SGLD_pixel_gm_coef is not None else 0\n",
    "        SGLD_object_exceed_coef = kwargs[\"SGLD_object_exceed_coef\"] if \"SGLD_object_exceed_coef\" in kwargs else 0\n",
    "        # For selector discovery:\n",
    "        SGLD_iou_batch_consistency_coef = kwargs[\"SGLD_iou_batch_consistency_coef\"] if \"SGLD_iou_batch_consistency_coef\" in kwargs and kwargs[\"SGLD_iou_batch_consistency_coef\"] is not None else self.SGLD_iou_batch_consistency_coef if self.SGLD_iou_batch_consistency_coef is not None else 0\n",
    "        SGLD_iou_concept_repel_coef = kwargs[\"SGLD_iou_concept_repel_coef\"] if \"SGLD_iou_concept_repel_coef\" in kwargs and kwargs[\"SGLD_iou_concept_repel_coef\"] is not None else self.SGLD_iou_concept_repel_coef if self.SGLD_iou_concept_repel_coef is not None else 0\n",
    "        SGLD_iou_relation_repel_coef = kwargs[\"SGLD_iou_relation_repel_coef\"] if \"SGLD_iou_relation_repel_coef\" in kwargs and kwargs[\"SGLD_iou_relation_repel_coef\"] is not None else self.SGLD_iou_relation_repel_coef if self.SGLD_iou_relation_repel_coef is not None else 0\n",
    "        SGLD_iou_relation_overlap_coef = kwargs[\"SGLD_iou_relation_overlap_coef\"] if \"SGLD_iou_relation_overlap_coef\" in kwargs and kwargs[\"SGLD_iou_relation_overlap_coef\"] is not None else self.SGLD_iou_relation_overlap_coef if self.SGLD_iou_relation_overlap_coef is not None else 0\n",
    "        SGLD_iou_attract_coef = kwargs[\"SGLD_iou_attract_coef\"] if \"SGLD_iou_attract_coef\" in kwargs and kwargs[\"SGLD_iou_attract_coef\"] is not None else self.SGLD_iou_attract_coef if self.SGLD_iou_attract_coef is not None else 0\n",
    "        # Other settings:\n",
    "        step_size_start = kwargs[\"step_size_start\"] if \"step_size_start\" in kwargs else -1\n",
    "        step_size = kwargs[\"step_size\"] if \"step_size\" in kwargs and kwargs[\"step_size\"] is not None else self.step_size if self.step_size is not None else 20\n",
    "        step_size_img = kwargs[\"step_size_img\"] if \"step_size_img\" in kwargs and kwargs[\"step_size_img\"] is not None else self.step_size_img if self.step_size_img is not None else -1\n",
    "        step_size_z = kwargs[\"step_size_z\"] if \"step_size_z\" in kwargs and kwargs[\"step_size_z\"] is not None else self.step_size_z if self.step_size_z is not None else 2\n",
    "        step_size_zgnn = kwargs[\"step_size_zgnn\"] if \"step_size_zgnn\" in kwargs and kwargs[\"step_size_zgnn\"] is not None else self.step_size_zgnn if self.step_size_zgnn is not None else 2\n",
    "        step_size_wtarget = kwargs[\"step_size_wtarget\"] if \"step_size_wtarget\" in kwargs and kwargs[\"step_size_wtarget\"] is not None else self.step_size_wtarget if self.step_size_wtarget is not None else -1\n",
    "        sample_step = kwargs[\"sample_step\"] if \"sample_step\" in kwargs else 60\n",
    "        ensemble_size = kwargs[\"ensemble_size\"] if \"ensemble_size\" in kwargs else 1\n",
    "        kl_all_step = kwargs[\"kl_all_step\"] if \"kl_all_step\" in kwargs else True\n",
    "        is_grad = kwargs[\"is_grad\"] if \"is_grad\" in kwargs else False\n",
    "        w_init_type = kwargs[\"w_init_type\"] if \"w_init_type\" in kwargs and kwargs[\"w_init_type\"] is not None else self.w_init_type if self.w_init_type is not None else \"random\"\n",
    "        indiv_sample = kwargs[\"indiv_sample\"] if \"indiv_sample\" in kwargs and kwargs[\"indiv_sample\"] is not None else self.indiv_sample if self.indiv_sample is not None else -1\n",
    "        img_init_type = kwargs[\"img_init_type\"] if \"img_init_type\" in kwargs else \"random\"\n",
    "        is_return_E = kwargs[\"is_return_E\"] if \"is_return_E\" in kwargs else False\n",
    "        isplot = kwargs[\"isplot\"] if \"isplot\" in kwargs else 0\n",
    "        verbose = kwargs[\"verbose\"] if \"verbose\" in kwargs else 0\n",
    "\n",
    "        args = get_pdict()(\n",
    "            lambd_start=lambd_start,\n",
    "            lambd=lambd,\n",
    "            image_value_range=image_value_range,\n",
    "            step_size_start=step_size_start,\n",
    "            step_size=step_size,\n",
    "            step_size_img=step_size_img,\n",
    "            step_size_z=step_size_z,\n",
    "            step_size_zgnn=step_size_zgnn,\n",
    "            step_size_wtarget=step_size_wtarget,\n",
    "            image_size=input.shape[-2:],\n",
    "            kl_all_step=kl_all_step,\n",
    "            SGLD_is_anneal=SGLD_is_anneal,\n",
    "            SGLD_is_penalize_lower=SGLD_is_penalize_lower,\n",
    "            SGLD_object_exceed_coef=SGLD_object_exceed_coef,\n",
    "            SGLD_mutual_exclusive_coef=SGLD_mutual_exclusive_coef,\n",
    "            SGLD_pixel_entropy_coef=SGLD_pixel_entropy_coef,\n",
    "            SGLD_pixel_gm_coef=SGLD_pixel_gm_coef,\n",
    "            # Selector:\n",
    "            SGLD_iou_batch_consistency_coef=SGLD_iou_batch_consistency_coef,\n",
    "            SGLD_iou_concept_repel_coef=SGLD_iou_concept_repel_coef,\n",
    "            SGLD_iou_relation_repel_coef=SGLD_iou_relation_repel_coef,\n",
    "            SGLD_iou_relation_overlap_coef=SGLD_iou_relation_overlap_coef,\n",
    "            SGLD_iou_attract_coef=SGLD_iou_attract_coef,\n",
    "        )\n",
    "\n",
    "        ebm_0 = self.ebm_dict[next(iter(self.ebm_dict))]\n",
    "        if w_init_type in [\"random\", \"input-mask\", \"input-gaus\"] or w_init_type.startswith(\"k-means\"):\n",
    "            mask = None\n",
    "        elif w_init_type == \"input\":\n",
    "            mask_arity = len(self.nodes)\n",
    "            if \"obj\" in ebm_0.w_type:\n",
    "                assert \"mask\" not in ebm_0.w_type\n",
    "                mask = tuple(input.detach().clone().repeat_interleave(ensemble_size, dim=0) for k in range(mask_arity))\n",
    "            elif \"mask\" in ebm_0.w_type:\n",
    "                assert \"obj\" not in ebm_0.w_type\n",
    "                if input.shape[1] == 10:\n",
    "                    mask = tuple((input[:,:1]!=1).detach().clone().repeat_interleave(ensemble_size, dim=0) for k in range(mask_arity))\n",
    "                elif input.shape[1] == 3:\n",
    "                    mask = None\n",
    "                else:\n",
    "                    raise\n",
    "            else:\n",
    "                raise\n",
    "            for k in range(mask_arity):\n",
    "                if mask is not None:\n",
    "                    mask[k].requires_grad = True\n",
    "        else:\n",
    "            raise Exception(\"w_init_type '{}' is not valid!\".format(w_init_type))\n",
    "\n",
    "        is_reconstruct = \"mask\" in ebm_0.w_type and ebm_0.z_mode != \"None\"\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            args.ebm_target = \"mask\" if ebm_0.z_mode == \"None\" else \"mask+z\"\n",
    "            if self.is_selector_gnn:\n",
    "                args.ebm_target += \"+zgnn\"\n",
    "            # SGLD w.r.t. w (and z):\n",
    "            (_, neg_mask_ensemble_sorted, z_ensemble_sorted, zgnn_ensemble_sorted, _), neg_out_ensemble_sorted, info = self.ground(\n",
    "                input,\n",
    "                args,\n",
    "                mask=mask,\n",
    "                ensemble_size=ensemble_size,\n",
    "                topk=1,\n",
    "                w_init_type=w_init_type,\n",
    "                sample_step=sample_step,\n",
    "                is_grad=is_grad,\n",
    "                is_return_E=is_return_E,\n",
    "                batch_shape=batch_shape,\n",
    "                isplot=isplot,\n",
    "            )\n",
    "            if is_return_E:\n",
    "                E_all = info[\"E_all\"]\n",
    "            \n",
    "            if verbose >= 1 and self.is_selector_gnn:\n",
    "                if hasattr(self.gnn, \"softmax_coef\"):\n",
    "                    print(\"softmax_coef:\")\n",
    "                    print(self.gnn.softmax_coef)\n",
    "                print(list(self.gnn.parameters())[:4])\n",
    "                print(\"z_node:\")\n",
    "                if zgnn_ensemble_sorted[0] is None:\n",
    "                    print(None)\n",
    "                else:\n",
    "                    print(zgnn_ensemble_sorted[0].squeeze())\n",
    "                print(\"z_edge:\")\n",
    "                print(zgnn_ensemble_sorted[1].squeeze())\n",
    "                print()\n",
    "\n",
    "            # Obtain the w_selected (list of w selected by the refer nodes) and a dict of all ws:\n",
    "            is_squeeze = True if len(input.shape) == 3 else False\n",
    "            w_op_dict = {}\n",
    "            z_op_dict = {}\n",
    "            w_selector = []\n",
    "            nodes = list(self.nodes)\n",
    "            for i, neg_mask_ele in enumerate(neg_mask_ensemble_sorted):\n",
    "                neg_mask_top = squeeze_batch(neg_mask_ele[:,0], is_squeeze)\n",
    "                if self.refer_node_names is None or nodes[i] in self.refer_node_names:\n",
    "                    # Accumulate the objects specified by the self.refer_node_names:\n",
    "                    w_selector.append(neg_mask_top)\n",
    "                w_op_dict[nodes[i]] = neg_mask_top\n",
    "                if indiv_sample != -1:\n",
    "                    z_op_dict[nodes[i]] = z_ensemble_sorted[i][:,0]\n",
    "\n",
    "            if not is_reconstruct:\n",
    "                # If not is_reconstruct, then the pred is certain combination of the w_op_dict from the first SGLD:\n",
    "                assert len(w_selector[0].shape) == 4\n",
    "#                 assert \"obj\" in ebm_0.w_type\n",
    "                if w_selector[0].shape[-3] == 10:\n",
    "                    # Obj with 10 color channels (first is empty channel):\n",
    "                    w_selector_stack = torch.stack(w_selector)  # [n_obj, B, 10, H, W]\n",
    "                    pred = torch.cat([\n",
    "                        w_selector_stack[:,:,:1].mean(0),\n",
    "                        w_selector_stack[:,:,1:].sum(0),\n",
    "                    ], 1)\n",
    "                elif w_selector[0].shape[-3] == 3:\n",
    "                    # Obj with RGB channels:\n",
    "                    pred = torch.stack(w_selector).sum(0)\n",
    "                elif w_selector[0].shape[-3] == 1:\n",
    "                    pred = None\n",
    "                else:\n",
    "                    raise\n",
    "            else:\n",
    "                # If is_reconstruct, perform a second SGLD w.r.t. img given the inferred w (and z):\n",
    "                if img_init_type == \"random\":\n",
    "                    img_value_min, img_value_max = image_value_range.split(\",\")\n",
    "                    img_value_min, img_value_max = eval(img_value_min), eval(img_value_max)\n",
    "                    img_init = torch.rand(input.shape, device=input.device) * (img_value_max - img_value_min) + img_value_min\n",
    "                elif img_init_type == \"input\":\n",
    "                    img_init = input.detach()\n",
    "                else:\n",
    "                    raise\n",
    "\n",
    "                # Specify the mask according to the refer nodes:\n",
    "                neg_mask_latent = []\n",
    "                mask_info = self.get_mask_info()\n",
    "                for i, neg_mask_ele in enumerate(neg_mask_ensemble_sorted):\n",
    "                    if self.refer_node_names is None or nodes[i] in self.refer_node_names:\n",
    "                        if mask_info[\"id_to_type\"][i][0] == \"relation\" and self.is_relation_z is False:\n",
    "                            neg_mask_latent.append(None)\n",
    "                        else:\n",
    "                            neg_mask_latent.append(neg_mask_ele[:,0])\n",
    "                    else:\n",
    "                        # If not refer nodes, then turn off the mask:\n",
    "                        neg_mask_latent.append(torch.zeros(neg_mask_ele[:,0].shape).to(device))\n",
    "                # 2nd SGLD w.r.t. img, given mask and z:\n",
    "                z_latent = tuple(ele[:,0] if ele is not None else None for ele in z_ensemble_sorted) if \"z\" in args.ebm_target else None\n",
    "                zgnn_latent = tuple(zgnn_ele[:,0] if zgnn_ele is not None else None for zgnn_ele in zgnn_ensemble_sorted) if zgnn_ensemble_sorted is not None else None\n",
    "                args.ebm_target = \"image\"\n",
    "                if indiv_sample != -1:\n",
    "                    # Go through each EBM\n",
    "                    pred = None\n",
    "                    ebm_dict = self.get_ebms()\n",
    "                    for key in w_op_dict.keys():\n",
    "                        model = list(ebm_dict[key].values())[0]\n",
    "                        (img_ensemble_sorted, _, _, _, _), neg_out_ensemble_sorted = model.ground(\n",
    "                            img_init,\n",
    "                            args,\n",
    "                            mask=tuple([w_op_dict[key]]),\n",
    "                            z=tuple([z_op_dict[key]]),\n",
    "                            zgnn=zgnn_latent,\n",
    "                            ensemble_size=ensemble_size,\n",
    "                            topk=1,\n",
    "                            sample_step=indiv_sample,\n",
    "                            is_grad=is_grad,\n",
    "                            batch_shape=batch_shape,\n",
    "                            isplot=isplot,\n",
    "                        )\n",
    "                        if pred is None:\n",
    "                            pred = img_ensemble_sorted[:, 0] * w_op_dict[key]\n",
    "                        else:\n",
    "                            pred = pred + img_ensemble_sorted[:, 0] * w_op_dict[key]\n",
    "                    img_value_min, img_value_max = args.image_value_range.split(\",\")\n",
    "                    img_value_min, img_value_max = eval(img_value_min), eval(img_value_max)\n",
    "                    pred = pred.clamp(min=img_value_min, max=img_value_max)\n",
    "                else:\n",
    "                    (img_ensemble_sorted, _, _, _, _), neg_out_ensemble_sorted, info = self.ground(\n",
    "                        img_init,\n",
    "                        args,\n",
    "                        mask=neg_mask_latent,\n",
    "                        z=z_latent,\n",
    "                        zgnn=zgnn_latent,\n",
    "                        ensemble_size=ensemble_size,\n",
    "                        topk=1,\n",
    "                        sample_step=sample_step,\n",
    "                        is_grad=is_grad,\n",
    "                        batch_shape=batch_shape,\n",
    "                        isplot=isplot,\n",
    "                    )\n",
    "                    pred = img_ensemble_sorted[:,0]\n",
    "        if pred is not None:\n",
    "            assert pred.shape == input.shape\n",
    "            # Recover the task-batch dimension:\n",
    "            pred = pred.view(*batch_dims, *pred.shape[-3:])\n",
    "        w_op_dict = {key: item.view(*batch_dims, *item.shape[-3:]) for key, item in w_op_dict.items()}\n",
    "\n",
    "        # Store the results in a cache, to be used by the policy:\n",
    "        if self.cache_forward:\n",
    "            # Input should be a batch of examples\n",
    "            self.forward_cache[inp_hash] = (pred, w_op_dict)\n",
    "        if is_return_E:\n",
    "            return pred, w_op_dict, E_all\n",
    "        else:\n",
    "            return pred, w_op_dict\n",
    "\n",
    "\n",
    "    def ground(\n",
    "        self,\n",
    "        input,\n",
    "        args,\n",
    "        mask=None,\n",
    "        c_repr=None,\n",
    "        z=None,\n",
    "        zgnn=None,\n",
    "        wtarget=None,\n",
    "        ensemble_size=18,\n",
    "        topk=-1,\n",
    "        w_init_type=\"random\",\n",
    "        sample_step=150,\n",
    "        ground_truth_mask=None,\n",
    "        is_grad=False,\n",
    "        is_return_E=False,\n",
    "        batch_shape=None,\n",
    "        isplot=2,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Ground the input with the selector itself and return the discovered objects (masks).\n",
    "        \"\"\"\n",
    "        def init_neg_mask(input, init, ensemble_size, mask_arity, w_type):\n",
    "            \"\"\"Initialize negative mask\"\"\"\n",
    "            device = input.device\n",
    "            w_dim = input.shape[1] if \"obj\" in w_type else 1\n",
    "            neg_mask = tuple(torch.rand(input.shape[0]*ensemble_size, w_dim, *input.shape[2:]).to(device) for k in range(mask_arity))\n",
    "            input_l = repeat_n(input, n_repeats=ensemble_size)\n",
    "            if init == \"input-mask\":\n",
    "                assert input.shape[1] == 10\n",
    "                neg_mask = tuple(neg_mask[k] * (input_l.argmax(1)[:, None] != 0) for k in range(mask_arity))\n",
    "            elif init == \"input-gaus\":\n",
    "                means = input_l.argmax(1)[:, None].float()\n",
    "                std = 0.01 * torch.ones_like(means, device=device)\n",
    "                neg_mask = tuple(neg_mask[k] * torch.normal(means, std).clamp(min=0, max=1) for k in range(mask_arity))\n",
    "            elif init.startswith(\"k-means\"):\n",
    "                parts = init.split(\"^\")\n",
    "                num_clusters = mask_arity if len(parts) < 2 else eval(parts[1])\n",
    "                input_l_dim = input_l.shape\n",
    "                # Flatten H, W then permute to get [batch_size, n_pixels, n_channels]. Flatten to\n",
    "                # [batch_size * n_pixels, n_channels]\n",
    "                input_flat = input_l.flatten(2).permute(0, 2, 1).flatten(end_dim=1)\n",
    "                cluster_ids, cluster_centers = kmeans(\n",
    "                    X=input_flat, num_clusters=num_clusters, distance='euclidean', device=device, tqdm_flag=False,\n",
    "                )\n",
    "                # Get the most common cluster\n",
    "                background_cluster, _ = cluster_ids.mode(0)\n",
    "                all_ex_mask = (cluster_ids != background_cluster).reshape(input_l_dim[0], *input_l_dim[2:]).unsqueeze(1).to(device)\n",
    "                neg_mask = tuple(neg_mask[k] * all_ex_mask for k in range(mask_arity))\n",
    "            for k in range(mask_arity):\n",
    "                neg_mask[k].requires_grad = True\n",
    "            return neg_mask\n",
    "\n",
    "        def plot_discovered_mask_summary(num_examples: int, neg_mask_ensemble_sorted: torch.Tensor, should_quantize: bool):\n",
    "            plt.figure(figsize=(18,3))\n",
    "            for batch_idx in range(len(neg_mask_ensemble_sorted[0])):\n",
    "                for ex in range(num_examples):\n",
    "                    for mask_idx in range(len(neg_mask_ensemble_sorted)):\n",
    "                        ax = plt.subplot(1, num_examples, ex + 1)\n",
    "                        # Pull single-channel (0)\n",
    "                        mask = to_np_array(neg_mask_ensemble_sorted[mask_idx][batch_idx][ex][0])\n",
    "                        image = np.zeros((*mask.shape, 4)) # (H, W, C, alpha)\n",
    "                        color = np.asarray(matplotlib.colors.to_rgb(COLOR_LIST[mask_idx]))\n",
    "                        for h in range(mask.shape[0]):\n",
    "                            for w in range(mask.shape[1]):\n",
    "                                opacity = mask[h][w].round() if should_quantize else mask[h][w]\n",
    "                                pixel = opacity * np.asarray((*color, 0.5)) # add alpha channel\n",
    "                                image[h][w] = pixel\n",
    "                        plt.imshow(image)\n",
    "                        ax.set_title(\"E: {:.5f}\\n\".format(neg_out_ensemble_sorted[batch_idx][ex]))\n",
    "            plt.show()\n",
    "\n",
    "        from reasoning.experiments.models import neg_mask_sgd_ensemble\n",
    "        ebm_0 = self.ebm_dict[next(iter(self.ebm_dict))]\n",
    "        z_mode = ebm_0.z_mode\n",
    "\n",
    "        # Update args:\n",
    "        args = deepcopy(args)\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(args, key, value)\n",
    "        args.sample_step = sample_step\n",
    "        args.is_two_branch = True if ebm_0.mode in [\"operator\"] else False\n",
    "\n",
    "        assert (not isinstance(input, tuple)) and (not isinstance(input, list))\n",
    "        if input is not None:\n",
    "            if len(input.shape) == 3:\n",
    "                input = input[None]\n",
    "            assert len(input.shape) == 4\n",
    "            device = input.device\n",
    "            args.is_image_tuple = (isinstance(input, tuple) or isinstance(input, list)) and len(input) == 2\n",
    "        else:\n",
    "            device = mask[0].device\n",
    "            args.is_image_tuple = False\n",
    "\n",
    "        if ground_truth_mask is not None:\n",
    "            print(\"Ground truth mask energies:\")\n",
    "            energy = self.forward(input, ground_truth_mask, c_repr)\n",
    "            # Plot all masks together, followed by each individual mask\n",
    "            visualize_matrices(torch.cat([sum(ground_truth_mask), *ground_truth_mask], dim=0).squeeze(1),\n",
    "                               images_per_row=len(ground_truth_mask) + 1,\n",
    "                               subtitles=[\n",
    "                                   # Print overall energy\n",
    "                                   \"E: {:.5f}\\n\".format(float(energy)) + \\\n",
    "                                    # Print energy of individual components\n",
    "                                    \"\\n\".join([\"{}: {:.5f}\".format(key, float(self.info[key])) for key in self.info])] + [\"\"] * len(ground_truth_mask))\n",
    "\n",
    "\n",
    "        # Initialization:\n",
    "        if input is not None:\n",
    "            if mask is None:\n",
    "                neg_mask = init_neg_mask(input, init=w_init_type, ensemble_size=ensemble_size, mask_arity=self.mask_arity, w_type=ebm_0.w_type)\n",
    "            else:\n",
    "                neg_mask = tuple(repeat_n(mask[k], n_repeats=ensemble_size) for k in range(len(mask)))\n",
    "            if z_mode != \"None\":\n",
    "                z_mode_dict = self.get_z_mode_dict()\n",
    "                z_tuple = []\n",
    "                for k, (ebm_key, z_mode_ele) in enumerate(z_mode_dict.items()):\n",
    "                    if z_mode_ele == \"None\":\n",
    "                        z_tuple.append(None)\n",
    "                    else:\n",
    "                        if z is None:\n",
    "                            z_tuple.append(torch.rand(neg_mask[0].shape[0], ebm_0.z_dim, device=device))\n",
    "                        else:\n",
    "                            z_tuple.append(repeat_n(z[k], n_repeats=ensemble_size))\n",
    "                z = tuple(z_tuple)\n",
    "            else:\n",
    "                z = None\n",
    "        else:\n",
    "            neg_mask = tuple(repeat_n(mask[k], n_repeats=ensemble_size) for k in range(len(mask)))\n",
    "        # Perform SGLD:\n",
    "        (img_ensemble, neg_mask_ensemble, z_ensemble, zgnn_ensemble, wtarget_ensemble), neg_out_list_ensemble, info_ensemble = neg_mask_sgd_ensemble(\n",
    "            self, input, neg_mask, c_repr, z=z, zgnn=zgnn, wtarget=wtarget,\n",
    "            args=args,\n",
    "            ensemble_size=ensemble_size,\n",
    "            mask_info = self.get_mask_info(),\n",
    "            is_grad=is_grad,\n",
    "            is_return_E=is_return_E,\n",
    "            batch_shape=batch_shape,\n",
    "        )  # neg_mask_ensemble: [ensemble_size, B, C, H, W]; neg_out_list_ensemble: [sample_step, ensemble_size, B]\n",
    "        neg_out_ensemble = neg_out_list_ensemble[-1]  # neg_out_ensemble: [ensemble_size, B]\n",
    "\n",
    "        topk_core = ensemble_size if topk==-1 else min(ensemble_size, topk)\n",
    "        # Sort the obtained results by energy for each example:\n",
    "        neg_out_ensemble = torch.FloatTensor(neg_out_ensemble).transpose(0,1)  # neg_out_ensemble (new): [B, ensemble_size]\n",
    "        neg_out_argsort = neg_out_ensemble.argsort(1)  # [B, ensemble_size]\n",
    "        batch_size = neg_out_argsort.shape[0]\n",
    "        neg_out_ensemble_sorted = torch.stack([neg_out_ensemble[i][neg_out_argsort[i]][:topk_core] for i in range(batch_size)])  # [B, ensemble_size]\n",
    "        if zgnn_ensemble is not None or wtarget is not None:\n",
    "            neg_task_out_ensemble = neg_out_ensemble.reshape(*batch_shape, -1).mean(1)  # [B_task, ensemble_size]\n",
    "            neg_task_out_argsort = neg_task_out_ensemble.argsort(1)\n",
    "\n",
    "        if img_ensemble is not None:\n",
    "            if args.is_image_tuple:\n",
    "                img_ensemble = tuple(img_ensemble[k].transpose(0,1) for k in range(len(img)))  # Each element [B, ensemble_size, C, H, W]\n",
    "                img_ensemble_sorted = []\n",
    "                for k in range(len(img)):\n",
    "                    img_ensemble_sorted.append(torch.stack([img_ensemble[k][i][neg_out_argsort[i]][:topk_core] for i in range(batch_size)]))\n",
    "                img_ensemble_sorted = tuple(img_ensemble_sorted)  # each element: [B, ensemble_size, C, H, W] sorted along dim=1 according to neg_out\n",
    "            else:\n",
    "                img_ensemble = img_ensemble.transpose(0,1)  # [B, ensemble_size, C, H, W]\n",
    "                img_ensemble_sorted = torch.stack([img_ensemble[i][neg_out_argsort[i]][:topk_core] for i in range(batch_size)])\n",
    "        else:\n",
    "            img_ensemble_sorted = None\n",
    "\n",
    "        if neg_mask_ensemble is not None:\n",
    "            neg_mask_ensemble = tuple(neg_mask_ensemble[k].transpose(0,1) for k in range(self.mask_arity))  # Each element [B, ensemble_size, C, H, W]\n",
    "            neg_mask_ensemble_sorted = []\n",
    "            for k in range(self.mask_arity):\n",
    "                neg_mask_ensemble_sorted.append(torch.stack([neg_mask_ensemble[k][i][neg_out_argsort[i]][:topk_core] for i in range(len(neg_mask_ensemble[0]))]))\n",
    "            neg_mask_ensemble_sorted = tuple(neg_mask_ensemble_sorted)  # each element: [B, ensemble_size, C, H, W] sorted along dim=1 according to neg_out\n",
    "        else:\n",
    "            neg_mask_ensemble_sorted = None\n",
    "\n",
    "        if z_ensemble is not None:\n",
    "            z_ensemble = tuple(z_ensemble[k].transpose(0,1) if z_ensemble[k] is not None else None for k in range(len(z_ensemble)))  # Each element [B, ensemble_size, Z]\n",
    "            z_ensemble_sorted = []\n",
    "            for k in range(len(z_ensemble)):\n",
    "                if z_ensemble[k] is not None:\n",
    "                    z_ensemble_sorted.append(torch.stack([z_ensemble[k][i][neg_out_argsort[i]][:topk_core] for i in range(batch_size)]))\n",
    "                else:\n",
    "                    z_ensemble_sorted.append(None)\n",
    "            z_ensemble_sorted = tuple(z_ensemble_sorted)  # each element: [B, ensemble_size, Z] sorted along dim=1 according to neg_out\n",
    "        else:\n",
    "            z_ensemble_sorted = None\n",
    "\n",
    "        if zgnn_ensemble is not None:\n",
    "            zgnn_ensemble = tuple(zgnn_ensemble[k].transpose(0,1) if zgnn_ensemble[k] is not None else None for k in range(len(zgnn_ensemble)))  # Each element [B, ensemble_size, Zgnn]\n",
    "            zgnn_ensemble_sorted = []\n",
    "            for k in range(len(zgnn_ensemble)):\n",
    "                if zgnn_ensemble[k] is not None:\n",
    "                    zgnn_ensemble_sorted.append(torch.stack([zgnn_ensemble[k][i][neg_task_out_argsort[i]][:topk_core] for i in range(batch_shape[0])]))\n",
    "                else:\n",
    "                    zgnn_ensemble_sorted.append(None)\n",
    "            zgnn_ensemble_sorted = tuple(zgnn_ensemble_sorted)  # each element: [B, ensemble_size, zgnn_dim] sorted along dim=1 according to neg_out\n",
    "        else:\n",
    "            zgnn_ensemble_sorted = None\n",
    "\n",
    "        if wtarget_ensemble is not None:\n",
    "            wtarget_ensemble = wtarget_ensemble.transpose(0,1)  # [B, ensemble_size, w_dim, H, W]\n",
    "            wtarget_ensemble_sorted = torch.stack([wtarget_ensemble[i][neg_out_argsort[i]][:topk_core] for i in range(batch_size)])\n",
    "        else:\n",
    "            wtarget_ensemble_sorted = None\n",
    "\n",
    "        # Obtain each individual energy for component models:\n",
    "        info = {}\n",
    "        if is_return_E:\n",
    "            info[\"E_all\"] = info_ensemble.pop(\"E_all\")\n",
    "            self.info.pop(\"E_all\", None)\n",
    "        neg_out_argsort = to_np_array(neg_out_argsort)\n",
    "        for key, value in self.info.items():\n",
    "            value_reshape = value.reshape(ensemble_size, -1).T  # [B, ensemble_size]\n",
    "            info[key] = []\n",
    "            for i in range(len(value_reshape)):\n",
    "                info[key].append(value_reshape[i][neg_out_argsort[i]][:topk_core])\n",
    "            info[key] = np.stack(info[key])\n",
    "\n",
    "        NUM_PREV_EXAMPLES = min(6, ensemble_size)\n",
    "\n",
    "        if isplot >= 2:\n",
    "            # Plot SGLD learning curve:\n",
    "            print(\"SGLD learning curve:\")\n",
    "            plt.figure(figsize=(12,6))\n",
    "            for i in range(min(neg_out_list_ensemble.shape[-1], 6)):  # neg_out_list_ensemble: [sample_step, ensemble_size, B]\n",
    "#                 print(\"Example {}\".format(i))\n",
    "                for k in range(min(6, ensemble_size)):\n",
    "                    plt.plot(neg_out_list_ensemble[:, neg_out_argsort[i][k],i], c=COLOR_LIST[k], label=\"id_{}\".format(k), alpha=0.4)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        if isplot >= 3 and \"mask\" in args.ebm_target:\n",
    "            import matplotlib\n",
    "            w_type = ebm_0.w_type\n",
    "            # Show original input image for reference\n",
    "            print(\"Original inputs:\")\n",
    "            for i in range(len(neg_mask_ensemble_sorted)):\n",
    "                visualize_matrices(input[i:i+1].argmax(1).repeat_interleave(NUM_PREV_EXAMPLES, 0))\n",
    "\n",
    "            # Plot a summary plot, superimposing different masks such that each mask has a different color\n",
    "            if \"mask\" in w_type:\n",
    "                print(f\"Top-{NUM_PREV_EXAMPLES} lowest-energy mask sets, all plotted together\")\n",
    "                print(f\"Key parameters: SGLD_mutual_exclusive_coef={str(args.SGLD_mutual_exclusive_coef)}\",\n",
    "                        f\"object_exceed_coef={str(args.SGLD_object_exceed_coef)}\")\n",
    "                plot_discovered_mask_summary(NUM_PREV_EXAMPLES, neg_mask_ensemble_sorted, should_quantize=False)\n",
    "\n",
    "                # Plot the same plot, but this time quantized so there are no color gradations\n",
    "                print(\"Quantized plot:\")\n",
    "                plot_discovered_mask_summary(NUM_PREV_EXAMPLES, neg_mask_ensemble_sorted, should_quantize=True)\n",
    "\n",
    "            # For each batch element\n",
    "            for i in range(len(neg_mask_ensemble_sorted[0])):\n",
    "                print(\"Example {}\".format(i))\n",
    "                # Loop through each mask (show them horizontally)\n",
    "                for k in range(len(neg_mask_ensemble_sorted)):\n",
    "                    if \"mask\" in w_type:\n",
    "                        img_to_plot = torch.round(neg_mask_ensemble_sorted[k][i][:NUM_PREV_EXAMPLES].squeeze(1))\n",
    "                    elif \"obj\" in w_type:\n",
    "                        img_to_plot = neg_mask_ensemble_sorted[k][i][:NUM_PREV_EXAMPLES].argmax(1)\n",
    "                    else:\n",
    "                        raise\n",
    "                    visualize_matrices(\n",
    "                        img_to_plot, images_per_row=NUM_PREV_EXAMPLES,\n",
    "                        subtitles=[\"E: {:.5f}\\n\".format(neg_out_ensemble_sorted[i][j]) + \"\\n\".join([\"{}: {:.5f}\".format(key, info[key][i][j]) for key in info]) for j in range(NUM_PREV_EXAMPLES)] if k == 0 else None\n",
    "                    )\n",
    "        return (img_ensemble_sorted, neg_mask_ensemble_sorted, z_ensemble_sorted, zgnn_ensemble_sorted, wtarget_ensemble_sorted), neg_out_ensemble_sorted, info\n",
    "\n",
    "\n",
    "    def forward_NN_relation(\n",
    "        self,\n",
    "        train_input,\n",
    "        train_target,\n",
    "        test_input,\n",
    "        **kwargs\n",
    "    ):\n",
    "        def get_compat_ids(mask_info):\n",
    "            def get_triu_ids(array, is_triu=True):\n",
    "                if isinstance(array, Number):\n",
    "                    array = np.arange(array)\n",
    "                rows_matrix, col_matrix = np.meshgrid(array, array)\n",
    "                matrix_cat = np.stack([rows_matrix, col_matrix], -1)\n",
    "                rr, cc = np.triu_indices(len(matrix_cat), k=1)\n",
    "                rows, cols = matrix_cat[cc, rr].T\n",
    "                return rows, cols\n",
    "\n",
    "            n_masks = len(mask_info['id_to_type'])\n",
    "            concept_ids = [id for id, item in mask_info[\"id_to_type\"].items() if item[0] == \"concept\"]\n",
    "            concept_rows, concept_cols = get_triu_ids(concept_ids)\n",
    "            if len(mask_info[\"id_same_relation\"]) > 0:\n",
    "                relation_rows, relation_cols = np.stack(mask_info[\"id_same_relation\"]).T\n",
    "                repel_rows = np.concatenate([concept_rows, relation_rows])\n",
    "                repel_cols = np.concatenate([concept_cols, relation_cols])\n",
    "            else:\n",
    "                repel_rows, repel_cols = concept_rows, concept_cols\n",
    "            all_rows, all_cols = get_triu_ids(n_masks)\n",
    "            all_tuples = [(row, col) for row, col in zip(all_rows, all_cols)]\n",
    "            repel_tuples = [(row, col) for row, col in zip(repel_rows, repel_cols)]\n",
    "            compat_tuples = [ele for ele in all_tuples if ele not in repel_tuples]\n",
    "            if len(compat_tuples) > 0:\n",
    "                compat_rows, compat_cols = np.array(compat_tuples).T\n",
    "                return compat_rows, compat_cols\n",
    "            else:\n",
    "                return [], []\n",
    "\n",
    "        relation_merge_mode = kwargs[\"relation_merge_mode\"] if \"relation_merge_mode\" in kwargs else \"threshold\"\n",
    "\n",
    "        # Perform SGLD w.r.t. mask and then w.r.t. image:\n",
    "        assert len(train_input.shape) == 5\n",
    "        assert len(train_target.shape) == 5\n",
    "        assert len(test_input.shape) == 5\n",
    "        input = torch.cat([train_input, test_input], 1)\n",
    "        is_return_E = kwargs[\"is_return_E\"] if \"is_return_E\" in kwargs and kwargs[\"is_return_E\"] is True else False\n",
    "\n",
    "        if is_return_E:\n",
    "            recons, mask_dict, E_all = self.forward_NN(input, **kwargs)\n",
    "        else:\n",
    "            recons, mask_dict = self.forward_NN(input, **kwargs)\n",
    "        device = input.device\n",
    "\n",
    "        mask = torch.stack(list(mask_dict.values()), 2)  # [B_task, B_example, n_masks, 1, H, W]\n",
    "        n_masks = len(mask_dict)\n",
    "        mask_expand_0 = mask[:,:,:,None]  # [B_task, B_example, n_masks, 1, 1, H, W]\n",
    "        mask_expand_1 = mask[:,:,None]  # [B_task, B_example, 1, n_masks, 1, H, W]\n",
    "        distance_matrix_batch = get_soft_Jaccard_distance(mask_expand_0, mask_expand_1, dim=(-3,-2,-1))  # [B_task, B_example, n_masks, n_masks]\n",
    "        distance_matrix_mean = distance_matrix_batch.mean(1)  # [B_task, n_masks, n_masks]\n",
    "\n",
    "        mask_info = self.get_mask_info()\n",
    "        compat_rows, compat_cols = get_compat_ids(mask_info)\n",
    "        if len(compat_rows) > 0 and len(compat_cols) > 0:\n",
    "            distance_compat = distance_matrix_mean[:, compat_rows, compat_cols]  # [B_task, n_compat]\n",
    "\n",
    "            if relation_merge_mode == \"threshold\":\n",
    "                relation_threshold = 0.5\n",
    "                merged_mask = distance_matrix_mean < relation_threshold  # [B_task, n_masks, n_masks]\n",
    "                compat_mask = torch.zeros(distance_matrix_mean.shape).bool().to(device)\n",
    "                compat_mask[:, compat_rows, compat_cols] = True\n",
    "                mask_combined = merged_mask & compat_mask  # [B_task, n_masks, n_masks]\n",
    "\n",
    "                test_pred = -torch.ones(train_target[:,:1].shape).to(device)  # Default: if it is all -1, means no prediction\n",
    "                for i in range(mask_combined.shape[0]):\n",
    "                    combined_rows, combined_cols = to_np_array(*torch.where(mask_combined[i]), full_reduce=False)\n",
    "                    unique_ids = np.unique(np.concatenate([combined_rows, combined_cols]))\n",
    "                    if len(unique_ids) == 0:\n",
    "                        continue\n",
    "                    selected_mask = mask[i, :-1, unique_ids]\n",
    "                    train_target_aug = train_target[i,:,None]\n",
    "                    selected_distance = get_soft_Jaccard_distance(selected_mask, train_target_aug, dim=(-3,-2,-1))  # [n_train, n_unique_ids]\n",
    "                    nearest_id = unique_ids[selected_distance.mean(0).argmin().item()]\n",
    "                    test_pred[i] = mask[i, -1:, nearest_id]\n",
    "            else:\n",
    "                raise Exception(\"relation_merge_mode '{}' is not valid!\".format(relation_merge_mode))\n",
    "        else:\n",
    "            test_pred = -torch.ones(train_target[:,:1].shape).to(device)\n",
    "        if is_return_E:\n",
    "            return recons, mask_dict, test_pred, E_all\n",
    "        else:\n",
    "            return recons, mask_dict, test_pred\n",
    "\n",
    "\n",
    "    def forward_NN_gnn(\n",
    "        self,\n",
    "        train_input,\n",
    "        train_target,\n",
    "        test_input,\n",
    "        **kwargs\n",
    "    ):\n",
    "        # Perform SGLD w.r.t. mask and then w.r.t. image:\n",
    "        assert len(train_input.shape) == 5\n",
    "        assert len(train_target.shape) == 5\n",
    "        assert len(test_input.shape) == 5\n",
    "        input = torch.cat([train_input, test_input], 1)\n",
    "\n",
    "        # If there is an additional task-batch dimension, combine that with the example-batch:\n",
    "        batch_shape = tuple(train_input.shape[:2])\n",
    "        batch_shape_combined = tuple(input.shape[:2])\n",
    "        train_input = train_input.reshape(-1, *train_input.shape[-3:])\n",
    "        train_target = train_target.reshape(-1, *train_target.shape[-3:])\n",
    "        test_input = test_input.reshape(-1, *test_input.shape[-3:])\n",
    "        input = input.reshape(-1, *input.shape[-3:])\n",
    "        is_return_E = kwargs[\"is_return_E\"] if \"is_return_E\" in kwargs and kwargs[\"is_return_E\"] is True else False\n",
    "\n",
    "        # Setting up the hyperparameters:\n",
    "        lambd_start = kwargs[\"lambd_start\"] if \"lambd_start\" in kwargs and kwargs[\"lambd_start\"] is not None else self.lambd_start if self.lambd_start is not None else 0.1\n",
    "        lambd = kwargs[\"lambd\"] if \"lambd\" in kwargs and kwargs[\"lambd\"] is not None else self.lambd if self.lambd is not None else 0.005\n",
    "        image_value_range = kwargs[\"image_value_range\"] if \"image_value_range\" in kwargs and kwargs[\"image_value_range\"] is not None else self.image_value_range if self.image_value_range is not None else \"0,1\"\n",
    "        SGLD_is_anneal = kwargs[\"SGLD_is_anneal\"] if \"SGLD_is_anneal\" in kwargs and kwargs[\"SGLD_is_anneal\"] is not None else self.SGLD_is_anneal if self.SGLD_is_anneal is not None else True\n",
    "        SGLD_is_penalize_lower = kwargs[\"SGLD_is_penalize_lower\"] if \"SGLD_is_penalize_lower\" in kwargs and kwargs[\"SGLD_is_penalize_lower\"] is not None else self.SGLD_is_penalize_lower if self.SGLD_is_penalize_lower is not None else True\n",
    "        SGLD_mutual_exclusive_coef = kwargs[\"SGLD_mutual_exclusive_coef\"] if \"SGLD_mutual_exclusive_coef\" in kwargs and kwargs[\"SGLD_mutual_exclusive_coef\"] is not None else self.SGLD_mutual_exclusive_coef if self.SGLD_mutual_exclusive_coef is not None else 0\n",
    "        SGLD_pixel_entropy_coef = kwargs[\"SGLD_pixel_entropy_coef\"] if \"SGLD_pixel_entropy_coef\" in kwargs and kwargs[\"SGLD_pixel_entropy_coef\"] is not None else self.SGLD_pixel_entropy_coef if self.SGLD_pixel_entropy_coef is not None else 0\n",
    "        SGLD_pixel_gm_coef = kwargs[\"SGLD_pixel_gm_coef\"] if \"SGLD_pixel_gm_coef\" in kwargs and kwargs[\"SGLD_pixel_gm_coef\"] is not None else self.SGLD_pixel_gm_coef if self.SGLD_pixel_gm_coef is not None else 0\n",
    "        SGLD_object_exceed_coef = kwargs[\"SGLD_object_exceed_coef\"] if \"SGLD_object_exceed_coef\" in kwargs else 0\n",
    "        # For selector discovery:\n",
    "        SGLD_iou_batch_consistency_coef = kwargs[\"SGLD_iou_batch_consistency_coef\"] if \"SGLD_iou_batch_consistency_coef\" in kwargs and kwargs[\"SGLD_iou_batch_consistency_coef\"] is not None else self.SGLD_iou_batch_consistency_coef if self.SGLD_iou_batch_consistency_coef is not None else 0\n",
    "        SGLD_iou_concept_repel_coef = kwargs[\"SGLD_iou_concept_repel_coef\"] if \"SGLD_iou_concept_repel_coef\" in kwargs and kwargs[\"SGLD_iou_concept_repel_coef\"] is not None else self.SGLD_iou_concept_repel_coef if self.SGLD_iou_concept_repel_coef is not None else 0\n",
    "        SGLD_iou_relation_repel_coef = kwargs[\"SGLD_iou_relation_repel_coef\"] if \"SGLD_iou_relation_repel_coef\" in kwargs and kwargs[\"SGLD_iou_relation_repel_coef\"] is not None else self.SGLD_iou_relation_repel_coef if self.SGLD_iou_relation_repel_coef is not None else 0\n",
    "        SGLD_iou_relation_overlap_coef = kwargs[\"SGLD_iou_relation_overlap_coef\"] if \"SGLD_iou_relation_overlap_coef\" in kwargs and kwargs[\"SGLD_iou_relation_overlap_coef\"] is not None else self.SGLD_iou_relation_overlap_coef if self.SGLD_iou_relation_overlap_coef is not None else 0\n",
    "        SGLD_iou_attract_coef = kwargs[\"SGLD_iou_attract_coef\"] if \"SGLD_iou_attract_coef\" in kwargs and kwargs[\"SGLD_iou_attract_coef\"] is not None else self.SGLD_iou_attract_coef if self.SGLD_iou_attract_coef is not None else 0\n",
    "        # Other settings:\n",
    "        step_size_start = kwargs[\"step_size_start\"] if \"step_size_start\" in kwargs else -1\n",
    "        step_size = kwargs[\"step_size\"] if \"step_size\" in kwargs and kwargs[\"step_size\"] is not None else self.step_size if self.step_size is not None else 20\n",
    "        step_size_img = kwargs[\"step_size_img\"] if \"step_size_img\" in kwargs and kwargs[\"step_size_img\"] is not None else self.step_size_img if self.step_size_img is not None else -1\n",
    "        step_size_z = kwargs[\"step_size_z\"] if \"step_size_z\" in kwargs and kwargs[\"step_size_z\"] is not None else self.step_size_z if self.step_size_z is not None else 2\n",
    "        step_size_zgnn = kwargs[\"step_size_zgnn\"] if \"step_size_zgnn\" in kwargs and kwargs[\"step_size_zgnn\"] is not None else self.step_size_zgnn if self.step_size_zgnn is not None else 2\n",
    "        step_size_wtarget = kwargs[\"step_size_wtarget\"] if \"step_size_wtarget\" in kwargs and kwargs[\"step_size_wtarget\"] is not None else self.step_size_wtarget if self.step_size_wtarget is not None else -1\n",
    "        sample_step = kwargs[\"sample_step\"] if \"sample_step\" in kwargs else 60\n",
    "        ensemble_size = kwargs[\"ensemble_size\"] if \"ensemble_size\" in kwargs else 1\n",
    "        kl_all_step = kwargs[\"kl_all_step\"] if \"kl_all_step\" in kwargs else True\n",
    "        is_grad = kwargs[\"is_grad\"] if \"is_grad\" in kwargs else False\n",
    "        w_init_type = kwargs[\"w_init_type\"] if \"w_init_type\" in kwargs and kwargs[\"w_init_type\"] is not None else self.w_init_type if self.w_init_type is not None else \"random\"\n",
    "        indiv_sample = kwargs[\"indiv_sample\"] if \"indiv_sample\" in kwargs and kwargs[\"indiv_sample\"] is not None else self.indiv_sample if self.indiv_sample is not None else -1\n",
    "        img_init_type = kwargs[\"img_init_type\"] if \"img_init_type\" in kwargs else \"random\"\n",
    "        is_return_E = kwargs[\"is_return_E\"] if \"is_return_E\" in kwargs else False\n",
    "        isplot = kwargs[\"isplot\"] if \"isplot\" in kwargs else 0\n",
    "        verbose = kwargs[\"verbose\"] if \"verbose\" in kwargs else 0\n",
    "\n",
    "        args = get_pdict()(\n",
    "            lambd_start=lambd_start,\n",
    "            lambd=lambd,\n",
    "            image_value_range=image_value_range,\n",
    "            step_size_start=step_size_start,\n",
    "            step_size=step_size,\n",
    "            step_size_img=step_size_img,\n",
    "            step_size_z=step_size_z,\n",
    "            step_size_zgnn=step_size_zgnn,\n",
    "            step_size_wtarget=step_size_wtarget,\n",
    "            image_size=input.shape[-2:],\n",
    "            kl_all_step=kl_all_step,\n",
    "            SGLD_is_anneal=SGLD_is_anneal,\n",
    "            SGLD_is_penalize_lower=SGLD_is_penalize_lower,\n",
    "            SGLD_object_exceed_coef=SGLD_object_exceed_coef,\n",
    "            SGLD_mutual_exclusive_coef=SGLD_mutual_exclusive_coef,\n",
    "            SGLD_pixel_entropy_coef=SGLD_pixel_entropy_coef,\n",
    "            SGLD_pixel_gm_coef=SGLD_pixel_gm_coef,\n",
    "            # Selector:\n",
    "            SGLD_iou_batch_consistency_coef=SGLD_iou_batch_consistency_coef,\n",
    "            SGLD_iou_concept_repel_coef=SGLD_iou_concept_repel_coef,\n",
    "            SGLD_iou_relation_repel_coef=SGLD_iou_relation_repel_coef,\n",
    "            SGLD_iou_relation_overlap_coef=SGLD_iou_relation_overlap_coef,\n",
    "            SGLD_iou_attract_coef=SGLD_iou_attract_coef,\n",
    "        )\n",
    "        ebm_0 = self.ebm_dict[next(iter(self.ebm_dict))]\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            ###################################################\n",
    "            # 1. SGLD w.r.t. w, z and zgnn:\n",
    "            ###################################################\n",
    "            if self.z_mode != \"None\":\n",
    "                args.ebm_target = \"mask+z+zgnn\"\n",
    "            else:\n",
    "                args.ebm_target = \"mask+zgnn\"\n",
    "\n",
    "            if w_init_type in [\"random\", \"input-mask\", \"input-gaus\"] or w_init_type.startswith(\"k-means\"):\n",
    "                mask = None\n",
    "            elif w_init_type == \"input\":\n",
    "                mask_arity = len(self.nodes)\n",
    "                if \"obj\" in ebm_0.w_type:\n",
    "                    assert \"mask\" not in ebm_0.w_type\n",
    "                    mask = tuple(train_input.detach().clone().repeat_interleave(ensemble_size, dim=0) for k in range(mask_arity))\n",
    "                elif \"mask\" in ebm_0.w_type:\n",
    "                    assert \"obj\" not in ebm_0.w_type\n",
    "                    if train_input.shape[1] == 10:\n",
    "                        mask = tuple((train_input[:,:1]!=1).detach().clone().repeat_interleave(ensemble_size, dim=0) for k in range(mask_arity))\n",
    "                    elif train_input.shape[1] == 3:\n",
    "                        mask = None\n",
    "                    else:\n",
    "                        raise\n",
    "                else:\n",
    "                    raise\n",
    "                for k in range(mask_arity):\n",
    "                    if mask is not None:\n",
    "                        mask[k].requires_grad = True\n",
    "            else:\n",
    "                raise Exception(\"w_init_type '{}' is not valid!\".format(w_init_type))\n",
    "\n",
    "            (_, neg_mask_ensemble_sorted_train, z_ensemble_sorted_train, zgnn_ensemble_sorted, _), neg_out_ensemble_sorted, info = self.ground(\n",
    "                train_input,\n",
    "                args,\n",
    "                mask=mask,\n",
    "                wtarget=train_target,\n",
    "                ensemble_size=ensemble_size,\n",
    "                topk=1,\n",
    "                w_init_type=w_init_type,\n",
    "                sample_step=sample_step,\n",
    "                is_grad=is_grad,\n",
    "                is_return_E=is_return_E,\n",
    "                batch_shape=batch_shape,\n",
    "                isplot=isplot,\n",
    "            )\n",
    "            if verbose >= 1:\n",
    "                if hasattr(self.gnn, \"softmax_coef\"):\n",
    "                    print(\"softmax_coef:\")\n",
    "                    print(self.gnn.softmax_coef)\n",
    "                print(list(self.gnn.parameters())[:4])\n",
    "                print(\"zgnn_node:\")\n",
    "                if zgnn_ensemble_sorted[0] is None:\n",
    "                    print(None)\n",
    "                else:\n",
    "                    print(zgnn_ensemble_sorted[0].squeeze())\n",
    "                print(\"zgnn_edge:\")\n",
    "                print(zgnn_ensemble_sorted[1].squeeze())\n",
    "                print()\n",
    "\n",
    "            ###################################################\n",
    "            # 2nd SGLD w.r.t. m', z', wtarget', given zgnn, on test examples:\n",
    "            ###################################################\n",
    "            args.ebm_target = \"mask+z+wtarget\"\n",
    "            if w_init_type in [\"random\", \"input-mask\", \"input-gaus\"] or w_init_type.startswith(\"k-means\"):\n",
    "                mask = None\n",
    "            elif w_init_type == \"input\":\n",
    "                mask_arity = len(self.nodes)\n",
    "                if \"obj\" in ebm_0.w_type:\n",
    "                    assert \"mask\" not in ebm_0.w_type\n",
    "                    mask = tuple(input.detach().clone().repeat_interleave(ensemble_size, dim=0) for k in range(mask_arity))\n",
    "                elif \"mask\" in ebm_0.w_type:\n",
    "                    assert \"obj\" not in ebm_0.w_type\n",
    "                    if input.shape[1] == 10:\n",
    "                        mask = tuple((input[:,:1]!=1).detach().clone().repeat_interleave(ensemble_size, dim=0) for k in range(mask_arity))\n",
    "                    elif input.shape[1] == 3:\n",
    "                        mask = None\n",
    "                    else:\n",
    "                        raise\n",
    "                else:\n",
    "                    raise\n",
    "                for k in range(mask_arity):\n",
    "                    if mask is not None:\n",
    "                        mask[k].requires_grad = True\n",
    "            else:\n",
    "                raise Exception(\"w_init_type '{}' is not valid!\".format(w_init_type))\n",
    "            zgnn_latent = tuple(zgnn_ele[:,0] if zgnn_ele is not None else None for zgnn_ele in zgnn_ensemble_sorted)\n",
    "\n",
    "            (_, neg_mask_ensemble_sorted, z_ensemble_sorted, _, wtarget_ensemble_sorted), neg_out_ensemble_sorted, info = self.ground(\n",
    "                input,\n",
    "                args,\n",
    "                mask=mask,\n",
    "                zgnn=zgnn_latent,\n",
    "                ensemble_size=ensemble_size,\n",
    "                topk=1,\n",
    "                w_init_type=w_init_type,\n",
    "                sample_step=sample_step,\n",
    "                is_grad=is_grad,\n",
    "                is_return_E=is_return_E,\n",
    "                batch_shape=batch_shape_combined,\n",
    "                isplot=isplot,\n",
    "            )\n",
    "            wtarget_pred = wtarget_ensemble_sorted[:,0]\n",
    "            wtarget_pred = wtarget_pred.view(*batch_shape_combined, *wtarget_pred.shape[-3:])\n",
    "            if is_return_E:\n",
    "                E_all = info[\"E_all\"]\n",
    "\n",
    "            ###################################################\n",
    "            # 3rd SGLD w.r.t. img, given mask and z:\n",
    "            ###################################################\n",
    "            args.ebm_target = \"image\"\n",
    "            # Obtain the w_selected (list of w selected by the refer nodes) and a dict of all ws:\n",
    "            w_op_dict = {}\n",
    "            z_op_dict = {}\n",
    "            w_selector = []\n",
    "            nodes = list(self.nodes)\n",
    "            for i, neg_mask_ele in enumerate(neg_mask_ensemble_sorted):\n",
    "                neg_mask_top = neg_mask_ele[:,0]\n",
    "                if self.refer_node_names is None or nodes[i] in self.refer_node_names:\n",
    "                    # Accumulate the objects specified by the self.refer_node_names:\n",
    "                    w_selector.append(neg_mask_top)\n",
    "                w_op_dict[nodes[i]] = neg_mask_top\n",
    "                if indiv_sample != -1:\n",
    "                    z_op_dict[nodes[i]] = z_ensemble_sorted[i][:,0]\n",
    "\n",
    "            # If is_reconstruct, perform a second SGLD w.r.t. img given the inferred w (and z):\n",
    "            if img_init_type == \"random\":\n",
    "                img_value_min, img_value_max = image_value_range.split(\",\")\n",
    "                img_value_min, img_value_max = eval(img_value_min), eval(img_value_max)\n",
    "                img_init = torch.rand(input.shape, device=train_input.device) * (img_value_max - img_value_min) + img_value_min\n",
    "            elif img_init_type == \"input\":\n",
    "                img_init = input.detach()\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "            # Specify the mask according to the refer nodes:\n",
    "            neg_mask_latent = []\n",
    "            mask_info = self.get_mask_info()\n",
    "            for i, neg_mask_ele in enumerate(neg_mask_ensemble_sorted):\n",
    "                if self.refer_node_names is None or nodes[i] in self.refer_node_names:\n",
    "                    if mask_info[\"id_to_type\"][i][0] == \"relation\" and self.is_relation_z is False:\n",
    "                        neg_mask_latent.append(None)\n",
    "                    else:\n",
    "                        neg_mask_latent.append(neg_mask_ele[:,0])\n",
    "                else:\n",
    "                    # If not refer nodes, then turn off the mask:\n",
    "                    neg_mask_latent.append(torch.zeros(neg_mask_ele[:,0].shape).to(device))\n",
    "\n",
    "            # Specify the z:\n",
    "            z_latent = tuple(ele[:,0] if ele is not None else None for ele in z_ensemble_sorted) if \"z\" in args.ebm_target else None\n",
    "\n",
    "            if indiv_sample != -1:\n",
    "                # Go through each EBM\n",
    "                pred = None\n",
    "                ebm_dict = self.get_ebms()\n",
    "                for key in w_op_dict.keys():\n",
    "                    model = list(ebm_dict[key].values())[0]\n",
    "                    (img_ensemble_sorted, _, _, _, _), neg_out_ensemble_sorted = model.ground(\n",
    "                        img_init,\n",
    "                        args,\n",
    "                        mask=tuple([w_op_dict[key]]),\n",
    "                        z=tuple([z_op_dict[key]]),\n",
    "                        ensemble_size=ensemble_size,\n",
    "                        topk=1,\n",
    "                        sample_step=indiv_sample,\n",
    "                        is_grad=is_grad,\n",
    "                        isplot=isplot,\n",
    "                    )\n",
    "                    if pred is None:\n",
    "                        pred = img_ensemble_sorted[:, 0] * w_op_dict[key]\n",
    "                    else:\n",
    "                        pred = pred + img_ensemble_sorted[:, 0] * w_op_dict[key]\n",
    "                img_value_min, img_value_max = args.image_value_range.split(\",\")\n",
    "                img_value_min, img_value_max = eval(img_value_min), eval(img_value_max)\n",
    "                pred = pred.clamp(min=img_value_min, max=img_value_max)\n",
    "            else:\n",
    "                (img_ensemble_sorted, _, _, _, _), neg_out_ensemble_sorted, info = self.ground(\n",
    "                    img_init,\n",
    "                    args,\n",
    "                    mask=neg_mask_latent,\n",
    "                    z=z_latent,\n",
    "                    ensemble_size=ensemble_size,\n",
    "                    topk=1,\n",
    "                    sample_step=sample_step,\n",
    "                    is_grad=is_grad,\n",
    "                    isplot=isplot,\n",
    "                )\n",
    "                pred = img_ensemble_sorted[:,0]\n",
    "\n",
    "        assert pred.shape == input.shape\n",
    "\n",
    "        # Recover the task-batch dimension:\n",
    "        pred = pred.view(*batch_shape_combined, *pred.shape[-3:])\n",
    "        w_op_dict = {key: item.view(*batch_shape_combined, *item.shape[-3:]) for key, item in w_op_dict.items()}\n",
    "\n",
    "        # Store the results in a cache, to be used by the policy:\n",
    "        if is_return_E:\n",
    "            return pred, w_op_dict, wtarget_pred, E_all\n",
    "        else:\n",
    "            return pred, w_op_dict, wtarget_pred\n",
    "\n",
    "\n",
    "    # Pivot and refer nodes:\n",
    "    def set_pivot_nodes(self, node_names):\n",
    "        \"\"\"Set pivot nodes for the concept pattern, which are used for identifying\n",
    "        a pivot in the concept_graph.\n",
    "        \"\"\"\n",
    "        if not isinstance(node_names, list):\n",
    "            node_names = [node_names]\n",
    "        node_name_list = []\n",
    "        for node_name in node_names:\n",
    "            node_name = self.get_node_name(node_name)\n",
    "            if node_name in self.nodes:\n",
    "                node_name_list.append(node_name)\n",
    "        self.pivot_node_names = node_name_list\n",
    "        self.clear_instance()\n",
    "        return self\n",
    "\n",
    "\n",
    "    def set_refer_nodes(self, node_names):\n",
    "        \"\"\"Set refer nodes for the concept pattern, which are used for identifying\n",
    "        the nodes intended to refer to in the concept_graph.\n",
    "        \"\"\"\n",
    "        if not isinstance(node_names, list):\n",
    "            node_names = [node_names]\n",
    "        node_name_list = []\n",
    "        for node_name in node_names:\n",
    "            node_name = self.get_node_name(node_name)\n",
    "            if node_name in self.nodes:\n",
    "                node_name_list.append(node_name)\n",
    "        self.refer_node_names = node_name_list\n",
    "        return self\n",
    "\n",
    "\n",
    "    # Clear stuff:\n",
    "    def clear_instance(self):\n",
    "        \"\"\"If pivot_node_names is set, clear the value of all other nodes\n",
    "        (since it is graph pattern, we want to refer to all other nodes using relation w.r.t. pivot_node).\"\"\"\n",
    "        if self.pivot_node_names is not None:\n",
    "            for node_name in self.nodes:\n",
    "                if node_name not in self.pivot_node_names:\n",
    "                    self.remove_node_value(node_name)\n",
    "        else:\n",
    "            print(\"Pivot nodes are not set. Do not clear any content of the nodes.\")\n",
    "        return self\n",
    "\n",
    "\n",
    "    def add_obj(self, concept, *opsc, **kwargs):\n",
    "        \"\"\"Add a concept node (representing an object) to the Concept_Pattern.\n",
    "\n",
    "        Args:\n",
    "            concept: concept to be added to the Concept_Pattern\n",
    "            *opsc: a list of existing concept nodes (whose default must be DEFAULT_OBJ_TYPE) whose mode\n",
    "                will be replaced by the concept's mode.\n",
    "            **kwargs: kwargs for init_ebm() if the concept does not exist in the selector.\n",
    "        \"\"\"\n",
    "        if len(opsc) == 0:\n",
    "            # Add a concept node and do not merge with any existing concept node:\n",
    "            if \"add_obj_name\" in kwargs:\n",
    "                obj_name = kwargs[\"add_obj_name\"]\n",
    "            else:\n",
    "                obj_name = get_next_available_key([node_name.split(\":\")[0] for node_name in self.nodes], \"obj\", is_underscore=True)\n",
    "            placeholder = Placeholder(concept.name)\n",
    "            if concept.name not in self.ebm_dict:\n",
    "                self.init_ebm(\n",
    "                    method=\"random\",\n",
    "                    mode=concept.name,\n",
    "                    ebm_mode=\"concept\",\n",
    "                    ebm_model_type=\"CEBM\",\n",
    "                    **kwargs\n",
    "                )\n",
    "            placeholder.set_ebm_key(concept.name)\n",
    "            self.add_node(\"{}:{}\".format(obj_name, concept.name), value=placeholder, type=\"obj\")\n",
    "        else:\n",
    "            # Replacing the default concept type in the nodes in opsc by the concept's type:\n",
    "            for op_name in opsc:\n",
    "                op_name = self.get_node_name(op_name)\n",
    "                op_mode = op_name.split(\":\")[-1]\n",
    "                assert op_mode == DEFAULT_OBJ_TYPE, \"To assign concept to existing opsc, the opsc '{}' must have DEFAULT_OBJ_TYPE of '{}', not '{}'.\".format(op_name, DEFAULT_OBJ_TYPE, op_mode)\n",
    "                placeholder = self.get_node_content(op_name)\n",
    "                if concept.name not in self.ebm_dict:\n",
    "                    self.init_ebm(\n",
    "                        method=\"random\",\n",
    "                        mode=concept.name,\n",
    "                        ebm_mode=\"concept\",\n",
    "                        ebm_model_type=\"CEBM\",\n",
    "                        **kwargs\n",
    "                    )\n",
    "                placeholder.change_mode(concept.name, new_ebm_key=concept.name)\n",
    "                nx.relabel_nodes(self, {op_name: \"{}:{}\".format(op_name.split(\":\")[0], concept.name)}, copy=False)\n",
    "        # Important: reset the forward results cache\n",
    "        if self.cache_forward:\n",
    "            self.forward_cache = {}\n",
    "        return self\n",
    "\n",
    "\n",
    "    def remove_attr(self, attr_name):\n",
    "        \"\"\"Remove an attribute and all its descendant attributes.\"\"\"\n",
    "        super(Concept_Pattern, self).remove_attr(attr_name, change_root=False)\n",
    "        if self.pivot_node_names is not None and attr_name in self.pivot_node_names:\n",
    "            self.pivot_node_names.remove(attr_name)\n",
    "        if self.refer_node_names is not None and attr_name in self.refer_node_names:\n",
    "            self.refer_node_names.remove(attr_name)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def remove_attrs_from(self, attr_names):\n",
    "        \"\"\"Remove multiple nodes from the graph, also remove them in pivot and refer nodes if they are inside.\"\"\"\n",
    "        if not isinstance(attr_names, list):\n",
    "            attr_names = [attr_names]\n",
    "        for attr_name in attr_names:\n",
    "            self.remove_attr(attr_name)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"Obtain the parameters of the relevant ebms in the self.ebm_dict.\"\"\"\n",
    "        keys = []\n",
    "        for src, ebm_dict in self.get_ebms().items():\n",
    "            if ebm_dict is not None:\n",
    "                keys += list(ebm_dict.keys())\n",
    "        keys = remove_duplicates(keys)\n",
    "        return itertools.chain.from_iterable([self.ebm_dict[key].parameters() for key in keys])\n",
    "\n",
    "\n",
    "    def to(self, device):\n",
    "        super(Concept_Pattern, self).to(device)\n",
    "        self.device = device\n",
    "        if hasattr(self, \"ebm_dict\"):\n",
    "            for key in self.ebm_dict:\n",
    "                self.ebm_dict[key].to(device)\n",
    "            if self.ebm_dict.__class__.__name__ == \"Shared_Param_Dict\":\n",
    "                self.ebm_dict.to(device)\n",
    "        if hasattr(self, \"gnn\"):\n",
    "            self.gnn.to(device)\n",
    "        return self\n",
    "\n",
    "\n",
    "    @property\n",
    "    def mask_arity(self):\n",
    "        return len(self.nodes)\n",
    "\n",
    "\n",
    "    # Printing:\n",
    "    def __str__(self):\n",
    "        string = \"nodes={}, edges={}\".format(len(self.nodes), len(self.edges))\n",
    "        if self.pivot_node_names is not None:\n",
    "            string += \", pivot={}\".format(self.pivot_node_names)\n",
    "        if self.refer_node_names is not None:\n",
    "            string += \", refer={}\".format(self.refer_node_names)\n",
    "        return \"Concept_Pattern({})\".format(string)\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        if IS_VIEW:\n",
    "            if len(self.nodes) > 0:\n",
    "                self.draw()\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.3 Concept_Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Concept_Ensemble(BaseGraph):\n",
    "    \"\"\"An ensemble of concepts, including their inter-concept relations.\"\"\"\n",
    "    def __init__(self, concepts=None):\n",
    "        super(Concept_Ensemble, self).__init__()\n",
    "        self.add_concepts(concepts)\n",
    "\n",
    "\n",
    "    def add_concept(self, concept, name):\n",
    "        \"\"\"Add a single concept.\"\"\"\n",
    "        self.add_node(name, value=concept, type=\"concept\")\n",
    "        return self\n",
    "\n",
    "\n",
    "    def add_concepts(self, concepts):\n",
    "        \"\"\"Add multiple concepts.\"\"\"\n",
    "        if concepts is not None:\n",
    "            for name, concept in concepts.items():\n",
    "                self.add_concept(concept, name=name)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def get_concept(self, name):\n",
    "        \"\"\"Obtain the concept according to its name.\"\"\"\n",
    "        return self.get_node_content(name)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def concept_names(self):\n",
    "        \"\"\"Return all concept names in the emsemble.\"\"\"\n",
    "        return list(self.nodes)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def concepts(self):\n",
    "        \"\"\"Return all concepts in the emsemble.\"\"\"\n",
    "        concepts = OrderedDict()\n",
    "        for name in self.concept_names:\n",
    "            concepts[name] = self.get_concept(name)\n",
    "        return concepts\n",
    "\n",
    "\n",
    "    @property\n",
    "    def relations(self):\n",
    "        \"\"\"Return all relations between inter-concept objects in the emsemble.\"\"\"\n",
    "        relations = {}\n",
    "        for edge in self.edges:\n",
    "            if edge[-1] == 1:\n",
    "                relations[(edge[0], edge[1])] = self.edges[edge][\"value\"]\n",
    "        return relations\n",
    "\n",
    "\n",
    "    @property\n",
    "    def theories(self):\n",
    "        \"\"\"Return all theory strings between inter-concept objects in the emsemble.\"\"\"\n",
    "        theories = {}\n",
    "        for edge in self.edges:\n",
    "            if edge[-1] == 0:\n",
    "                theories[(edge[0], edge[1])] = self.edges[edge][\"value\"]\n",
    "        return theories\n",
    "\n",
    "\n",
    "    def add_relations(self, concept1_name, concept2_name, OPERATORS, allowed_types=[\"Bool\"]):\n",
    "        \"\"\"Add all possible relations between objects of concept1 and objects of concept2.\"\"\"\n",
    "        if not isinstance(allowed_types, list):\n",
    "            allowed_types = [allowed_types]\n",
    "        for obj1_name, obj1 in self.get_concept(concept1_name).objs.items():\n",
    "            for obj2_name, obj2 in self.get_concept(concept2_name).objs.items():\n",
    "                for name, op in OPERATORS.items():\n",
    "                    output_mode = op.get_to_outnode(op.name).split(\":\")[-1]\n",
    "                    if \"Bool\" in allowed_types and output_mode == \"Bool\":\n",
    "                        is_valid = op(obj1, obj2)\n",
    "                    elif \"Op\" in allowed_types and output_mode != \"Bool\" and len(op.dangling_nodes) == 0 and len(op.input_placeholder_nodes) == 1:\n",
    "                        obj1_trans = op(obj1)\n",
    "                        is_valid = obj1_trans == obj2\n",
    "                    if is_valid:\n",
    "                        edge_key = (concept1_name, concept2_name, 1)\n",
    "                        if edge_key not in self.edges:\n",
    "                            self.add_edge(concept1_name, concept2_name, 1, value={}, type=\"inter-concept-relation\")\n",
    "                        record_data(self.edges[edge_key][\"value\"], [name], [(obj1_name, obj2_name)])\n",
    "        return self\n",
    "\n",
    "\n",
    "    def add_theories(self, concept1_name, concept2_name, pair_list):\n",
    "        \"\"\"Add theories as specified by pair_list between objects in concept1 and concept2\"\"\"\n",
    "        for pair in pair_list:\n",
    "            edge_key = (concept1_name, concept2_name, 0)\n",
    "            if edge_key not in self.edges:\n",
    "                self.add_edge(concept1_name, concept2_name, 0, value={}, type=\"inter-concept-theory\")\n",
    "            input_obj_name, target_obj_name = pair[\"input_obj_name\"], pair[\"target_obj_name\"]\n",
    "            self.edges[edge_key][\"value\"][(input_obj_name, target_obj_name)] = pair[\"op_names\"]\n",
    "        return self\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        string = \", \".join([\"{}: {}\".format(name, self.get_concept(name).name) for name in self.concept_names])\n",
    "        return \"Concept_Ensemble({})\".format(string)\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        self.draw()\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch processing:\n",
    "def add_relations(concepts, OPERATORS, allowed_types=[\"Bool\"]):\n",
    "    \"\"\"Endow each concept the valid relations among objects.\"\"\"\n",
    "    if not isinstance(allowed_types, list):\n",
    "        allowed_types = [allowed_types]\n",
    "    if isinstance(concepts, list):\n",
    "        return [add_relations(concept, OPERATORS, allowed_types=allowed_types) for concept in concepts]\n",
    "    if not isinstance(concepts, dict):\n",
    "        return concepts.add_relations(OPERATORS, allowed_types=allowed_types)\n",
    "    else:\n",
    "        return OrderedDict([[name, concept.add_relations(OPERATORS, allowed_types=allowed_types)] for name, concept in concepts.items()])\n",
    "    \n",
    "    \n",
    "def get_refer_nodes(concepts, concept_pattern):\n",
    "    \"\"\"For each concept, obtain refer node.\"\"\"\n",
    "    if not isinstance(concepts, dict):\n",
    "        return concepts.get_refer_nodes(concept_pattern)\n",
    "    else:\n",
    "        return OrderedDict([[name, concept.get_refer_nodes(concept_pattern)] for name, concept in concepts.items()])\n",
    "\n",
    "\n",
    "def to_Concept(patches):\n",
    "    \"\"\"PyTorch tensors to Concept list.\"\"\"\n",
    "    return {\"obj_{}:Image\".format(i): CONCEPTS[DEFAULT_OBJ_TYPE].copy().set_node_value(value).set_node_value(pos, \"pos\") \n",
    "                  for i, (value, pos) in enumerate(patches)}\n",
    "\n",
    "\n",
    "def parse_obj(concepts, is_colordiff=True, is_diag=True, verbose=False):\n",
    "    \"\"\"Endow concepts with parsed objects\"\"\"\n",
    "    if isinstance(concepts, list):\n",
    "        return [parse_obj(concept, is_colordiff=is_colordiff, is_diag=is_diag, verbose=verbose) for concept in concepts]\n",
    "    if not isinstance(concepts, dict):\n",
    "        return parse_obj_ele(concepts, is_colordiff=is_colordiff, is_diag=is_diag, verbose=verbose)\n",
    "    else:\n",
    "        return OrderedDict([[name, parse_obj_ele(concept, is_colordiff=is_colordiff, is_diag=is_diag, verbose=verbose)] for name, concept in concepts.items()])\n",
    "\n",
    "\n",
    "def parse_obj_ele(concept, is_colordiff=True, is_diag=True, verbose=False):\n",
    "    \"\"\"Endow a concept with parsed objects\"\"\"\n",
    "    if isinstance(concept, Concept):\n",
    "        tensor = concept.get_node_value()\n",
    "    else:\n",
    "        tensor = concept\n",
    "    if is_colordiff:\n",
    "        patches = find_connected_components_colordiff(tensor, is_diag=is_diag)\n",
    "    else:\n",
    "        patches = find_connected_components(tensor, is_diag=is_diag)\n",
    "    objs = to_Concept(patches)\n",
    "    if len(objs) > 1:\n",
    "        for obj_name, obj in objs.items():\n",
    "            if verbose:\n",
    "                print('Displaying {}:'.format(obj_name))\n",
    "                display(obj)\n",
    "                print()\n",
    "            concept.add_obj(obj, obj_name=obj_name)\n",
    "    elif len(objs) == 1:\n",
    "        obj_name = list(objs.keys())[0]\n",
    "        obj = objs[obj_name]\n",
    "        if not (obj.get_node_value(\"pos\") == concept.get_node_value(\"pos\")).all():\n",
    "            concept.add_obj(obj, obj_name=obj_name)\n",
    "    return concept\n",
    "\n",
    "\n",
    "def endow_objs_from_pairing_ele(input, target, pair_list):\n",
    "    \"\"\"Add objects to input and target (assuming originally no obj parsing).\"\"\"\n",
    "    input_graph = input.copy()\n",
    "    target_graph = target.copy()\n",
    "    for pair in pair_list:\n",
    "        input_obj = pair[\"input\"]\n",
    "        target_obj = pair[\"target\"]\n",
    "        input_obj_name = input_graph.add_obj(input_obj)\n",
    "        target_obj_name = target_graph.add_obj(target_obj)\n",
    "        pair[\"input_obj_name\"] = input_obj_name\n",
    "        pair[\"target_obj_name\"] = target_obj_name\n",
    "    return input_graph, target_graph\n",
    "\n",
    "\n",
    "def endow_objs_from_pairing(inputs, targets, pair_list_all, is_inter_relation=False):\n",
    "    \"\"\"Return a concept_ensemble where the each pair of (input, target) are endowed objects and their relations.\"\"\"\n",
    "    concept_ensemble = Concept_Ensemble()\n",
    "    if not isinstance(inputs, dict):\n",
    "        inputs = {0: inputs}\n",
    "    if not isinstance(targets, dict):\n",
    "        targets = {0: targets}\n",
    "    for key, input in inputs.items():\n",
    "        target = targets[key]\n",
    "        pair_list = pair_list_all[key][\"pair_list\"]\n",
    "        input_graph, target_graph = endow_objs_from_pairing_ele(input, target, pair_list)\n",
    "        concept_ensemble.add_concept(input_graph, (key, \"input\"))\n",
    "        concept_ensemble.add_concept(target_graph, (key, \"target\"))\n",
    "        concept_ensemble.add_theories((key, \"input\"), (key, \"target\"), pair_list)\n",
    "        if is_inter_relation:\n",
    "            concept_ensemble.add_relations((key, \"input\"), (key, \"target\"), OPERATORS)\n",
    "    return concept_ensemble\n",
    "\n",
    "\n",
    "def get_pair_PyG_data(\n",
    "    input,\n",
    "    target,\n",
    "    OPERATORS,\n",
    "    parse_pair_ele,\n",
    "    allowed_attr=\"obj\",\n",
    "    repr_format=\"onehot\",\n",
    "    cache_dirname=None,\n",
    "):\n",
    "    \"\"\"Given input and target, parse pair_list and return edge_index and edge_attr on the relation between objects in input and targets.\"\"\"\n",
    "    if len(input.obj_names) == 0:\n",
    "        input = parse_obj_ele(input)\n",
    "    if len(target.obj_names) == 0:\n",
    "        target = parse_obj_ele(target)\n",
    "    pair_list, info = parse_pair_ele(input, target, use_given_objs=False, cache_dirname=cache_dirname, isplot=False)\n",
    "\n",
    "    # Build (composite) objects:\n",
    "    for pair in pair_list:\n",
    "        input_patch = pair[\"input\"]\n",
    "        target_patch = pair[\"target\"]\n",
    "\n",
    "        obj_names_input, unexplained_input = input.parse_comp_obj(input_patch)\n",
    "        assert unexplained_input.sum() == 0\n",
    "        if len(obj_names_input) > 1:\n",
    "            obj_name_input = input.combine_objs(obj_names_input)\n",
    "        else:\n",
    "            obj_name_input = obj_names_input[0]\n",
    "        pair[\"obj_name_input\"] = obj_name_input\n",
    "        obj_names_target, unexplained_target = target.parse_comp_obj(target_patch)\n",
    "        assert unexplained_target.sum() == 0\n",
    "        if len(obj_names_target) > 1:\n",
    "            obj_name_target = target.combine_objs(obj_names_target)\n",
    "        else:\n",
    "            obj_name_target = obj_names_target[0]\n",
    "        pair[\"obj_name_target\"] = obj_name_target\n",
    "\n",
    "    # Build PyG data:\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "\n",
    "    input_nodes_sorted = input.get_graph(allowed_attr).topological_sort\n",
    "    num_input_nodes = len(input_nodes_sorted)\n",
    "    target_nodes_sorted = target.get_graph(allowed_attr).topological_sort\n",
    "    \n",
    "    # x denotes whether the node is input (0) or target node:\n",
    "    x = torch.zeros(num_input_nodes + len(target_nodes_sorted), 2)\n",
    "    x[:num_input_nodes, 0] = 1\n",
    "    x[num_input_nodes:, 1] = 1\n",
    "\n",
    "    # Build relations between objects in input and objects in target:\n",
    "    for pair in pair_list:\n",
    "        input_patch = pair[\"input\"]\n",
    "        target_patch = pair[\"target\"]\n",
    "        op_names_pair = pair[\"op_names\"]\n",
    "        source_id = input_nodes_sorted.index(pair[\"obj_name_input\"])\n",
    "        target_id = num_input_nodes + target_nodes_sorted.index(pair[\"obj_name_target\"])\n",
    "        edge_index.append([source_id, target_id])\n",
    "        if repr_format == \"onehot\":\n",
    "            edge_attr_vec = torch.zeros(len(OPERATORS) + 4)\n",
    "            for op_name in OPERATORS:\n",
    "                for op_name_pair in op_names_pair:\n",
    "                    if op_name in op_name_pair:\n",
    "                        op_id = list(OPERATORS.keys()).index(op_name)\n",
    "                        edge_attr_vec[op_id] = 1\n",
    "                    if \"changeColor\" in op_name_pair:\n",
    "                        op_id = list(OPERATORS.keys()).index(\"Draw\")\n",
    "                        edge_attr_vec[op_id] = 1\n",
    "        elif repr_format == \"embedding\":\n",
    "            edge_attr_vec = torch.zeros(REPR_DIM)\n",
    "            for op_name in OPERATORS:\n",
    "                for op_name_pair in op_names_pair:\n",
    "                    if op_name in op_name_pair:\n",
    "                        edge_attr_vec = edge_attr_vec + OPERATORS[op_name].get_node_repr()\n",
    "                        break\n",
    "        else:\n",
    "            raise Exception(\"repr_format {} is not supported!\".format(repr_format))\n",
    "        edge_attr.append(edge_attr_vec)\n",
    "    if len(edge_index) > 0:\n",
    "        edge_index = to_Variable(edge_index).long().T.to(input.device)\n",
    "        edge_attr = torch.stack(edge_attr).to(input.device)\n",
    "    else:\n",
    "        edge_index = torch.zeros(2, 0).long().to(input.device)\n",
    "        if repr_format == \"onehot\":\n",
    "            edge_attr = torch.zeros(0, len(OPERATORS) + 4).to(input.device)\n",
    "        elif repr_format == \"embedding\":\n",
    "            edge_attr = torch.zeros(0, REPR_DIM).to(input.device)\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    # Combine with relations among objects in input and objects in target:\n",
    "    if len(input.get_relations()) == 0:\n",
    "        input.add_relations(OPERATORS)\n",
    "    input_data = input.get_PyG_data(OPERATORS, allowed_attr=allowed_attr, repr_format=repr_format)\n",
    "    if len(target.get_relations()) == 0:\n",
    "        target.add_relations(OPERATORS)\n",
    "    target_data = target.get_PyG_data(OPERATORS, allowed_attr=allowed_attr, repr_format=repr_format)\n",
    "#     x_repr = torch.cat([input_data.x, target_data.x], 0)\n",
    "#     x = torch.cat([x, x_repr], -1)\n",
    "    edge_index = torch.cat([edge_index, input_data.edge_index, num_input_nodes + target_data.edge_index], -1)\n",
    "    edge_attr = torch.cat([edge_attr, input_data.edge_attr, target_data.edge_attr])\n",
    "\n",
    "    return x, edge_index, edge_attr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Important methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_relations(\n",
    "    inputs,\n",
    "    num_copies=100,\n",
    "    max_operators=8,\n",
    "    add_full_concept=True,\n",
    "    add_attr_prob=0.2,\n",
    "    mode=\"stack\",\n",
    "    input_mode_dict=None,\n",
    "    OPERATORS=None,\n",
    "    CONCEPTS=None,\n",
    "    ):\n",
    "    \"\"\"Find a list of Graph() that returns True in one of its results, for all examples in positives_inputs.\"\"\"\n",
    "    # Find relations:\n",
    "    relation = Graph()\n",
    "    ## Add initial concepts:\n",
    "    key = list(inputs[0].keys())[0]\n",
    "    for input_arg in inputs:\n",
    "        relation.add_subgraph(input_arg[key], add_full_concept=add_full_concept)\n",
    "\n",
    "    graph_dict = OrderedDict([[i, deepcopy(relation)] for i in range(num_copies)])\n",
    "    concepts = combine_dicts([OPERATORS])\n",
    "    graph_id_true = {}\n",
    "    is_stop = False\n",
    "    for k in range(max_operators):\n",
    "        for i in range(num_copies):\n",
    "            if i not in graph_id_true:\n",
    "                graph_dict[i] = add_operator_random(graph_dict[i], concepts, add_attr_prob=add_attr_prob, input_mode_dict=input_mode_dict, OPERATORS=OPERATORS, CONCEPTS=CONCEPTS)\n",
    "                results = graph_dict[i](*inputs, is_output_all=True)\n",
    "                is_result_true, node_true = check_result_true(results)\n",
    "                if is_result_true:\n",
    "                    # Preserve minimal subgraph that contains node_true:\n",
    "                    G = graph_dict[i].copy().preserve_subgraph(node_true, level=\"node\")\n",
    "                    if len(G.input_placeholder_nodes) == len(inputs):\n",
    "                        graph_id_true[i] = k\n",
    "                        graph_dict[i] = G\n",
    "                        if mode == \"exists\":\n",
    "                            is_stop = True\n",
    "                            break\n",
    "                        elif mode == \"stack\":\n",
    "                            pass\n",
    "                        else:\n",
    "                            raise\n",
    "                    else:\n",
    "                        graph_dict[i].preserve_subgraph(node_true, level=\"operator\")\n",
    "        if is_stop:\n",
    "            break\n",
    "    graph_list = [graph for i, graph in graph_dict.items() if i in graph_id_true]\n",
    "    return graph_list\n",
    "\n",
    "\n",
    "def compose_relations(graph_list, max_operators=2):\n",
    "    \"\"\"Unify multiple graphs into a single relational operator graph.\"\"\"\n",
    "    relation = Graph()\n",
    "    for graph in graph_list:\n",
    "        if len(graph.operators) <= max_operators:\n",
    "            # Remove duplicate operators (temporary):\n",
    "            operator_to_remove = []\n",
    "            for operator in graph.operators:\n",
    "                if operator in relation.operators:\n",
    "                    operator_to_remove.append(operator)\n",
    "            if len(operator_to_remove) > 0:\n",
    "                graph = graph.copy().remove_subgraph(operator_to_remove, is_rename=False)\n",
    "            # Compose:\n",
    "            relation.compose(graph)\n",
    "    bool_output_node = relation.add_And_over_bool()\n",
    "    length = len(relation.nodes)\n",
    "    if bool_output_node is not None:\n",
    "        relation.preserve_subgraph(bool_output_node, level=\"node\")\n",
    "    else:\n",
    "        raise Exception(\"The relation does not have a Boolean output node!\")\n",
    "    relation.remove_subgraph(relation.operators_dangling)\n",
    "    if len(relation.nodes) != length:\n",
    "        print(\"The composed relation had {} dangling outputs that are removed.\".format(length - len(relation.nodes)))\n",
    "    return relation\n",
    "\n",
    "\n",
    "def create_new_concept(\n",
    "    positive_inputs,\n",
    "    relation,\n",
    "    name=None,\n",
    "    inherit_from=[DEFAULT_OBJ_TYPE],\n",
    "    is_cuda=False,\n",
    "):\n",
    "    \"\"\"Create a new concept based on the data and discovered relational operator graph.\"\"\"\n",
    "    # Add basic properties:\n",
    "    if name is None:\n",
    "        name = get_next_available_key(NEW_CONCEPTS, \"CNew\",\n",
    "                                      suffix=\"Id\", is_underscore=False)\n",
    "    kwargs = {}\n",
    "    kwargs[\"name\"] = name\n",
    "    kwargs[\"repr\"] = to_Variable(torch.rand(REPR_DIM), is_cuda=is_cuda)\n",
    "    kwargs[\"inherit_from\"] = inherit_from\n",
    "    kwargs[\"value\"] = Placeholder(Tensor(dtype=\"cat\"))\n",
    "    kwargs[\"attr\"] = OrderedDict()\n",
    "\n",
    "    # Add attributes based on positives_parse_dict:\n",
    "    key = next(iter(positive_inputs[0]))\n",
    "    for input_arg in positive_inputs:\n",
    "        concept = input_arg[key]\n",
    "        concept_name = concept.name\n",
    "        concept_name_lower = concept_name[0].lower() + concept_name[1:]\n",
    "        current_keys = Counter([split_string(string)[0] for string in list(kwargs[\"attr\"].keys())])\n",
    "        if concept_name_lower not in current_keys:\n",
    "            current_keys[concept_name_lower] = 1\n",
    "            suffix = \"\"\n",
    "        else:\n",
    "            suffix = current_keys[concept_name_lower]\n",
    "            current_keys[concept_name_lower] += 1\n",
    "        kwargs[\"attr\"][\"{}{}\".format(concept_name_lower, suffix)] = Placeholder(concept_name)\n",
    "    # Add relation:\n",
    "    kwargs[\"re\"] = {tuple(kwargs[\"attr\"].keys()): relation}\n",
    "    # Add position attribute:\n",
    "    kwargs[\"attr\"][\"pos\"] = Placeholder(\"Pos\")\n",
    "    return Concept(**kwargs)\n",
    "\n",
    "\n",
    "def add_operator_random(\n",
    "    graph,\n",
    "    concepts,\n",
    "    dangling_mode={\"possible\": 0.2, \"dangling\": 0.8},\n",
    "    add_attr_prob=0.2,\n",
    "    input_mode_dict=None,\n",
    "    OPERATORS=None,\n",
    "    CONCEPTS=None,\n",
    "):\n",
    "    \"\"\"Return a new graph randomly adding a valid operator or expand attributes of an out_node, according to the \n",
    "    probabilities assigned.\n",
    "    \"\"\"\n",
    "    output_nodes_dict = graph.get_output_nodes([\"input\", \"attr\", \"fun-out\"], dangling_mode=dangling_mode)\n",
    "    output_nodes, priority_score = list(output_nodes_dict.keys()), np.array(list(output_nodes_dict.values()))\n",
    "    priority_score = priority_score / priority_score.sum()\n",
    "\n",
    "    for i in range(500):\n",
    "        p0 = np.random.rand()\n",
    "        ## Expand attributes from current out_nodes:\n",
    "        if p0 < add_attr_prob:\n",
    "            output_node_chosen = np.random.choice(output_nodes)\n",
    "            output_node_mode = output_node_chosen.split(\":\")[-1]\n",
    "            attrs = concepts[output_node_mode].child_nodes(output_node_mode)\n",
    "            if len(attrs) == 0:\n",
    "                continue\n",
    "            attr_chosen = np.random.choice(attrs)\n",
    "            G = deepcopy(graph)\n",
    "            G.add_get_attr(output_node_chosen, attr_chosen)\n",
    "\n",
    "        ## Add operators on current out_nodes\n",
    "        else:\n",
    "            p = np.random.rand()\n",
    "            if p < 0.2:\n",
    "                arity = 1\n",
    "                nodes = np.random.choice(output_nodes, size=1, replace=False, p=priority_score).tolist()\n",
    "            elif p > 0.8:\n",
    "                arity = 3\n",
    "                if len(output_nodes) < arity:\n",
    "                    continue\n",
    "                nodes = np.random.choice(output_nodes, size=3, replace=False, p=priority_score).tolist()\n",
    "            else:\n",
    "                arity = 2\n",
    "                nodes = np.random.choice(output_nodes, size=2, replace=False, p=priority_score).tolist()\n",
    "            options = find_valid_operators(nodes, operators=OPERATORS, concepts=CONCEPTS, input_mode_dict=input_mode_dict, arity=arity,\n",
    "                                           exclude=[\"Identity\", \"DrawOn\", \"Draw\", \"Move\"])\n",
    "            if len(options) == 0:\n",
    "                continue\n",
    "            key = np.random.choice(list(options.keys()))\n",
    "            G = deepcopy(graph)\n",
    "            G.add_subgraph(OPERATORS[key])\n",
    "            for source, target in options[key]:\n",
    "                if G.rename_mapping is not None:\n",
    "                    for operator_name, new_operator_name in G.rename_mapping.items():\n",
    "                        if target.startswith(operator_name):\n",
    "                            target = new_operator_name + target[len(operator_name):]\n",
    "                            break\n",
    "                G.connect_nodes(source, target)\n",
    "        break\n",
    "\n",
    "    if \"G\" not in locals():\n",
    "        return graph\n",
    "    else:\n",
    "        return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Testing selector augmented with EBMs:\n",
    "    IS_CUDA = True\n",
    "    num_colors = 10\n",
    "    CONCEPTS[\"Image\"] = Concept(name=\"Image\",\n",
    "        repr=to_Variable(torch.rand(REPR_DIM), is_cuda=IS_CUDA),\n",
    "        inherit_to=[\"c0\", \"c1\"],\n",
    "        value=Placeholder(Tensor(dtype=\"cat\", range=range(num_colors))),\n",
    "    )\n",
    "\n",
    "    CONCEPTS[\"c0\"] = Concept(name=\"c0\",\n",
    "        repr=to_Variable(torch.rand(REPR_DIM), is_cuda=IS_CUDA),\n",
    "        inherit_from=[\"Image\"],\n",
    "        value=Placeholder(Tensor(dtype=\"cat\", range=range(num_colors))),\n",
    "    )\n",
    "\n",
    "    CONCEPTS[\"c1\"] = Concept(name=\"c1\",\n",
    "        repr=to_Variable(torch.rand(REPR_DIM), is_cuda=IS_CUDA),\n",
    "        inherit_from=[\"Image\"],\n",
    "        value=Placeholder(Tensor(dtype=\"cat\", range=range(num_colors))),\n",
    "    )\n",
    "\n",
    "    OPERATORS[\"r0\"] = Graph(name=\"r0\",\n",
    "        repr=to_Variable(torch.rand(REPR_DIM), is_cuda=IS_CUDA),\n",
    "        forward={\"args\": [Placeholder(\"Image\"), Placeholder(\"Image\")],\n",
    "                 \"output\": Placeholder(\"Bool\"),\n",
    "                 \"fun\": lambda x: x,\n",
    "                })\n",
    "\n",
    "    OPERATORS[\"r1\"] = Graph(name=\"r1\",\n",
    "        repr=to_Variable(torch.rand(REPR_DIM), is_cuda=IS_CUDA),\n",
    "        forward={\"args\": [Placeholder(\"Image\"), Placeholder(\"Image\")],\n",
    "                 \"output\": Placeholder(\"Bool\"),\n",
    "                 \"fun\": lambda x: x,\n",
    "                })\n",
    "\n",
    "    c_pattern = Concept_Pattern(\n",
    "        name=None,\n",
    "        value=Placeholder(Tensor(dtype=\"cat\", range=range(num_colors))),\n",
    "        attr={\n",
    "            \"obj_0\": Placeholder(\"c0\"),\n",
    "            \"obj_1\": Placeholder(\"c1\"),\n",
    "            \"obj_2\": Placeholder(\"c1\"),\n",
    "        },\n",
    "        re={\n",
    "            (\"obj_0\", \"obj_1\"): \"r0\",\n",
    "            (\"obj_1\", \"obj_2\"): \"r1\",\n",
    "        },\n",
    "        pivot_node_names = [\"obj_0\"],\n",
    "    )\n",
    "    self = c_pattern\n",
    "    self.init_ebms()\n",
    "    ebm_dict_all = self.get_ebms()\n",
    "\n",
    "    from reasoning.concept_env.BabyARC.code.dataset.dataset import *\n",
    "    from reasoning.util import get_root_dir\n",
    "    from reasoning.util import to_Variable_recur, visualize_dataset, visualize_matrices\n",
    "\n",
    "    seed = 42\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    # global vars\n",
    "    RUN_AS_CREATOR = False\n",
    "    ARC_OBJ_LOADED = False\n",
    "    DEMO_MAX_ARC_OBJS = 500\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    import logging\n",
    "    FORMAT = \"%(asctime)-15s %(message)s\"\n",
    "    logging.basicConfig(format=FORMAT, level=logging.DEBUG,\n",
    "                        datefmt=\"%Y-%m-%d %H:%M\")\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    try:\n",
    "        arc_obj_dir = os.path.join(get_root_dir(), 'concept_env/datasets/arc_objs.pt')\n",
    "        arc_objs = torch.load(arc_obj_dir)\n",
    "        logger.info(\"SUCCESS! You loaded the pre-collected object file from ARC!\")\n",
    "        ARC_OBJ_LOADED = True\n",
    "    except:\n",
    "        logger.info(\"Please check if obejct file in the directory indicated above!\")\n",
    "        logger.info(f\"WARNING: Please get those pre-collected ARC objects in {arc_obj_dir}!\")\n",
    "        logger.info(\"You can download this file from: https://drive.google.com/file/d/1dZhT1cUFGvivJbSTwnqjou2uilLXffGY/view?usp=sharing\")\n",
    "\n",
    "    # Same Color + IsTouch + Move\n",
    "    dataset_engine = \\\n",
    "        BabyARCDataset(pretrained_obj_cache=os.path.join(get_root_dir(), 'concept_env/datasets/arc_objs.pt'),\n",
    "                       save_directory=\"./BabyARCDataset/\",\n",
    "                       object_limit=1, noise_level=0, canvas_size=8)\n",
    "\n",
    "    def generate_babyarc_selector_task(\n",
    "        dataset_engine,\n",
    "        selector_dict,\n",
    "        concept_collection=[\"line\", \"Lshape\", \"rectangle\", \"rectangleSolid\"],\n",
    "        is_plot=False,\n",
    "    ):\n",
    "\n",
    "        canvas_dict = dataset_engine.sample_single_canvas_by_core_edges(\n",
    "            selector_dict,\n",
    "            allow_connect=True, is_plot=False, rainbow_prob=0.0,\n",
    "            concept_collection=concept_collection,\n",
    "        )\n",
    "        if canvas_dict == -1:\n",
    "            return -1\n",
    "        else:\n",
    "            if is_plot:\n",
    "                canvas = Canvas(\n",
    "                    repre_dict=canvas_dict\n",
    "                )\n",
    "                canvas.render()\n",
    "\n",
    "        return_dict = OrderedDict({\n",
    "            \"obj_masks\" : {},\n",
    "            \"obj_relations\" : {},\n",
    "        })\n",
    "        for k, v in canvas_dict[\"node_id_map\"].items():\n",
    "            return_dict[\"obj_masks\"][k] = canvas_dict[\"id_object_mask\"][v]\n",
    "        return_dict[\"obj_relations\"] = canvas_dict[\"partial_relation_edges\"]\n",
    "        return_dict[\"image_t\"] = canvas_dict[\"image_t\"]\n",
    "        return return_dict\n",
    "\n",
    "    input = torch.rand(128,10,8,8)\n",
    "    w = [torch.rand(128,1,8,8) for _ in range(3)]\n",
    "    energy = self.ebm_forward(input, w)\n",
    "    print(self.info.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
